# PodInsights

A comprehensive tool suite for scraping, analyzing, and visualizing podcast archives. PodInsights extracts episode metadata, transcripts, and shownotes, then uses AI-powered topic extraction and clustering to create interactive visualizations showing the evolution of topics, speakers, and themes across multiple podcasts.

**ğŸŒ Live Demo:** [https://pod-insights.freshx.de](https://pod-insights.freshx.de)

**ğŸ“¦ GitHub Repository:** [https://github.com/d0m1n1kr/pod-insights](https://github.com/d0m1n1kr/pod-insights)

## Features

### Episode Search & Discovery
- âœ… **Semantic Episode Search**: AI-powered search to find episodes by content, not just keywords
- âœ… **Direct Playback Links**: Multiple play buttons per search result linking to relevant positions in episodes
- âœ… **Latest Episodes**: Automatically shows the 10 most recent episodes when no search query is provided
- âœ… **Infinite Scroll**: Paginated results with seamless loading of more episodes
- âœ… **Episode Details**: Comprehensive episode information including speakers, topics, and descriptions
- âœ… **Episode Images**: Visual episode covers displayed throughout the interface

### Speaker Statistics & Analysis
- âœ… **Speaking Time Analysis**: Detailed breakdown of speaking time per speaker per episode
- âœ… **Flow Charts**: Visual representation of speaking time distribution over episode duration
- âœ… **Scatter Plots**: Individual speech segments with duration vs. position visualization
- âœ… **Box Plot Visualizations**: Statistical distribution of speaking patterns
- âœ… **Monologue Analysis**: Longest and shortest speaking segments per speaker
- âœ… **Speaker Profiles**: Rich speaker metadata with images and descriptions

### Global Audio Player
- âœ… **Persistent Player**: Audio playback continues seamlessly across all pages and views
- âœ… **Dual States**: Compact small state and expanded large state with transcript display
- âœ… **Live Transcript**: Real-time transcript display synchronized with audio playback
- âœ… **Speaker Identification**: Current speaker highlighted with profile images
- âœ… **Episode Links**: Clickable episode titles linking to episode detail pages
- âœ… **State Persistence**: Player size preference saved across sessions

### Multi-Podcast Support
- âœ… **Podcast-Auswahl**: Dropdown zur Auswahl zwischen mehreren Podcasts
- âœ… **Podcast-spezifische Daten**: Alle Daten werden pro Podcast organisiert
- âœ… **Dynamische Pfade**: Automatische Pfad-Generierung basierend auf ausgewÃ¤hltem Podcast
- âœ… **Konfigurierbare Podcasts**: Einfache Erweiterung um weitere Podcasts Ã¼ber `podcasts.json`
- âœ… **Podcast-Metadaten**: Logo, Tab-Namen und URLs pro Podcast konfigurierbar

### Data Collection
- âœ… Scrapes episodes from podcast archives
- âœ… Extracts metadata (title, date, duration, speakers, chapters)
- âœ… Extracts transcripts with timestamps and speaker attribution
- âœ… Extracts shownotes with links and categorization
- âœ… Concurrent processing with automatic browser restart

### AI-Powered Analysis
- âœ… LLM-based topic extraction from transcripts
- âœ… Semantic embedding generation for topics
- âœ… Multiple clustering algorithms:
  - **V1**: Hierarchical Agglomerative Clustering (HAC) with fixed clusters
  - **V2**: HDBSCAN with automatic cluster detection
- âœ… Dimensionality reduction (Random Projection for V2)
- âœ… High-performance Rust implementation (10x faster than JavaScript)
- âœ… Variant system for comparing different clustering approaches
- âœ… Multiple linkage methods (weighted, ward, average, complete, single)
- âœ… LLM-based cluster naming with heuristic fallback

### Interactive Visualizations
- âœ… **Variant Selector**: Switch between different clustering variants
- âœ… **Topic River Chart**: Evolution of topics over time
- âœ… **Category River Chart**: High-level overview (legacy)
- âœ… **Speaker River Chart**: Speaker participation over time
- âœ… **UMAP Scatter Plot**: 2D visualization of topic embeddings
- âœ… **Speaker Scatter Plot**: Individual speech segments by duration and position
- âœ… **Heatmaps**:
  - Speaker Ã— Cluster relationships
  - Cluster Ã— Cluster co-occurrence
  - Speaker Ã— Speaker co-occurrence (legacy)
- âœ… **Duration Analysis**: Episode length patterns by year/day of week
- âœ… **Variant Info Panel**: Displays details about the active clustering configuration
- âœ… **Episode Search**: Semantic search for episodes with direct playback links to relevant positions
- âœ… **Speaker Statistics**: Detailed analysis of speaking time, monologues, and time distribution per episode
- âœ… **Global Seamless Player**: Persistent audio player across all pages with small and large states
- âœ… Multilingual interface (German, English, French)
- âœ… Dark mode support with persistent settings

## Quick Start

### Prerequisites

```bash
# Node.js 18+ for scraping and data processing
node --version

# Git LFS (required for embedding databases)
# macOS
brew install git-lfs

# Ubuntu/Debian
sudo apt-get install git-lfs

# Windows
# Download installer from https://git-lfs.github.com

# Initialize Git LFS (one-time setup)
git lfs install

# Rust (optional, for 10x faster clustering)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

### Installation

```bash
# Clone repository (Git LFS will automatically download embedding databases)
git clone https://github.com/d0m1n1kr/pod-insights.git
cd pod-insights

# If you already cloned without Git LFS, fetch the LFS files:
# git lfs pull

# Install dependencies
npm install

# Configure API keys (copy and edit)
cp settings.example.json settings.json
# Add your OpenAI API key (or alternative LLM provider)
```

## Automated Pipeline

For a complete automated processing of a podcast, you can use the `process-podcast.sh` script. This script orchestrates all steps from scraping to visualization generation:

```bash
# Process a complete podcast (all steps)
./scripts/process-podcast.sh <podcast-id>

# Example: Process Freak Show
./scripts/process-podcast.sh freakshow

# Example: Process Logbuch:Netzpolitik
./scripts/process-podcast.sh lnp

# Skip scraping if data already exists
./scripts/process-podcast.sh <podcast-id> --skip-scraping

# Skip RAG database creation
./scripts/process-podcast.sh <podcast-id> --skip-rag
```

**What it does:**
1. Scrapes episode data (metadata, transcripts, shownotes, speakers, chapters)
2. Extracts and normalizes topics using LLMs
3. Creates semantic embeddings for topics
4. Performs topic clustering (V2 auto-v2.1 variant)
5. Generates visualization data files (river charts, heatmaps, UMAP)
6. Generates optional data (MP3 index, speaker profiles, TS-live files)
7. Creates RAG database (optional)
8. Organizes all files into the correct frontend structure
9. Creates necessary symbolic links

**Output:** All data files organized in `frontend/public/podcasts/<podcast-id>/` ready for frontend use.

## Step-by-Step Guide

If you prefer to run individual steps manually or need more control over the process, you can follow these 6 phases. Each script accepts a `--podcast <id>` parameter for multi-podcast support.

### Phase 1: Data Collection

#### 1. Scrape Episode List
Extract basic metadata for all episodes:

```bash
node scripts/scrape.js --podcast freakshow
```

**Output:** `podcasts/freakshow/episodes/1.json`, `podcasts/freakshow/episodes/2.json`, ... (300+ files)

**Time:** ~5 minutes

#### 2. Scrape Episode Details
Extract transcripts, shownotes, and descriptions:

```bash
node scripts/scrape-details.js --podcast freakshow --all
```

**Output:**
- Transcripts: `podcasts/freakshow/episodes/1-ts.json`, `podcasts/freakshow/episodes/2-ts.json`, ...
- Shownotes: `podcasts/freakshow/episodes/1-sn.json`, `podcasts/freakshow/episodes/2-sn.json`, ...
- Descriptions: `podcasts/freakshow/episodes/1-text.html`, `podcasts/freakshow/episodes/2-text.html`, ...

**Time:** ~30-60 minutes (concurrent processing, 3 episodes at a time)

#### 3. Scrape Speakers
Extract speaker information and profiles:

```bash
node scripts/scrape-speakers.js --podcast freakshow
```

**Output:** Speaker data in `podcasts/freakshow/speakers/`

**Time:** ~2 minutes

#### 4. Generate Speaker Stats
Create detailed speaker statistics for each episode:

```bash
node scripts/generate-speaker-stats.js --podcast freakshow --all
```

**Output:** `podcasts/freakshow/episodes/1-speaker-stats.json`, ...

**Time:** ~1 minute

#### 6. Scrape Chapters
Extract episode chapters:

```bash
node scripts/scrape-chapters.js --podcast freakshow --all
```

**Output:** `podcasts/freakshow/episodes/1-chapters.json`, ...

**Time:** ~5 minutes

#### 8. Scrape Legacy Shownotes (Optional)
Extract OSF-format shownotes for older episodes (if applicable):

```bash
node scripts/scrape-osf.js --podcast freakshow
```

**Output:** `podcasts/freakshow/episodes/89-osf.json`, ...

**Time:** ~15 minutes

**Total Data:** 1000+ files organized by podcast

### Phase 2: Topic Extraction & Analysis

#### 10. Extract Topics with LLM
Identify main topics from episode transcripts:

```bash
# Test with a single episode first
node scripts/extract-topics.js --podcast freakshow 296

# Process all episodes
node scripts/extract-topics.js --podcast freakshow --all
```

**Output:** `podcasts/freakshow/episodes/1-topics.json`, `podcasts/freakshow/episodes/2-topics.json`, ...

**Time:** ~2-4 hours for all episodes

**Cost:** ~$5-10 (with gpt-4o-mini)

**Configuration:** Edit `settings.json` to customize:
```json
{
  "llm": {
    "provider": "openai",
    "model": "gpt-4o-mini",
    "apiKey": "YOUR_API_KEY",
    "temperature": 0.3
  },
  "topicExtraction": {
    "maxTopics": 10,
    "language": "de"
  }
}
```

#### 12. Normalize Topics
Clean up and standardize extracted topics:

```bash
node scripts/normalize-topics.js --podcast freakshow
```

**Output:** Updates topic files in place

**Time:** ~30 seconds

#### 13. Generate Extended Topics (for RAG)
Create additional topic data for better AI search:

```bash
node scripts/generate-extended-topics.js --podcast freakshow --all
```

**Output:** Extended topic data for RAG database

**Time:** ~10 minutes

#### 14. Create Embeddings
Generate semantic embeddings for all topics:

```bash
node scripts/create-embeddings.js --podcast freakshow
```

**Output:** `db/freakshow/topic-embeddings.json` (~500MB per podcast)

**Time:** ~10-15 minutes

**Cost:** ~$2-3 (OpenAI text-embedding-3-large)

### Phase 3: Clustering

#### 15. Build Clustering Variant
Create topic clusters using V2 HDBSCAN (recommended):

```bash
# Build the standard auto-v2.1 variant
./scripts/build-variant.sh v2 auto-v2.1 --podcast freakshow

# Alternative: V1 with fixed clusters
./scripts/build-variant.sh v1 default-v1 --podcast freakshow
```

**Available Variants:**
- **auto-v2.1**: HDBSCAN with automatic cluster detection (recommended)
- **default-v1**: HAC with 256 fixed clusters
- **coarse-v1/fine-v1**: Different cluster granularities

**What it does:**
1. Reads configuration from `variants.json`
2. Runs appropriate clustering algorithm (Rust)
3. Generates taxonomy, river charts, UMAP, and heatmaps
4. Moves output to `frontend/public/podcasts/freakshow/topics/auto-v2.1/`

**Time:**
- V2: ~60-90 seconds + LLM naming (~2 minutes)
- V1: ~20-30 seconds + LLM naming (~2 minutes)

**Cost:** ~$0.50 per variant for LLM-based cluster naming

### Phase 4: Generate Visualizations

#### 17. Generate Speaker River Data
Create speaker participation timeline:

```bash
node scripts/generate-speaker-river.js --podcast freakshow
```

**Output:** `speaker-river-data.json`

**Time:** ~30 seconds

#### 18. Generate Speaker-Speaker Heatmap
Analyze speaker co-occurrence patterns:

```bash
node scripts/generate-speaker-speaker-heatmap.js --podcast freakshow
```

**Output:** `speaker-speaker-heatmap.json`

**Time:** ~10 seconds

#### 19. Generate Duration Heatmaps
Analyze episode length patterns:

```bash
node scripts/generate-year-duration-heatmap.js --podcast freakshow
node scripts/generate-dayofweek-duration-heatmap.js --podcast freakshow
node scripts/generate-speaker-duration-heatmap.js --podcast freakshow
```

**Output:** `year-duration-heatmap.json`, `dayofweek-duration-heatmap.json`, `speaker-duration-heatmap.json`

**Time:** ~20 seconds total

### Phase 5: Optional Processing

#### 20. Generate MP3 Index (Optional)
Create index for MP3 files:

```bash
node scripts/generate-episodes-mp3.js --podcast freakshow
```

**Output:** MP3 metadata index

#### 21. Generate TS-Live Files
Create live transcript files for the player:

```bash
node scripts/generate-ts-live.js --podcast freakshow --all
```

**Output:** `podcasts/freakshow/episodes/1-ts-live.json`, ...

**Time:** ~5 minutes

#### 23. Create RAG Database
Build AI search database:

```bash
node scripts/create-rag-db.js --podcast freakshow
```

**Output:** `db/freakshow/rag-embeddings.json`

**Time:** ~5 minutes

**Cost:** ~$1-2

### Phase 6: Organize Frontend Files

**Diese Phase wird automatisch von `process-podcast.sh` ausgefÃ¼hrt**

**Output Structure per Podcast:**
```
podcasts/freakshow/
â”œâ”€â”€ episodes/              # Raw episode data
â”œâ”€â”€ speakers/              # Speaker profiles
â””â”€â”€ ...

frontend/public/podcasts/freakshow/
â”œâ”€â”€ episodes.json          # Episode index
â”œâ”€â”€ speaker-river-data.json
â”œâ”€â”€ *-heatmap.json         # All heatmap data
â”œâ”€â”€ episodes/              # Symlink to raw episodes
â”œâ”€â”€ speakers/              # Speaker data
â””â”€â”€ topics/
    â””â”€â”€ auto-v2.1/         # Clustering variant data
        â”œâ”€â”€ topic-taxonomy.json
        â”œâ”€â”€ topic-river-data.json
        â”œâ”€â”€ topic-umap-data.json
        â”œâ”€â”€ speaker-cluster-heatmap.json
        â””â”€â”€ cluster-cluster-heatmap.json
```

## Empfohlener Workflow

**Verwende das automatische Script fÃ¼r die komplette Verarbeitung:**

```bash
# Komplette Verarbeitung eines Podcasts
./scripts/process-podcast.sh freakshow

# Von bestimmter Phase starten (z.B. nur Clustering)
./scripts/process-podcast.sh freakshow --from-step 3

# Scraping Ã¼berspringen wenn Daten vorhanden
./scripts/process-podcast.sh freakshow --skip-scraping

# RAG-Datenbank Ã¼berspringen
./scripts/process-podcast.sh freakshow --skip-rag
```

**Alle Scripts unterstÃ¼tzen den `--podcast <id>` Parameter fÃ¼r Multi-Podcast-Support**

### Phase 4: Run Frontend

#### 11. Install Frontend Dependencies

```bash
cd frontend
npm install
```

#### 12. Start Development Server

```bash
npm run dev
```

**Access:** http://localhost:5173

#### 13. Build for Production

```bash
npm run build
```

**Output:** `frontend/dist/` (static files ready for deployment)

## RAG AI Search Backend (Rust)

This repo includes a small Rust HTTP backend (`rag-backend`) that does RAG over podcast-specific databases in `db/<podcast-id>/rag-embeddings.json` (created by `node scripts/create-rag-db.js --podcast <id>`). It supports all podcasts simultaneously and selects the appropriate database based on the `podcastId` parameter in API requests.

### Build the RAG DB

```bash
# Build RAG database for a specific podcast
npm run create-rag-db -- --podcast freakshow

# Build RAG databases for all podcasts
for podcast in cre forschergeist lnp raumzeit ukw freakshow; do
  npm run create-rag-db -- --podcast "$podcast"
done
```

### Run the backend

The backend reads LLM settings from `settings.json` (fallback: `settings.example.json`) and also supports **env overrides**. The LLM API must be **OpenAI-compatible** for both embeddings and chat.

```bash
export LLM_API_KEY="sk-..."
# Optional overrides (otherwise taken from settings.json):
export LLM_BASE_URL="https://api.openai.com/v1"
export LLM_MODEL="gpt-4o-mini"
export EMBEDDING_MODEL="text-embedding-3-small"

# Optional:
export RAG_DB_PATH="./db/rag-embeddings.json"
export EPISODES_DIR="./episodes"
export RAG_BIND_ADDR="127.0.0.1:7878"
export RAG_TOP_K="6"

cargo run --bin rag-backend
```

### Call the API

```bash
# Suche in allen verfÃ¼gbaren Podcasts (fallback: freakshow)
curl -s http://127.0.0.1:7878/api/chat \
  -H 'Content-Type: application/json' \
  -d '{ "query": "Worum ging es bei Universal Control?" }' | jq

# Explizit einen Podcast angeben
curl -s http://127.0.0.1:7878/api/chat \
  -H 'Content-Type: application/json' \
  -d '{ "query": "Worum ging es bei Universal Control?", "podcastId": "freakshow" }' | jq
```

Response shape:

- **`answer`**: LLM answer (with citations like `(Episode 281, 12:38-17:19)`)
- **`sources[]`**: list of sources with `episodeNumber`, `startSec/endSec`, and an `excerpt`

## Multi-Podcast Setup

Die Anwendung unterstÃ¼tzt jetzt mehrere Podcasts. Jeder Podcast hat seine eigenen Daten und Konfiguration.

### Podcast-Konfiguration

Bearbeite `frontend/public/podcasts.json` um Podcasts hinzuzufÃ¼gen:

```json
{
  "podcasts": [
    {
      "id": "freakshow",
      "name": "Freak Show",
      "tabName": "FdFS",
      "logoUrl": "https://freakshow.fm/files/2013/07/cropped-freakshow-logo-600x600-180x180.jpg",
      "homeUrl": "https://freakshow.fm/",
      "feedUrl": "https://feeds.metaebene.me/freakshow/mp3",
      "archiveUrl": "https://freakshow.fm/archiv",
      "teamUrl": "https://freakshow.fm/team"
    }
  ]
}
```

### Verzeichnisstruktur

Jeder Podcast hat seine eigenen Verzeichnisse:

```
podcasts/
â””â”€â”€ <podcast-id>/
    â”œâ”€â”€ episodes/            # Episode-Daten
    â””â”€â”€ speakers/           # Speaker-Daten

frontend/public/podcasts/
â””â”€â”€ <podcast-id>/
    â”œâ”€â”€ episodes.json       # Episode-Index
    â”œâ”€â”€ speaker-river-data.json
    â”œâ”€â”€ topic-river-data.json
    â”œâ”€â”€ topics/             # Clustering-Varianten
    â”‚   â”œâ”€â”€ manifest.json
    â”‚   â””â”€â”€ <variant>/
    â””â”€â”€ speakers/           # Speaker-Metadaten
```

### Skripte mit Podcast-Parameter

Alle Skripte unterstÃ¼tzen den `--podcast` Parameter:

```bash
# Scraping fÃ¼r einen bestimmten Podcast
node scripts/scrape.js --podcast freakshow
node scripts/scrape-details.js --podcast freakshow

# Topic-Extraktion
node scripts/extract-topics.js --podcast freakshow --all

# Clustering
./scripts/build-variant.sh v2 auto-v2.1 --podcast freakshow

# Visualisierungen generieren
node scripts/generate-topic-river.js --podcast freakshow
node scripts/generate-speaker-river.js --podcast freakshow
```

### RAG-Backend

Das RAG-Backend unterstÃ¼tzt alle Podcasts gleichzeitig. Die Podcast-Auswahl erfolgt pro Anfrage:

```bash
cargo run --bin rag-backend
```

## Project Structure

```
pod-insights/
â”œâ”€â”€ Cargo.lock               # Rust dependencies lock file
â”œâ”€â”€ Cargo.toml               # Rust project configuration
â”œâ”€â”€ db/                      # Database files and embeddings
â”‚   â”œâ”€â”€ rag-embeddings.json  # RAG database for AI search
â”‚   â”œâ”€â”€ topic-embeddings.json # Topic semantic embeddings (~500MB)
â”‚   â”œâ”€â”€ cre/                 # Podcast-specific embeddings
â”‚   â”œâ”€â”€ forschergeist/
â”‚   â”œâ”€â”€ lnp/
â”‚   â”œâ”€â”€ raumzeit/
â”‚   â””â”€â”€ ukw/
â”‚
â”œâ”€â”€ docs/                    # Documentation files
â”œâ”€â”€ frontend/                # Vue.js visualization application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ views/           # Main view components
â”‚   â”‚   â”œâ”€â”€ components/      # Reusable components (charts, selectors)
â”‚   â”‚   â”œâ”€â”€ composables/     # Vue composables (podcast, audio, etc.)
â”‚   â”‚   â”œâ”€â”€ stores/          # Pinia state management
â”‚   â”‚   â”œâ”€â”€ i18n/            # Internationalization (de, en, fr)
â”‚   â”‚   â””â”€â”€ types.ts         # TypeScript type definitions
â”‚   â””â”€â”€ public/
â”‚       â”œâ”€â”€ podcasts.json    # Podcast configuration
â”‚       â””â”€â”€ podcasts/        # Frontend data per podcast
â”‚           â””â”€â”€ <podcast-id>/
â”‚               â”œâ”€â”€ episodes.json     # Episode index
â”‚               â”œâ”€â”€ episodes/         # Episode data (symlinks)
â”‚               â”œâ”€â”€ speakers/         # Speaker metadata
â”‚               â””â”€â”€ topics/           # Clustering variants
â”‚                   â”œâ”€â”€ manifest.json
â”‚                   â””â”€â”€ <variant>/
â”‚
â”œâ”€â”€ lib/                     # Shared JavaScript utilities
â”œâ”€â”€ podcasts/                # Raw podcast data (organized by podcast)
â”‚   â”œâ”€â”€ cre/                 # Chaosradio Express
â”‚   â”œâ”€â”€ forschergeist/       # Forschergeist
â”‚   â”œâ”€â”€ lnp/                 # Logbuch:Netzpolitik
â”‚   â”œâ”€â”€ raumzeit/            # Raumzeit
â”‚   â””â”€â”€ ukw/                 # UKW
â”‚
â”œâ”€â”€ scripts/                 # Build and utility scripts
â”‚   â”œâ”€â”€ process-podcast.sh   # Complete podcast processing pipeline
â”‚   â”œâ”€â”€ build-variant.sh     # Clustering variant builder
â”‚   â”œâ”€â”€ sync.sh             # Data synchronization
â”‚   â””â”€â”€ generate-*.js       # Individual data generators
â”‚
â”œâ”€â”€ src/                    # Rust source code
â”‚   â”œâ”€â”€ cache.rs            # Caching utilities
â”‚   â”œâ”€â”€ cluster_topics.rs   # V1 HAC clustering
â”‚   â”œâ”€â”€ cluster_topics_v2.rs # V2 HDBSCAN clustering
â”‚   â”œâ”€â”€ config.rs           # Configuration handling
â”‚   â”œâ”€â”€ handlers/           # HTTP request handlers
â”‚   â”œâ”€â”€ lib.rs              # Library exports
â”‚   â”œâ”€â”€ rag/                # RAG search implementation
â”‚   â”œâ”€â”€ rag_backend.rs      # RAG HTTP server
â”‚   â”œâ”€â”€ transcript.rs       # Transcript processing
â”‚   â””â”€â”€ utils.rs            # Utility functions
â”‚
â”œâ”€â”€ settings.json           # Configuration (API keys, etc.)
â”œâ”€â”€ settings.example.json   # Example configuration template
â”œâ”€â”€ variants.json           # Clustering variant definitions
â”œâ”€â”€ package.json            # Node.js dependencies and scripts
â””â”€â”€ README.md              # This documentation
```

## Configuration

### LLM Providers

Edit `settings.json` to configure your preferred LLM:

**OpenAI (Default)**
```json
{
  "llm": {
    "provider": "openai",
    "model": "gpt-4o-mini",
    "apiKey": "sk-...",
    "baseURL": "https://api.openai.com/v1"
  }
}
```

**Anthropic Claude**
```json
{
  "llm": {
    "provider": "anthropic",
    "model": "claude-3-haiku-20240307",
    "apiKey": "sk-ant-..."
  }
}
```

**OpenRouter (Access multiple models)**
```json
{
  "llm": {
    "provider": "openrouter",
    "model": "anthropic/claude-3-haiku",
    "apiKey": "sk-or-...",
    "baseURL": "https://openrouter.ai/api/v1"
  }
}
```

**Ollama (Local/Free)**
```json
{
  "llm": {
    "provider": "ollama",
    "model": "llama2",
    "baseURL": "http://localhost:11434/api"
  }
}
```

### Clustering Options

**V1 (Hierarchical Agglomerative Clustering):**
```json
{
  "topicClustering": {
    "embeddingModel": "text-embedding-3-large",
    "embeddingBatchSize": 100,
    "clusters": 256,
    "outlierThreshold": 0.7,
    "linkageMethod": "weighted",
    "useRelevanceWeighting": true,
    "useLLMNaming": true,
    "model": "gpt-4o-mini"
  }
}
```

**V2 (HDBSCAN with Dimensionality Reduction):**
```json
{
  "topicClustering": {
    "embeddingModel": "text-embedding-3-large",
    "embeddingBatchSize": 100,
    "minClusterSize": 5,
    "minSamples": 3,
    "reducedDimensions": 50,
    "outlierThreshold": 0.7,
    "useRelevanceWeighting": true,
    "useLLMNaming": true,
    "model": "gpt-4o-mini"
  }
}
```

**Parameters:**
- `clusters` (V1 only): Fixed number of clusters to create
- `linkageMethod` (V1 only): Linkage method (weighted, ward, average, complete, single)
- `minClusterSize` (V2 only): Minimum points to form a cluster
- `minSamples` (V2 only): Core point threshold
- `reducedDimensions` (V2 only): Target dimensions for Random Projection (50-100 recommended)
- `outlierThreshold`: Distance threshold for outlier detection
- `useRelevanceWeighting`: Weight topics by episode frequency
- `useLLMNaming`: Use LLM for cluster naming (vs. heuristic)

**Legacy Category Grouping:**
```json
{
  "categoryGrouping": {
    "categories": 12               // Number of high-level categories
  }
}
```

## Output Files

### Episode Data
- `episodes/<N>.json` - Episode metadata
- `episodes/<N>-ts.json` - Transcript with timestamps
- `episodes/<N>-sn.json` - Modern shownotes (episodes 191+)
- `episodes/<N>-osf.json` - Legacy OSF shownotes (episodes 89-190)
- `episodes/<N>-text.html` - Episode description
- `episodes/<N>-topics.json` - Extracted topics

### Analysis Results
- `db/topic-embeddings.json` - Semantic embeddings for all topics (~500MB)
- `topic-taxonomy.json` - Generated by variant builds (in variant folders)
- `topic-taxonomy-detailed.json` - Extended cluster information (in variant folders)
- `topic-categories.json` - 12 high-level categories (legacy)

### Visualization Data (per Variant)
Located in `frontend/public/topics/<variant-name>/`:
- `topic-taxonomy.json` - Cluster hierarchy for this variant
- `topic-taxonomy-detailed.json` - Detailed topic-to-cluster mapping
- `topic-river-data.json` - Topic evolution over time
- `topic-umap-data.json` - 2D UMAP projection
- `speaker-cluster-heatmap.json` - Speaker-cluster matrix
- `cluster-cluster-heatmap.json` - Cluster co-occurrence matrix

### Legacy Visualization Data
- `category-river-data.json` - Category overview
- `speaker-river-data.json` - Speaker participation
- `speaker-category-heatmap.json` - Speaker-category matrix (obsolete)
- `speaker-speaker-heatmap.json` - Speaker co-occurrence (obsolete)
- `year-duration-heatmap.json` - Duration patterns by year
- `dayofweek-duration-heatmap.json` - Duration patterns by weekday

## Cost Estimation

| Phase | Service | Approx. Cost |
|-------|---------|--------------|
| Topic Extraction | OpenAI API (gpt-4o-mini) | $5-10 |
| Embeddings | OpenAI API (text-embedding-3-large) | $2-3 |
| Cluster Naming | OpenAI API (gpt-4o-mini, 256 clusters) | $0.50 |
| Category Naming | OpenAI API (gpt-4o-mini, 12 categories) | $0.10 |
| **Total** | | **~$8-14** |

**Note:** Using local models (Ollama) reduces cost to ~$0 but may affect quality.

## Performance

### Clustering Performance Comparison

**V1 (Hierarchical Agglomerative Clustering):**

| Operation | JavaScript | Rust | Speedup |
|-----------|-----------|------|---------|
| Distance Matrix (4500 topics) | ~20s | ~2s | 10x |
| Clustering (â†’256 clusters) | ~180s | ~15s | 12x |
| Total (excl. LLM) | ~3-5 min | ~20-30s | ~10x |

**V2 (HDBSCAN with Dimensionality Reduction):**

| Operation | Rust Only | Time |
|-----------|-----------|------|
| Random Projection (3072â†’50 dims) | ~5s | - |
| Distance Matrix (4500 topics, 50 dims) | ~3s | - |
| HDBSCAN Clustering | ~30s | - |
| Merge Small Clusters | ~5s | - |
| Total (excl. LLM) | ~60-90s | - |

**Note:** V2 automatically detects the optimal number of clusters (typically 30-50) vs. V1's fixed 256 clusters.

## Documentation

### Clustering & Analysis
- `RUST-CLUSTERING.md` - V1 Rust implementation guide (HAC)
- `CLUSTERING-V2.md` - V2 HDBSCAN implementation guide
- `VARIANTS-SYSTEM.md` - Variant system architecture
- `VARIANTS-QUICKSTART.md` - Quick start guide for variants
- `VARIANTS-COMPLETE.md` - Complete variant feature summary

### Visualizations
- `CATEGORY-RIVER-GUIDE.md` - Category grouping explanation
- `RIVER-CHARTS-OVERVIEW.md` - Comparison of all chart types
- `VISUAL-EXPLANATION.md` - Visual guide to the hierarchy
- `DURATION-HEATMAPS.md` - Duration analysis documentation
- `UMAP-FEATURE.md` - UMAP visualization guide

### Frontend
- `frontend/README.md` - Frontend-specific documentation

## Troubleshooting

### API Rate Limits
If clustering hangs during LLM naming:
- The Rust version includes automatic retry with exponential backoff
- Configure delays in `settings.json`:
  ```json
  {
    "topicExtraction": {
      "requestDelayMs": 2000,
      "maxRetries": 5
    }
  }
  ```

### Out of Memory
For large datasets, increase Node.js memory:
```bash
NODE_OPTIONS="--max-old-space-size=4096" npm run create-embeddings
```

### Missing Data Files
Ensure all steps are completed in order. Each phase depends on outputs from previous phases.

### Rust Build Issues
```bash
# Update Rust toolchain
rustup update

# Clean and rebuild
cargo clean
cargo build --release
```

## Development

### Running Tests
```bash
# Backend
npm test

# Frontend
cd frontend
npm run test
```

### Code Style
```bash
# Format frontend code
cd frontend
npm run format
```

## Contributing

This is a personal analysis project, but improvements are welcome! Focus areas:
- Additional visualization types
- Performance optimizations
- New clustering algorithms (V3?)
- Support for other podcast formats
- Improved cluster quality metrics
- Better dimensionality reduction techniques

## License

This project is for personal/educational use. Podcast content belongs to their respective creators.

## Credits

- **Technologies:** Node.js, Rust, Vue.js, D3.js, Puppeteer, OpenAI API
- **Inspiration:** Exploring podcast evolution through data visualization
