{"v":1,"episode":289,"speakers":["Tim Pritlove","roddi","Ralf Stockmann"],"t":[110,151,152,154,157,181,229,233,252,263,264,305,344,349,354,356,364,368,371,410,423,425,427,431,439,481,482,668,674,708,723,794,796,807,812,847,850,860,868,875,882,913,917,979,982,1153,1159,1160,1193,1194,1197,1232,1322,1388,1395,1405,1407,1418,1422,1425,1429,1432,1609,1625,1717,1784,1792,1833,1939,1984,2010,2013,2014,2015,2016,2017,2023,2025,2027,2029,2031,2052,2062,2081,2162,2169,2172,2175,2198,2257,2257,2319,2323,2328,2330,2337,2341,2347,2352,2354,2400,2404,2413,2564,2568,2589,2598,2622,2652,2655,2659,2676,2684,2703,2726,2781,2794,3005,3008,3244,3249,3287,3291,3302,3304,3306,3309,3318,3322,3357,3362,3409,3428,3441,3448,3450,3491,4058,4070,4122,4215,4223,4330,4509,4640,4723,4725,4735,4739,4742,4759,4762,4819,4822,4826,4829,4835,4840,4843,4861,4862,4868,4888,4892,4923,4925,4927,4959,4962,5153,5225,5256,5285,5288,5294,5299,5305,5309,5316,5321,5340,5341,5378,5519,5520,5550,5551,5565,5572,5582,5584,5658,5709,5713,5721,5726,5730,5764,5769,5796,5800,5848,5861,5864,5876,5883,5885,5912,5914,5916,5919,5943,5945,5947,5949,5952,5954,5955,5958,5959,5961,5979,5984,5986,5988,5997,5999,6006,6007,6008,6013,6015,6018,6021,6026,6036,6039,6047,6048,6051,6054,6072,6072,6073,6126,6131,6166,6169,6345,6348,6353,6358,6630,6634,6637,6641,6922,6942,7013,7018,7034,7134,7177,7190,7191,7253,7257,7270,7296,7334,7335,7342,7345,7437,7449,7455,7455,7457,7461,7469,7478,7487,7490,7498,7505,7547,7549,7586,7587,7589,7593,7798,7803,7807,7819,8135,8137,8148,8151,8314,8366,8574,8605,8709,8715,9183,9217,9218,9334,9335,9376,9382,9399,9403,9404,9653,9901,9921,9923,9935,10129,10134,10202,10207,10235,10238,10421,10433,10577,10594,10599,10600,10612,10682,10736,10740,10794,10796,10807,10888,10890,10974,10979,11009,11175,11188,11194,11195,11196,11200,11211,11219,11229,11233,11400,11443,11473,11475,11538,11541,11558,11569,11577,11619,11660,11665,11668,11686,11704,11759,11763,11888,11924,11932,11937,12004,12009,12015,12030,12044,12061,12062,12069,12070,12141,12145,12157,12162,12191,12201,12204,12220,12259,12294,12306,12309,12322,12332,12560,12574,12576,12584,12605,12623,12636,12638,12643,12650,12651,12667,12669,12706,12782,12786,12792,12804,12835,12840,12843,12845,12927,12930,12972,12974,12980,12984,12992,13001,13011,13031,13035,13056,13059,13060,13072,13075,13098,13099,13119],"s":[0,1,0,2,0,2,1,2,0,1,0,2,0,2,1,2,1,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,1,0,2,0,2,1,0,2,1,0,2,0,1,0,1,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,1,0,2,1,2,1,2,0,1,0,2,0,2,0,1,2,0,1,2,1,2,1,2,1,0,1,2,0,1,2,0,1,2,1,0,1,0,1,0,1,0,1,0,1,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,1,2,0,2,0,2,0,2,1,2,1,0,1,2,1,0,1,0,2,0,1,2,1,0,1,0,1,2,1,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,1,0,1,0,1,0,2,0,2,0,2,0,1,0,1,0,2,1,2,1,2,1,2,1,0,1,2,0,2,1,0,2,0,2,0,1,0,1,2,1,2,0,2,0,2,1,2,1,2,1,2,1,2,0,2,0,2,1,0,1,0,2,0,1,0,1,0,2,0,2,0,1,2,1,2,0,2,0,2,0,1,0,1,2,0,1,0,2,0,2,0,2,1,2,1,0,1,0,1,0,2,0,2,0,2,0,2,0,2,0,1,0,2,0,2,0,2,0,2,0,2,0,2,1,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,1,2,0,2,0,2,0,2,0,2,0,1,2,1,0,2,0,2,1,0,1,0,1,0,2,0,1,0,1,0,1,2,1,2,0,1,2,1,2,1,0,1,0,1,2,0,2,1,2,1,0,2,0,2,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,2,0,2,1,2,0,1,0,2,1,2,1,2,0,1,0,2,1,0,2,0,2,0,1,0,1,0,2,0,2,0,2,0,1,0,2,0,2,0,2,0,2,0,1,0,1],"x":["So, jetzt aber.\nFast hätte alles so gut funktioniert. Leute, hier ist die Freakshow.\nWir sind wieder da. Zu einem ungewöhnlichen Zeitpunkt. Wer jetzt live zuhört,\nwundert sich, wundert sich. Draußen ist ja noch hell.\nGenau. Kommen wir gleich zu. Aus technischen Gründen. Diese Sendung wird an\neinem Freitagnachmittag ausgestrahlt.\nWenn man die Zeit zu einer MEZ heranzieht, bei mir ist es dann schon dunkel.\nIch bin nämlich hier aus der Ferne zugeschaltet zu den Herren und die Herren\nsind Roddy. Hallo Roddy.","Hallo.","Und Ralf, wer soll das anderes sein?","Einen wunderschönen guten Mittag.","Genau, ich bin Tim und wir befinden uns trotzdem zusammen.\nAber ich bin nämlich gerade in einer anderen Zeitzone und deswegen war das ein\netwas größerer Aufwasch, hier das Ganze so zusammenzubekommen.\nAber ich muss sagen, ich bin jetzt vorsichtig optimistisch, dass das ganz gut\nfunktioniert. Jetzt bin ich nämlich mal remote.\nDu kannst ja mal erzählen, wie das aus eurer Perspektive sich gerade darstellt.","Also akustisch erstmal hört man, finde ich, gar keinen Unterschied.\nAlso da muss man sagen, ist Zoom mittlerweile schon weit fortgeschritten dank Corona.\nUnd du hast, glaube ich, hattest du vorhin gesagt, auch diesen Musikermodus\naktiviert, dass da nichts an Kompression und Pseudo-Rauschreduzierung und sowas daherkommt.\nAlso die Audiospur hört sich jetzt schon ohne Lokalaufnahme sehr gut an hier.\nUnd ansonsten ist unser Setup eigentlich so wie sonst auch, also sprich rechts\nhaben wir deinen großen Curved Monitor und da läuft friedlich ein Ultraschall,\nnoch in der alten Version,\nich prangere es an und oben der Beamer und unten drunter halt der Gastbildschirm\nund da bist jetzt halt du statt andere Leute als Gast, aber psychologisch würde ich sagen,\nfühlt es sich eigentlich im Moment noch an wie eine ganz normale Freakshow, die wir hier machen.","Wenn es so etwas gibt.","Ja, also Latenz total gut. Wir hatten jetzt hier auch schon ein bisschen vorgeplänkelt und so.\nMan hat jetzt auch nicht den Eindruck, ständig muss ich wieder irgendwie aufhören\nzu sprechen, weil irgendjemand anders von draußen möchte irgendwie da mit rein.\nAber du hast auch, glaube ich, gutes Internet, hast du vorhin mal erläutert.","Ich habe hier exzellentes Internet. Der Speedtest sagt, ich habe hier ein Gigabit\ndown und ein halbes Gigabit up.\nDas ist hier so, was man hier normal nennt.","Das kleine Besteck.","Ich kann immer dazu sagen, wo ich gerade bin. Ich bin gerade in... Genau so der Standard.\nMan sieht ja immer irgendwie irgendwelche Techniker rumwurschteln,\nwie sie irgendwie draußen irgendwelche Leitungen zusammenballern.\nAber hier ist halt irgendwie alles auf Glasfaser und hier nimmt man halt irgendwie\nrichtig am Internet teil, nicht so wie bei uns.\nWo bin ich? Ich bin in Malaysia, genauer bin ich gerade auf der Insel Penang.\nUnd ja, habe hier gutes Netz, gutes Essen, gutes Wetter und bin an sich auch\nganz zufrieden, muss ich sagen, mit der Gesamtsituation.","Ja, das Essen ist glaube ich mein Hauptneidfaktor in meiner ganzen Geschichte.\nIch könnte mich glaube ich auch echt den Rest meines Lebens einfach nur asiatisch ernähren.\nDas wäre eine gute Sache und gar keine Einschränkung. Man muss allerdings sagen,\ndass das Videobild von dir, das ist schon sehr klötzerig. Also dafür,\ndass du jetzt da so eine Datenrate an den Start bringst, kannst du da eigentlich noch irgendwas tun?\nAlso das ist weit entfernt von irgendwie Hi-Res oder High Definition und Full HD oder sowas.\nAlso das ist okay, es ist jetzt nicht so, dass es jetzt total stört,\naber wenn man schon mal Bandbreite hat, hätte ich gedacht, dass Zoom die vielleicht\nauch durchreicht und da jetzt mal die Videoqualität noch ein bisschen hoch schraubt.\nJetzt wollen wir natürlich nicht her.","Auch gerade unklar, was da der Grund für sein kann.","Weil eigentlich ist es jetzt auch okay, aber es ist jetzt halt nicht super toll.","Du weißt doch, wie Tim aussieht.","Ich mache einfach mal ein Screenshot für unsere Gäste, damit die mal sehen,\nwie das hier so alles wirkt.","Das ist bestimmt das Netz.","Ja, müssen wir jetzt nicht.","Wir wissen jetzt nicht genau, an welcher Stelle das den Tod stirbt.\nIrgendwas ist ja immer.\nSoviel zum Vorgeplänkel. Also ich bin ja auf Reisen. Ich hatte das ja angekündigt\nund bewege mich hier so quer durch Südostasien und habe so meine ersten Etappen auch schon hinter mir.\nKann ich mal so vielleicht ein bisschen erzählen, falls es euch interessiert.\nIch war jetzt in Singapur.\nAllerdings nur ein Wochenende. Mir trägt das nicht, weil es ist ja so ein bisschen\nso Dubai nett, kann man sagen.","Da steht doch aber dieses, finde ich, doch sehr, sehr coole Gebäude auf diesen\nvier Stelzen, was in so einem Halbrund irgendwie gebaut ist.\nIch gucke mal eben gerade. Das ist so einer dieser...","Du meinst das Ding oben mit diesem fetten Pool oben drauf?","Ja, genau.","Ja, das ist hier so ein bisschen das Wahrzeichen geworden.","Genau, das finde ich schon... Das finde ich sehr krass, genau.\nWas aussieht wie so ein Schiff, was auf Stelzen irgendwie so läuft.","Also ich muss sagen, ich finde es so ein bisschen asymmetrisch.\nVergessen wieder, wie heißt denn das Ding überhaupt?\nIrgendwie wird es bestimmt gleich raus. Ich schaue auch nochmal kurz.\nJa, kann man irgendwie machen. Mein Film ist es irgendwie nicht so richtig.\nIst halt so Hochhaus mit viel Hotel drin und mit unten so einer fetten,\nvöllig übertriebenen Shopping Mall, wie das ja hier sowieso üblich ist.\nAlso alles mal so ein bisschen zu übertreiben, speziell die Shopping-Malls.\nUnd das ist dann halt nochmal so eine richtig übertriebene Shopping-Mall,\nwo übrigens auch Apple einen Apple-Store hat.","Ah, okay.","Und zwar haben sie den natürlich dann auch gleich,\nnicht einfach nur so in die Shopping-Mall reingebaut, weil das wäre ja zu einfach.\nJa, sondern diese Shopping-Mall-Geschichte, wie heißt denn dieses Teil,\nMarina Bay heißt das, genau, Marina Bay, also ihr könnt ja mal nach Marina Bay,\nwie heißt das Ding nochmal, Marina Bay Sands, genau, Marina Bay Sands.\nDas sind diese drei Hochhäuser, wo oben wie so eine Flunder da oben nochmal\nso eine Etage draufgelegt wurde, wo dann die eine Seite so ein durchgehender\nPool ist und auf der anderen Seite ist das alles so ein bisschen so parkmäßig\nund da können dann die Touris hochfahren.\nIch habe das nicht gemacht, mir war es das nicht wert. Und die Aussicht genießen.\nDavor ist dann nochmal so ein künstlich angelegter Park auf so einer künstlich angelegten Insel.\nEs ist alles ganz pompös und alles ganz groß und viel von allem immer so.\nAlso Singapur ist genauso wie Dubai so völlig übertrieben, weil die haben halt\neinfach die Kohle und It shows.\nAber man muss auch sagen, es ist aber auch wirklich ein interessantes Experiment.\nAlso dieses Land hat sich innerhalb von 50, 60 Jahren, mal kann man mal sagen,\ngut, sie haben ein, zwei geografische Vorteile gehabt,\naber die haben sie halt auch voll ausgenutzt und das Beste draus gemacht und\neigentlich einen ganz beachtlichen Stadtstaat dahin gelegt, nachdem sie ja eigentlich\nso von Malaysia so ein bisschen beschissen wurden.\nDie waren ja eigentlich erst Teil von Malaysia, aber dann hat Malaysia Singapur rausgeschmissen.\nDas hat immer so ein bisschen was mit so Bevölkerungsanteilen,\nchinesische versus muslimische Bevölkerung und so weiter zu tun.\nAuf jeden Fall wollten sie die irgendwie nicht haben und dann war das einfach\nnur so ein Fischerdorf, was aber ganz gut gelegen war.\nUnd daraus haben sie halt dann irgendwie diesen großen Hafen gebaut und haben\nsich dann eben mehrere sehr gut funktionierende Standbeine gemacht.\nAlso einerseits ist halt dieser Hafen, das ist so das Zwischenlager.\nDa müssen halt irgendwie alle Schiffe aus China, die nach Europa fahren, müssen da vorbei.\nDeswegen sind sie da als Umschlagplatz ganz groß. Das ist auch die größte Tankstelle der Welt.\nAlso nirgendwo wird irgendwie so viel Sprit abgegeben wie dort,\nden sie ja nicht selber liefern, sondern den sie sozusagen da nur verkaufen.\nUnd dann haben sie sich natürlich auch als Finanzplatz, als internationaler\nFinanzplatz etabliert und sind halt so zwischen den amerikanischen und europäischen\nBörsen da quasi Chef der jeweiligen Zeitzone und deswegen läuft es da ganz gut\nund deswegen haben sie eine Menge Kohle und ja, dann brauchst du halt so eine riesige Stadt.\nAber ganz ehrlich, länger als ein Wochenende trägt das irgendwie nicht,\nwenn man sich das so anschaut, es sei denn, man hat da irgendwelche Connections.\nDeswegen bin ich auch gleich weitergefahren, dann erstmal nach Malaysia und\ndanach nach Bali. Das war wiederum ganz interessant.\nDas ist also wirklich eine sehr nette Insel.\nAlles voll mit so Instagram-Menschen.","Es ist eigentlich irgendein Fleck der Erde nicht voll mit Instagram-Menschen.","Ja, wahrscheinlich nicht, aber da war das so eine Hipster-Surfer-Parade vor dem Herrn.\nAlso es sind natürlich da viele westliche Touristen. Trotzdem ist es da ganz\nangenehm, muss man sagen.\nAlso es ist schon wirklich eine nette Insel, alles sehr freundlich und da kann\nman schon sehr nett Urlaub machen.\nGenau, und jetzt bin ich aber erstmal wieder in Malaysia auf dem Zwischenstation\nund werde mich dann demnächst Richtung Taiwan weiter bewegen,\num mir das mal anzuschauen. Da habe ich nämlich Interesse dran.","Gab es denn irgendwelche, das finde ich immer spannend, so kulturelle Erweckungserlebnisse,\nwo du denkst so, ah, so macht ihr das hier, warum machen wir das nicht so? Ihr habt es raus.","Kulturelle Erweckungserlebnisse,\nja, so in den Ländern ein bisschen unterschiedlich also ich meine erstmal,\nda hatten wir ja schon so Internet,\nfunktioniert halt einfach alles so, es ist hier irgendwie so bezahlen,\ndigital, ist einfach alles vollkommen normal, Netz ist schnell gibt überall\nWLAN so in jedem Restaurant gibt es natürlich selbstverständlich überall,\nWLAN, aber wenn du jetzt mal von so ein paar Street-Hawkers, also diesen,\nStraßenrestaurants mal absiehst oder so,\nbrauchst du halt irgendwie kein Bargeld.\nUnd das ist halt alles irgendwie sehr angenehm. Also alles ist halt irgendwie\nso ganz selbstverständlich digital organisiert.\nAlles geht hier irgendwie online und das ist irgendwie so normal.\nMan hat einfach zunehmendem Maße das Gefühl, dass man einfach in Deutschland\nso richtig an der Zukunft nicht mehr so richtig teilnimmt.\nAlso die Zukunft findet statt, aber nicht gleichmäßig auf Deutschland verteilt.\nDeutschland kriegt da nicht ganz so viel ab.","Um den berühmten Douglas-Adams-Spruch.","Achso, jetzt hatte ich das mit diesem Apple Store ja gar nicht erzählt.\nWeißt du, wo dieser Apple Store ist? Der Apple Store ist im Wasser.\nIm Wasser. Das heißt, du musst dann, genau, und zwar haben sie so eine Kugel\ngebaut, muss mal Apple Store Singapur irgendwie.","Erstmal Schuhe ausziehen.","Das wäre ja mal was. Nee, also du kannst dann sozusagen unterirdisch von dieser\nShopping Mall unter dem Wasser durch und dann kommst du mit einer Rolltreppe\noben in dieser Glaskugel raus.\nOder du hast noch so einen Steg und kannst übers Wasser da so reinlaufen.\nDann bist du da irgendwie in\ndeinem Apple Store. Aber ansonsten ist es ein ganz normaler Apple Store.\nGibt wirklich kaum irgendwas langweiligeres, als sich Apple Stores anzuschauen.\nDie sehen wirklich alle gleich aus. Alle funktionieren gleich.\nEigentlich bin ich da nur reingegangen, weil ich das zufällig entdeckt habe,\ndas da eingibt und sagt mir so, ah, der ist ja ganz hübsch, den gucke ich mir\nmal von innen an, aber länger als 10 Minuten hat das auch nicht getragen.","Aber von außen ist er in der Tat ganz fancy.","Ja, das können sie ja auch, also so optisch, architektonisch,\nda haben sie immer was drauf.","Wobei der ist auch gar nicht mal so groß, wie es aussieht. Ich dachte jetzt\nzuerst, das sei so ein bisschen wie dieser S4 in Las Vegas, die neue, aber das ist sehr.","Sehr klein. Also es ist nicht so mega pompös, es ist halt nur wieder so, naja.","Genau, der Anschnitt der Kugel ist ziemlich vergleichbar zu der S4.\nÜber die müssen wir auch irgendwann nochmal reden, aber nicht heute.","Ja, ich hoffe auch noch, dass wir da nochmal technische Assistenz bekommen,\naber es gestaltet sich leider noch ein bisschen schwierig.\nMachen wir nochmal, aber nicht heute.\nJa,\nhabe ich mir jetzt gerade nicht zurechtgelegt, eine gute Antwort auf deine Frage.\nDas mache ich dann vielleicht nächste Sendung. Generell kann man feststellen,\nes geht auch alles in freundlich.\nAlso man kann so als Gesellschaft auch einfach so freundlich miteinander umgehen.","Auch keine deutsche Stärke, zumindest in Berlin.","Nee, es ist einfach, weiß ich auch nicht.\nAlso derzeit zieht mich gerade relativ wenig zurück, muss ich sagen.\nEs ist hier alles schon ganz okay so.\nNa gut. Ich werde später noch ein paar Reiseupdates geben, aber ich bin auch fleißig.\nIch bin hier nicht nur am,\nUrlaub machen oder so, sondern ich habe viel recherchiert. Ich poemiere auch\nganz viel. Kommen wir gleich noch zu.\nNaja, eine Sache, die richtig geil ist, ist natürlich Verkehr in Bali.\nDas ist natürlich für mich genau das Richtige.\nWas für ein geiles Chaos.\nAlso in Bali machst du halt am besten alles mit einem Zweirad.\nAlso mit Mopeds. So kleinen 150er Mopeds, das ist so ein bisschen der Standard.\nAutoverkehr geht halt irgendwie kaum irgendwas voran. Aber mit diesen Mopeds\nfahren sie natürlich fast alle. Und das ist so ein geiles Chaos.","Ist das schon ein Elektronen-Thema oder ist das alles noch verbunden?","Nee, leider nicht. Da ist es mit der Zukunft noch nicht so. Es wurde dann tatsächlich\nnoch ein Foto von einer BMW CE 04, die irgendwo auf Bali steht, zugetragen.\nAber leider habe ich die selber nicht gesehen. Und einmal ist auch einer mit\ndem Elektroteil an mir vorbeigefahren tatsächlich, aber alles andere ist knatter,\nknatter, ratter, ratter.\nUnd was anderes habe ich natürlich jetzt auch nicht geliehen bekommen.\nAber du leistest dir sowas, weil anders da sich fortzubewegen ist vollkommen unsinnig.\nAber dann einfach in so einem Pulk von Mopeds die einfach alle gleichzeitig\nlinks und rechts an den Autos vorbei durch jede Lücke sich durchschlängeln keine\nAmpeln, einfach alles go.\nIch kann euch sagen das ist einfach, also ich habe mich so so glücklich gefühlt\nin dem Moment, es war einfach das Beste überhaupt ein totales High habe ich\nda irgendwie bekommen in diesem Verkehr,\nnichts finde ich geiler als so voll chaotischer Verkehr, warum?\nWeil das so, das ist nicht rule-based, wie bei uns.\nDa ist es so, ja, Stratverkehrsordnung, Schilder, und so.\nUnd wir hatten ja die Diskussion schon, rote Ampeln und so sind ja eh,\nzumindest bei Fahrradfahrern, nicht besonders populär.\nDa ist dann schon so ein bisschen Anarchie. Aber es wird dann immer viel geschrien,\nalle anderen müssten halt immer alle Regeln einhalten.\nUnd dann macht man noch mehr Regeln, noch mehr Regeln, noch mehr Regeln,\nbis dann halt irgendwann gar nichts mehr funktioniert. und alles wird nur noch\nirgendwie bestraft und so und in Bali ist halt einfach scheiß drauf.\nEs gibt so ein paar Ampeln irgendwo wo so die Stadt ein bisschen sich mehr verdichtet\nbei so großen Kreuzungen und da lustigerweise auch ähnlich wie in der Türkei\nwenn du grün hast dann ist immer grün nur für eine Straßenseite und dann darfst\naber sowohl geradeaus als auch rechts und links fahren,\nund dann eine längere Phase, aber es gibt nicht so dieses Problem,\nLeute wollen rechts abbiegen, aber dann kommt auch der Gegenverkehr,\ninzwischen muss auch in der Türkei so.\nWusste ich vorher auch nicht. Aber davon abgesehen von diesen paar Ampeln gibt\nes halt einfach gar keine Ampeln, gar nichts.\nSondern wer irgendwie auf die Straße will, der muss halt irgendwo eine Lücke finden.\nUnd jede Lücke, die irgendwie da ist, wird mit einem Motorrad gefüllt.\nEs ist halt einfach automatisch immer irgendwie alles gefüllt und alles schlängelt sich durch.\nUnd du schaust dir das an und es ist einfach wie so eine Lehrstunde in Thermodynamik.\nDu hast einfach das Gefühl, es sind einfach nur so quellende Gase,\ndie sich automatisch miteinander vermischen und wieder verstreuen.\nAlles regelt sich von alleine.\nDas sind einfach nur noch so Temperaturschwankungen an Verkehrsdichte.\nUnd es ist einfach das Geilste überhaupt.","Der Chat schreibt, wer hupt, hat recht.","Da haben wir sehr viele Leute.","Aber das Interessante ist, auf Bali wird dann die Hupe aber auch tatsächlich\nso benutzt, wie sie gedacht war.\nAlso du warnst mit der Hupe, dass du da jetzt irgendwie kommst und dass du da\nbist oder was weiß ich, da kommt so eine enge, fährst um so eine Mauer,\nso eine 90-Grad-Kurve, die man nicht einsehen kann, da wird halt kurz angehubt oder so.\nAber man hupt nicht die anderen an aus Bestrafungsaspekten.\nIn Deutschland wird ja eigentlich immer nur gehupt, um Leute zu... zu bestrafen.\nNicht um irgendwie den Verkehr sicherer zu machen, sondern es ist immer Rache.","Das ist richtig.","Naja, eine Meinungsäußerung ist es.","Ja, so eine Meinungsäußerung. So ein irgendwie, ich finde dich doof und deswegen hupe ich.\nWeil ich war der Meinung, mir wurde irgendein Privileg genommen oder eingeschränkt oder so.\nUnd auf so eine Idee würde man in Bali gar nicht kommen. Dann macht man so, bip, bip, bip, bip, bip.\nUnd so, nee, ich komme jetzt hier gerade lang, pass auf, irgendwie.\nUnd keiner hat irgendwie Beef miteinander.\nAlso alle fahren wirklich wie die Berserker.\nAber alle machen das und deswegen ist es völlig okay und es fließt einfach und\nman kann einfach stundenlang geradeaus fahren und muss nicht irgendwo stehen\nbleiben und das ist toll.","Also ich kenne das aus Italien, in Florenz hatte ich den Effekt,\nals ich da selber mit dem Auto gefahren bin.\nDa hast du auch so vier, fünf spurige Straßen, wo es aber gar keine Spuren mehr\neingezeichnet sind, weil es sowieso überflüssig ist.\nUnd jeder drängelt sich so irgendwo durch, aber das Faszinierende ist,\nes passen halt auch alle auf.\nAlso man hat auch irgendwie so ein bisschen das Gefühl, du bist ja jetzt auch\nnicht alleine, sondern du bist ja jetzt auch schon Teil von so einem Mobilitätskollektiv,\nwo es keiner darauf anlegt, dich jetzt in ein echtes Problem reinzubringen.\nUnd das kann man sich in Deutschland überhaupt nicht vorstellen.\nWenn ich da auf meiner Spur bin, dann ist das hier meine Spur und wenn da irgendjemand\nanders kommt, dann soll er mir reinfahren und dann rennen, dann werden die Versicherungen\ndas schon unter sich ausmachen.\nAber ich meine, du fährst halt auch mehr motorisiertes Zweirad.\nIch bin Moped gefahren in Thailand und das fand ich schon anstrengend.\nAlso das war, glaube ich, ein Bruchteil, so hektisch wie das,\nwas du jetzt gerade beschreibst.\nAber dieses man muss erst mal irgendwie zwei Minuten warten,\nbis man irgendwie reinkommt, zumindest wenn man so ein ängstlicher Typ ist wie\nich, da hätte ich mir schon so die eine oder andere Ampel durchaus mal gewünscht.\nIch mag halt diese Knatterbüchsen überhaupt nicht. Fahrradfahren jederzeit.\nIch hätte ja super Fahrrad fahren können, aber ich mag halt einfach Mopeds nicht.\nUnd dann muss man glaube ich erstmal reinwachsen auch in diese Art des Verkehrs.\nAber ja, es funktioniert nachweislich.","Mit der Angst ist meine, ich finde ja, das ist ja genau das Problem.\nUnser regelüberfrachtetes Verkehrssystem\nschafft einfach ganz automatisch ängstliche Verkehrsteilnehmer.\nWeil alle immer davon ausgehen, dass alle sich an alle Regeln halten müssen.\nUnd wenn mal irgendeiner ein abweichendes Verhalten zeigt, dann hat keiner eine Erfahrung damit.\nUnd ich meine, ich bin wirklich wie ein Bekloppter da gefahren irgendwie und es ist völlig normal,\ndu bretterst da irgendwie mit 80 Stundenkilometern zwischen zwei Lassern irgendwie\ndurch und irgendwie fühlt es sich nie irgendwie nennenswert bedrohlich an und\ndadurch, dass du halt auch nie das Gefühl hast,\njetzt wird dich gleich irgendeiner für irgendwas bestrafen, weil du irgendein\nPrivileg genommen hast,\nwird man auch automatisch generöser. Also es ist so ein Gesamtgefühl und ich\nmag dieses Chaos und diese Thermodynamik.","Und nächstes Mal schauen wir uns auch die Unfallstatistiken an, oder?","Einen Unfall habe ich gesehen, tatsächlich. Das waren aber Autos,\ndie sind irgendwie miteinander geknallt.\nMoppets? Nix.","Gut.","So viel zu meinen Reise-Updates. So ganz allgemein.\nSo, was noch passiert?","Es hat sich ein Unfall in meinem Leben ereignet, gefühlt.","Ein Unfall in deinem Leben?","David Lynch ist tot. Das hätte nicht passieren dürfen.","Es hat sich aber abgezeichnet für die Ziegen, oder?","Es hat sich etwas abgezeichnet, in der Tat.\nAm 15.01. Das ist auch schon zwei Wochen her, drei Wochen fast und ich weiß\nnicht, wie es euch geht, man kommt jetzt ja in so ein Alter,\nwo auch einfach Leute sterben,\nzu dem man doch irgendwie einen Bezug hatte, so im Bereich Kunst, Gesellschaft,\nPolitik, was auch immer und ich habe bei mir da so ein Phasenmodell irgendwie\nhinterlegt, so Achselzucken, naja, gut, war an der Zeit,\nVersus, ach, schon schade, hätte man ja das eine oder andere noch irgendwie von erwarten können.\nUnd dann gibt es eine sehr, sehr kleine Kategorie von Leuten,\nwo es einem dann doch irgendwie an die Nieren geht.\nAlso David Bowie war so ein Moment, Steve Jobs war so ein Moment.\nAber nichts davon hat mich so aus der Kurve geworfen wie jetzt David Lynch.\nEr war jetzt irgendwie ja schon gar nicht mehr jung und das war jetzt alles\nüberhaupt nicht überraschend. Aber mir ist dann an dem Tag klar geworden oder\nich habe mir nochmal die Frage gestellt.\nGibt es eigentlich irgendeinen Künstler, Künstlerin da draußen,\ndie einen größeren Impact auf mein persönliches Leben hatten?\nUnd dann war die Antwort nein. So, es gibt also zwei ganz wichtige zentrale Pfeiler bei mir.\nDas eine ist Depeche Mode und das andere war David Lynch.\nSo, und der ist jetzt nicht mehr da.\nUnd dann habe ich mich jetzt wirklich hingesetzt und dachte so,\njetzt müssen wir es mal ganz kitschig machen.\nUnd ich habe einen Nachruf geschrieben auf David Lynch, was er mir in meinem\npersönlichen Leben bedeutet hat. Und das schmeiße ich jetzt mal einfach in die Shownotes rein.\nKeine Sorge, das werde ich jetzt nicht draus zitieren. Aber es ist schon frappierend,\nwie die sehr unterschiedlichen Filme, die er gemacht hat, in jeweils sehr unterschiedlichen\nLebenssituationen von mir,\nsehr unterschiedliche Dinge getriggert haben, aber die dann alle irgendwie sehr wichtig wurden.\nZum Beispiel Dune von 84 ist der erste Erwachsenenfilm, den ich überhaupt im Kino gesehen habe.\nVorher halt immer nur irgendwie Bud Spencer und so ein Kram,\naber da war ich elf. Der Film ist ab 16.\nMan könnte ihn auch ab 18 gut rausgeben lassen.\nUnd das war eine totale sensorische, inhaltliche Überforderung für mich.\nAber auf der anderen Seite, wenn man dann in sehr jungen Jahren sehr verstörenden\nDingen ausgesetzt ist, dann macht das mit dem Gehirn vielleicht auch das eine\noder andere Positive, glaube ich.\nUnd so ging das dann so durch die verschiedenen Filme durch.\nUnd so die relevanten Stationen habe ich einfach mal ein bisschen runtergeschrieben.\nWer auch David Lynch hinterher trauert, dort gerne mal reingucken.\nIch finde, das war ein ganz, ganz großer. Er hat fantastische Filme gemacht,\ndie man auch immer und immer und immer wieder gucken kann.\nUnd ja, nur ist er weg.","Also dieses Gefühl kenne ich ganz gut.\nWenn du sagst, es ging dir an die Nieren, wie weit ging das?\nSagst du das jetzt so daher oder hast du so eine körperliche Reaktion?","Ich hatte körperliche Reaktionen, ich habe ein paar Tränchen verdrückt.\nUnd mich dann halt viele Stunden auch mit diesem Gefühl und meinen Erinnerungen\nund meinem Kunstverständnis, also er hat auch sehr stark geformt,\nwie ich Kunst überhaupt wahrnehme.\nAlso dieses Aufschließen von Independent,\nvon Camp, von irgendwie Underground, ich würde gar nicht sagen,\ndass ich immer irgendwie jetzt Teil dieser Kunstszenen irgendwie besonders aktiv\nwar, weil es hat ein großes Verständnis und eine große Toleranz bei mir aufgeschlossen.\nSo, dass ich nicht sage, dass es alles abartig und verstört und pervers ist.\nIch denke so, ja, auch da gibt es Dinge zu finden, die irgendwie total spannend sind.\nSo, ohne David Lynch wäre ich mit Sicherheit nicht zu Christoph Schlingensief\ngekommen, beispielsweise.\nDas sind dann alles so Ketten, wo man erstmal so ein Grundlevel freigespielt,\nbekommt und da war Lynch also ganz zentral für.\nAuf was für Arten von Frauen ich stehe, hat er geprägt.\nWer damals als Teenager Blue Velvet weniger, aber Twin Peaks gesehen hat,\nwas da plötzlich für Frauen rumlaufen, das hatte so gar nichts mit dem zu tun,\nwas ich aus der Schule kannte.\nUnd jetzt gar nicht irgendwie nur optisch, sondern so dieses ganze Mindset.\nWas waren das für Wesen, die da plötzlich zu besichtigen waren?\nDas kannte man auch aus keiner anderen Serie zu der Zeit.\nUnd das sind alles so Effekte.","Da wir ja auch jüngere Zuhörer haben, sollte man vielleicht ein paar seiner\nWerke auch nochmal speziell empfehlen.\nUnd Twin Peaks ist sicherlich,\nManchmal kann man das sagen, das ist schon so ein bisschen das Bemerkenswerteste,\nwas er überhaupt glaube ich insgesamt.\nAlso es war sehr viel Bemerkenswertes dabei, ich will den Rest jetzt gar nicht\nso abtun, aber Twin Peaks hat ja gleich so mehrere neue Elemente gemacht.\nIch muss sagen, ich bin kein besonders guter Twin Peaks-Kenner.\nIch habe, glaube ich, die erste Staffel gesehen, die zweite und dann verliert\nsie es bei mir so ein bisschen und ich weiß nicht so ganz genau.\nUnd ich wollte es schon immer mal wieder komplett von vorne machen.\nAber was man festhalten kann, es war halt erst mal keine Fernsehserie wie jeder andere.\nDas kann man, glaube ich, mit Sicherheit sagen. Sehr viel Unerwartetes, sehr viel Spezielles.\nEs gibt so einen wunderbaren Mastodon-Account, der die ganze Zeit Twin Peaks\nZitate rauswirft. Das ist einfach fantastisch.","Die Eulen sind nicht, was sie scheinen und einsam bläst das Nebelhorn.\nDas ist wirklich verdammt guter Kaffee.","Und verdammt guter Kuchen auch. Also es ist so mimetisch reich,\nDie Story ist irre. Es geht ja um den Tod von Laura Palmer und ein Detektiv\nklärt es auf und so weiter.\nAber dann entwickelt sich das einfach alles komplett anders,\nals man normalerweise so Fernsehserien erwartet.\nUnd dann gab es ja auch noch ein paar Filme hinterher, die dann das ganze Mysterium\nnoch irgendwie weitererzählen und ich weiß nicht, ob sie es überhaupt abschließend klären.\nAlso definitiv Twin Peaks, wenn man es nicht kennt, das ist mal wirklich eine\nSerie, die man aufnehmen sollte, bevor ihr euch diesen ganzen modernen Quatsch anschaut.","Also das, was Twin Peaks wirklich erfunden hat, ist halt serielles Erzählen.\nDas heißt also vor Twin Peaks gab es eigentlich nicht den Gedanken,\ndass wir eine Serie, eine Fernsehserie über x Folgen, Querstrich Staffeln als\neine durchgängige Geschichte erzählen.\nDa gab es eigentlich immer so Monster of the Week oder der Fall des Kriminalkommissars\nder Woche und es gab so ein paar leise Themen,\ndie vielleicht ein bisschen weiterentwickelt worden sind, aber in Twin Peaks\nhast du eigentlich wirklich eine einzige durchgängige Geschichte und die endet\nzwar immer mit einem Cliffhanger, dadurch hast du schon ein bisschen eine Strukturierung,\naber du kannst das also auch problemlos als irgendwie 10 Stunden Film durchgucken.\nAnsonsten von der Struktur her was sollte man sich angucken und was nicht davon\nzu Twin Peaks, die erste Season würde ich sagen, das sind ja auch gar nicht\nso viele Folgen das sind glaube ich irgendwie nur 7 oder 8 oder sowas die würde\nich sagen, muss Mensch gesehen haben,\nso ansonsten fehlen einem popkulturell einfach so viele Dinge,\ndie man einfach voraussetzen muss, ja das ist halt irgendwie so wie Star Wars,\nso das muss einfach irgendwie mal und das ist auch fantastisch,\ndas tut überhaupt nicht weh, das Alter hervorragend,\nDie zweite Season kann man sich dann meiner Meinung nach ziemlich schenken.\nDas ging also ziemlich in die Binsen. Und dann hat er ja als Alterswerk,\nda war er schon 71 Jahre alt, hat er ja noch eine dritte Season hinterher gedreht.\nUnd die finde ich persönlich ganz, ganz hervorragend. 2017 war das.\nWo er also echt nochmal gezeigt hat, es steckt immer noch alles in ihm und er\nkann das immer noch alles abrufen, hat er auch mit Frost wieder zusammen gemacht.\nAlso die erste Season von Twin Peaks, die ist wirklich sagen,\nwirklich totales Pflichtprogramm.\nTja und ansonsten Filme also so die beiden großen.","Nochmal kurz bevor wir von Twin Peaks wegkommen, also auch die Besetzung war\ndamals schon, also es waren ja auch alles Leute die später noch sehr bekannt\nwurden also Kyle Mac Lachlan als Special Agent,\nDavid Bowie hat ja mitgespielt, Kiefer Sutherland hat mitgespielt David Lynch\nhat auch selber mitgespielt Laura Dern in einer ihrer ersten Rollen und,\nja, dann ein paar nicht ganz so bekannte Schauspieler, aber alles auf jeden\nFall auch sehr spezielle Personen, also überhaupt, ja,\nDavid Lynch hat immer tolle Schauspieler an den Start gebracht.","Er hat ja auch so ein durchaus Standard-Cast-Programm, so ein bisschen wie Fassbänder,\nso ein paar Lieblings Schauspielerinnen, mit denen er immer wieder viel gemacht\nhat. Laura Dern, glaube ich, drei Filme.\nBlue Velvet, also Kyle MacLachlan. Halt auch in Dune schon.\nVon Dune über Blue Velvet hin zu Twin Peaks.\nAlso da gibt es auch einfach immer so eine Konstante an Cast,\ndie man dann auch immer wieder erkennt.\nJa.","Vor dir hast du denn David Lynch geguckt in deinem Leben?","Nein.","Was?","Gar nicht?","Nee.","Auch damals in den 90ern, als es im Fernsehen Erstausstrahlung war?","In den 90ern habe ich gar nicht Fernsehen geguckt.","Uiuiui. Okay.","Oder Kino?","Kino ist auch nicht so mein Ding.","Okay. Dann musst du noch mal ein bisschen nach... Also von den Filmen...\nAlso der erste Dune war natürlich irgendwie episch hier mit Sting und so weiter.\nNicht zu vergleichen mit dem jetzigen Dune, ganz andere Abteilung,\naber halt crazy. Was ich auch hervorragend fand, war Wild at Heart.","Das ist ja mein am wenigsten gemochter Film in der Tat.\nDen finde ich, der hat mich nicht so erreicht. Also alle anderen eher.","Nicolas Cage und Laura Dern. Ich liebe ja Laura Dern. Also ich fand,\nWalt der Tat fand ich toll.\nMal Holland Drive. Habe ich nicht so richtig mehr in Erinnerung,\naber ich meine mich zu erinnern, der war auch total crazy, so Inception mäßig\nso ein bisschen. War das der oder verwechsel ich das gerade?","Ich habe den damals, als er rausgekommen ist, habe ich ihn nicht gemocht.\nIch mache gerade einen Rewatch, mit dem ich aber noch nicht durch bin.\nOh, die ersten 28 Minuten fand ich jetzt fantastisch.\nIch weiß gar nicht, was mir damals da nicht drin gefallen hat,\naber ich werde das noch rausfinden.\nKann sein, dass ich da einfach noch nicht das richtige Mindset für hatte.\nAnsonsten ich würde ja sagen, so die beiden totalen Klassiker sind halt Blue Velvet von 86.\nJa. Also sein erster richtiger, quasi verstörender, komischer Film mit Dennis\nHopper in einer unglaublichen Rolle.\nUnd was ich auch super finde, ist Lost Highway von 97. Das ist das Ganze nochmal\nso ein bisschen auf Hochglanz gedreht.\nAlles mal ausprobieren. Das kommt schon gut.\nUnd am Ende würde ich nochmal den Christoph zitieren, den Vanilla Chief,\nder hat da nämlich gepostet.\nI really hope David Lynch was able to appreciate crossing the border between life and death.\nIf someone has the capacity for that, it was him.\nAlso David Lynch beim Sterben zu gucken, das war bestimmt auch ein Event.\nGut. So gehen sie denn dahin.","Roddy, da hast du jetzt aber auf jeden Fall was auf deiner Liste.\nAlso da musst du mal schnell auf Letterboxd gehen und David Lynch mal durchklicken.","Okay.","Vielleicht wirklich mit Twin Peaks anfangen. Da merkt man sofort.","Was die Qualitäten sind. Das ist nicht nur irgendwie ganz gut oder so,\nsondern das ist wirklich außergewöhnliches Kino, außergewöhnliche Erzählungen,\nspeziell in jeder Hinsicht.\nAlso wer Freakshow mag, müsste eigentlich auch David Lynch mögen.\nSag ich jetzt mal so.\nNa gut, womit hast du dich denn beschäftigt, Roddy?","Aber ich bin irgendwie, ich weiß es nicht mehr, beim Scrollen durch die sozialen\nNetzwerke ist mir etwas in die Timeline gefallen, was sich Radiacode nennt.\nKeine Ahnung, wie man es genau ausspricht.\nDas sind Leute aus, ich glaube, Slowenien, Tschechoslowakei oder so.\nIch weiß es nicht so ganz genau. Die jetzt nochmal gucken müssen.\nNee, Zypern tatsächlich. Zypern. Und zwar stellen die ein Radioaktivitätsmessgerät her.\nAlso es gibt drei Versionen, drei Typen. Das ist nicht allzu groß.\nAlso wie würde man jetzt die Größe beschreiben?\nIst ungefähr so groß wie die Fernbedienung von einem Apple TV.","Eine kleine Fernbedienung.","Also keine richtig große Fernbedienung, sondern nur eine wirklich kleine Fernbedienung.\nDas ist kein Geigerzähler, weil es kein Geiger-Müller-Zählrohr hat,\nsondern so ein Sintilla- irgendwas-Kristall.\nDa muss ich euch jetzt die Erklärung, wie es genau funktioniert,\nschuldig bleiben. Aber im Prinzip ist das eigentliche Messding sehr, sehr klein.\nUnd was mich wirklich fasziniert hat an dem Ding ist, wie lange der Akku hält.\nAlso man kann das Ding zwei Tage anlassen und er ist nicht leer.\nDas ist irgendwie unglaublich.\nEs gibt eine App dazu, die kann also auch jede Menge.\nMan kann, weil er so eine Art Spektrum aufnimmt von der radioaktiven Strahlung,\nkann man also auch gewisse radioaktive Elemente damit erkennen.\nSo ist das jetzt Radium oder ist das Uran oder ist das sonst was.\nMan kann es als Dosimeter benutzen.","Da ist so eine Lampe dran, die blinkt jetzt immer so etwas hektisch grün.","Achso, warte mal. Das kann ich mal, ich kann mal kurz.","Oh, jetzt hat man auch den klassischen.","Jetzt hat man den klassischen Klicksound. Immer wenn ein Strahlungsereignis den Sensor trifft.","Das musst du mal auf das Mikrofon ranhalten, dass man das auch gut mitkriegt.","Es ist jetzt so gut wie zwischen dem Mikrofon und meinem Mund. Also es ist super laut.","Das ist auch jetzt nur ein bisschen gruselig.","Das ist ja nicht viel Strahlung jetzt.","Ja, das ist so die normale Hintergrundstrahlung, die jetzt hier in der Metaebene\ngenauso wie sie bei mir zu Hause ist.\nDas Ding, man kann auch Level setzen, wo er dann tatsächlich einen Alarm macht.\nIch weiß nicht, wie der klingt. Ich habe ihn noch nicht ausgelöst.\nIch habe irgendwie immer versucht, was zu finden, was vielleicht ein bisschen\nmehr strahlt, damit man mal sieht, ob das Teil überhaupt funktioniert oder ob sie einen bescheißen.\nEs gibt in Berlin angeblich eine U-Bahn-Station, wo so mit Cadmiumgelb glasierte\nKacheln oder Uranorange glasierte\nKacheln an der Wand sind, wo man tatsächlich was messen können soll.\nAber da hatte ich noch nicht die Zeit hinzufahren und zu messen.","Du musst uns jetzt aber schon noch erklären, warum.","Ich bin auch schon am Grübeln. Wie weit ist es denn jetzt mit der Nazi-Übernahme\njetzt schon gekommen? Also bereitet ihr euch jetzt schon auf einen Atomkrieg vor?","Also das Ding ist, dass vielleicht bin ich ja, nachdem Tschernobyl damals in\nmeiner Jugend war, immer noch so ein bisschen geschädigt von,\nda gibt es eine Möglichkeit, dass man,\nmachen wir das Klicken wieder aus, das nervt.\nUnd es gibt da eine Möglichkeit, dass da was passiert, was ich hinterher nicht\nspüren kann oder nicht sehen kann, nicht messen kann, wenn ich nicht so ein Ding habe.\nUnd deswegen habe ich über die letzten Jahre immer mal wieder geguckt,\nwas kosten eigentlich brauchbare Strahlungsmessgeräte.\nUnd das war immer so, es ging irgendwie immer so bei 600 Euro los für irgendwie\nso Dinger, wo man das Gefühl hatte, die funktionieren doch nicht richtig.\nDas ist doch billiges Plastik und das ist doch alles irgendwie so.\nUnd jetzt haben diese Radiacode-Leute, fängt halt bei 300 Euro an für so ein\nGerät, das ist natürlich jetzt schon so ein bisschen, wie soll ich sagen,\neinfach mal ein Gamble, wenn ich das jetzt kaufe, ist das jetzt brauchbar oder nicht.\nAber gerade deswegen bin ich so erstaunt, dass das Teil wirklich so gut ist.\nUnd ja, und dann habe ich das gesehen, habe ich dann irgendwie zwei Tage rum\nüberlegt. Dann dachte ich, ach, kauf es einfach mal ein und guck es mal.\nSo, und die App, die es dazu gibt, ist auch sehr geil, muss man sagen.\nDie kann sogar einen Track aufzeichnen.\nWenn ich also spazieren gehe, kann ich den Track-Funktion anschalten und dann\nsehe ich hinterher, wie viel Strahlung war denn wo, wo ich vorbeigelaufen bin. und so.\nUnd das geilste Hidden Feature ist, also das Teil hat so ein Display,\ndas ist so Landscape, da stehen da Zahlen drauf und so weiter,\naber man kann es halt so halten, dass das Display links ist und man kann es\nso halten, dass das Display rechts ist.\nUnd er schaltet tatsächlich das Display um.\nDie Zahlen sind immer richtig rum, egal wie rum ich es halte, weil er die dann dreht.\nUnd als notorischer Linkshänder, der sich immer beschwert, dass Dinge für Linkshänder\nnicht brauchbar sind, finde ich das natürlich absolut geil, dass du das da in\nder linken wie in der rechten Hand halten kannst.\nDas zeigt immer richtig an. Also, ähm,\nWie gesagt, ist vielleicht jetzt kein Spielzeug für jeden, aber für mich,\nder ich der Meinung bin, ich will schon wissen, was um mich herum passiert,\nwar das irgendwie dann doch das Ding.","Da auf den Weg hierhin hast du ja nochmal ausgeführt, sowas muss man immer antizyklisch kaufen.","Genau, das Teil ist, wenn der nächste große Unfall kommt, egal ob durch eine\nWaffe oder durch ein Kraftwerk erzeugt, dann wird es schwer sein, an Geräte zu kommen.\nDeswegen ist es etwas, was man kauft, bevor man es braucht.\nUnd dementsprechend habe ich dann da mal zugeschlagen und mir das hingelegt.","Der Shop ist ja auch geil. Also das Gerät, muss man mal dazu sagen, kostet 250 bis 500 Euro.\nJe nachdem, wie, ich weiß nicht, wie sensitiv das ist.","Ja, das Rechtsmodell, nenne ich es mal, also oben und unten gibt es ja nicht.\nDas hat irgendwie einen anderen Sensor verbaut, der nochmal einen ganzen Ticken,\nsensitiver ist als die anderen beiden.\nJa, der Shop hat noch jede Menge so wie Handyhüllen und sonstiges Zeichen.","Ja, das wollte ich nämlich gerade sagen. Also es gibt dann so ein Active Band.\nAlso so für die Leute, die so to go, weißt du, so strahlen. Also ich frage mich,\nwas ist das für eine Zielgruppe, wie so eine Handyhalterung,\nso einen Radioaktivitätszähler so direkt an den Arm ranbindet, so,\nfalls man, keine Ahnung, was weiß ich, im Atomkraftwerk joggen will oder so,\nich verstehe es nicht so ganz genau.\nAber noch geiler sind die Anhänger, die es da unten gibt, hast du die auch gesehen?\nEine Keychain, wo draufsteht Cäsium-137.","Ja, die habe ich auch gesehen, das fand ich ein bisschen bekloppt.","Oder die I love Cäsium-137 Keychain.","Ich glaube nicht, dass das eine Aussage ist, die ich dir vertreten kann.\nSchön fand ich den quasi Selfie-Stick, wo man das Ding vorne dran steckt und\ndann hat man die Verlängerung, dann kann man überall hinpoken und gucken. Also, ja.","Oh Mann.\nJa, der Radia-Rod.","Also nochmal Radia-Code, wenn man es jetzt deutsch aussprechen wollte,\nso heißt das Teil, für mich war das jetzt okay.\nIch, wie immer, keine Produktempfehlung.\nIch habe mir halt einen gekauft und ich finde, ihr solltet das wissen und wenn\nihr auch einen haben wollt, bitteschön.","Genau und ein Zintillator, das kommt vom Lateinischen Zintillare,\nFunkeln und Flackern, das ist einfach ein Körper,\nder so gebaut ist, dass wenn da so energiereiche Photonen oder geladene Teilchen,\ndie werden dann, die werden abgebremst, die Radioaktivität wird quasi in Licht umgewandelt.","Es entsteht ein Lichtblitz, wenn das radioaktive Teilchen abgebremst wird und\ndiesen Lichtblitz misst man dann.\nDas ist ja anders wie bei einem Geigermüller Zählrohr, wo du im Prinzip ein Gas hast,\nwo von dem Gas ein Elektron runtergeschlagen wird und weil das irgendwie in\nder Spannung ist, entsteht dann so ein Strom an Teilchen und der lässt sich\ndann als Klick hörbar machen.\nGeiger, Müller, Zählrohre sind halt sehr, sehr robust, aber halt auch nicht besonders genau.\nDas ist halt das Problem. Die Dinger sollten eigentlich relativ genau sein. Keine Ahnung.\nIch habe keine Möglichkeit, das mit einem Profigerät zu vergleichen.\nAlso insofern vielleicht ist es auch Unsinn.\nAber wenn, dann ist es gut gemachter Unsinn.","Sowas mögen wir ja hier. gut gemachter Unsinn. Das wäre Experten für.\nAlright.\nRalf, was macht das Fettiverse?","Ich habe was gemacht. Ich konnte es wieder nicht lassen.\nHabe in der Tat fast die Nacht durchge- na, gecodet, sagt man ja heutzutage\njetzt nicht mehr, die Nacht durchgepromptet, um mal etwas zu bauen.\nAlso was ist passiert? Ich habe mich irgendwie vor zwei Tagen mal wieder ein\nbisschen aufgeregt auf Mastodon, warum eigentlich jetzt im aktuellen Wahlkampf\ndort so wenig prominente,\nPolitikerinnen auch von den progressiven oder scheinbar progressiven Parteien\neigentlich im Fidiverse unterwegs sind.\nIch glaube, Aufhängepunkt war, dass hier die neue Vorsitzende der Linke auf\nTikTok jetzt so steil geht, mit dem Video aus dem Bundestag,\nwas ich in der Tat auch wirklich sehr, sehr feiere.\nAber warum ist sowas nur auf TikTok und nicht auch irgendwie im freien Internet?\nSo, dann habe ich mich halt ein bisschen aufgeregt und habe irgendwie ein bisschen\nwas gesucht und dann bin ich über eine Seite gestolpert.\nCodeberg.org ist glaube ich auch hier irgendwo so halb in Berlin ansässig,\nist ein eigentragener Verein zur Unterstützung freier offener Software.\nInfrastruktur habe ich mich jetzt noch gar nicht so intensiv mit beschäftigt. Die haben eine Seite,\nbetreiben die schon seit etlichen Jahren, mindestens vier Jahren glaube ich,\nwo sie mal sammeln, welche politischen Akteure und Institutionen eigentlich\nim Fediverse vertreten sind.\nDer großen Parteien, sowohl Einzelaccounts als auch Parteiaccounts,\nals auch nochmal Wissenschaftler, beziehungsweise Einrichtungen von Bundesbehörden und so weiter.\nZiemlich langes Ding. Sieht auf den ersten Blick erstmal ganz interessant aus.\nUnd da dachte ich so, das ist ja schön, dass das mal jemand gemacht hat.\nDa hätte ich doch jetzt ganz gerne meine statistische Auswertung drüber.\nUnd die gibt es halt nicht auf der Seite. Es hat einfach nur eine Markdown-Datei\nmit ein paar Tabellen drin.\nUnd dann sagte ich mir, naja, wir leben in 2025. Da gucken wir doch mal,\nwie weit wir in einer halben Stunde\nkommen, um da mal ein bisschen eine Datenvisualisierung drüber zu machen.\nUnd das habe ich jetzt mal gebaut. Tim, jetzt müsstest du vielleicht mal meinen\nScreen auf unserem Beamer größer machen und wir versuchen zu beschreiben,\nwas es ist. Ich werfe das auch mal in den Chat rein.\nDas ist noch nicht publiziert. Ich muss da noch ein bisschen was am Caching\ndrehen, damit ich nicht tausende von Leuten völlig nutzlos irgendwelche Instanzen\nmit Statistikabfragen beballern.\nDa muss ich mir noch das eine oder andere Clevere einfallen lassen.\nWas macht jetzt dieses Tool? Das nimmt diese Markdown-Datei und passt die durch.\nWer ist davon eigentlich noch aktiv?\nWie viele haben die Leute eigentlich jemals überhaupt gepostet im 4Diverse und\nwelcher Partei sind sie eigentlich zugeordnet? Und das Ganze wird jetzt immer\nso ein bisschen visualisiert auf verschiedene Arten und Weisen.\nWenn man das also jetzt neu aufbaut, dann schieben sich hier die Balken so ordentlich rein.\nWir sehen, wir haben insgesamt 259 Parteien- oder Einzelpersonen-Accounts und\ndann gibt es noch ein paar Institutionen-Accounts, die sind unten drunter.\nDas ist aber noch ein bisschen buggy, aber das oben für die Parteien funktioniert ganz gut.\nUnd jetzt sehen wir halt erstmal oben in einem Balken die Verteilung der Accounts\nim Fettiverse über die verschiedenen Parteien.","Also wir sehen natürlich nicht, weil wir sind ja ein Audio-Podcast.","Aber du siehst das gerade. Genau, aber wir beschreiben es, genau.\nWir sehen also jetzt einfach einen Balken, verschiedene Farben.\nSo, und dann sehen wir, oder wir hören jetzt, dass wir sehen,\ngrüne 115 Accounts, das ist viel.\nDavon auch nur ein Bot. Linke 61, davon aber 25 Bots.\nAlso Bots sind dann Accounts, die zwar da sind, aber die einfach nur eins zu\neins von einem anderen System, sei es Blue Sky oder was der Geier,\nwas halt die Nachrichten durchtunneln.\nSPD 38, zwei Bots davon.\nDie Partei, relativ prominent, mit 23 Accounts. Piraten auch nur noch sieben Accounts.\nCDU 6, Volt 5, FDP 3, Freie Wähler 1.\nUnd Bündnis Sarah Wagenknecht und AfD tauchen zumindest jetzt in dieser Markdown-Datei\nmit keinem einzigen Account auf. Keine Ahnung, ob das jetzt...\nOb irgendwie so moderiert ist, dass die dort nicht erscheinen wollen oder ob\ndie wirklich keinen einzigen Fediverse-Account haben, man weiß es nicht.\nSo und unten drunter habe ich nochmal einen Zeitstrahl gebaut,\nwo man sehen kann, wann sind die jeweiligen Accounts eigentlich dem Fediverse beigetreten.\nUnd da sieht man jetzt relativ schön die beiden quasi Musk-Events.\nZum einen, als er zum ersten Mal sagt, ich hätte Interesse mal Twitter zu kaufen.\nDas war im April 2022, da ist die erste Beitrittswelle und die zweite Beitrittswelle,\nals er es dann wirklich gemacht hat.\nDas war dann im Oktober 2022 des selben Jahres.\nUnd man sieht also, dass es dort eine starke Ballung gibt. Man kann oben auf\neine Partei draufklicken, bekommt dann den Zeitstrahl nur von dieser Partei angezeigt.\nIch kann mal einen Screenshot davon wieder in den Chat reinwerfen,\nwie es jetzt beispielsweise nur bei den Grünen aussieht.\nDas haben wir so. Wir haben unten drunter dann einen Freitextfilter,\nwo man beispielsweise mal gucken kann, ich tippe mal Berlin ein.\nWelche Politiker-Accounts haben denn irgendwas mit Berlin in ihren Metadaten drin hängen?\nUnd wir haben dann bei den Einzelaccounts eine Zuordnung, welcher Partei sind\nsie angehörig mit ein bisschen Logik dahinter, wo sich das daraus ableitet.\nDas funktioniert jetzt glaube ich ganz gut, wie ich das reingedrechselt habe\nund wir haben, das finde ich dann fast den entscheidenden Faktor,\nwie viele Leute haben die insgesamt schon gepostet und wie viele haben sie aber\nauch innerhalb der letzten 60 Tage gepostet.\nDas heißt also, lebt dieser Account eigentlich noch?\nUnd das ist etwas, was ich jetzt so als aktiv bezeichne. Also jetzt ist Bundestagswahlkampf.\nAlso wenn die Leute jetzt nicht aktiv sind, wann dann?\nDa finde ich also 60 Tage jetzt schon sehr generös.\nUnd dann sieht man halt die doch etwas deprimierende Erkenntnis von den 259\nAccounts, die da sind und die irgendwas gemacht haben,\nsind in den letzten 60 Tagen nur 57 aktiv gewesen. Das sind gerade mal 22 Prozent.\nDer Rest sind Karteileichen, wenn man so will.\nDas ist etwas betrüglich, aber ich finde das mal so visualisiert und durchsuchbar\nmal zusammen zu haben, finde ich vielleicht mal ganz hilfreich.\nJetzt überlege ich noch, was ich damit mache, ob ich das ganze Framework dann\nnochmal erweitere, dass man da auch andere Datenquellen hinterhängen kann.\nBeispielsweise halt irgendwie Uni-Accounts oder Bibliotheks-Accounts oder was auch immer.\nUnd man so ein bisschen den Eindruck dafür bekommt, so okay,\nwie entwickelt sich eigentlich hier das Fediverse, wenn es eben um solche zentralen Player geht.\nJournalistinnen, gibt es auch diverse Listen von, weiß ich.\nMal gucken, ob ich mich damit noch ein bisschen weiter beschäftige.\nUnd ich werde das, denke ich, am Wochenende mal auf GitHub abwerfen.","Ah, okay, gut. Das wäre jetzt meine nächste Frage. Das läuft jetzt noch nirgendwo\naußer auf deinem Computer.","Genau, das läuft jetzt im Moment lokal. Und ich bin jetzt quasi eine Stunde\ndavor, es so generisch zu haben, dass es auf einer GitHub-Page läuft.\nDas ist ja immer so mein beliebtester Publikationskanal, so gar nicht mehr eigener Web-Server oder sowas.\nEinfach so bauen, dass es als GitHub-Page geht.\nAber ich will halt, dass dort dann ein vernünftiges Caching aktiv ist und alle\n24 Stunden werden die Daten aktualisiert.\nWeil im Moment ist es so, dass wirklich dann halt jetzt die ganzen Server angefragt\nwerden und mit sauberen API-Requests, alles völlig innerhalb der Spielregeln,\nhole ich mir jetzt halt die Statistik-Daten raus, aber das willst du halt nicht\nbei jedem Browser-Aufruf von jedem Nutzer machen.","Was kommt hier für Technologie zum Einsatz bei deinem Tool?","Das ist so das Übliche, sprich HTML, JavaScript, Bootstrap, jQuery,\ngenau. Genau. Wobei beispielsweise...","Also finden Sie alles im Browser statt sozusagen?","Alles komplett browserbasiert, ja.","Und die Daten holt er sich dann woher?","Aus der Markdown-Datei, wirklich von dem Server hier auch. Also der Codeberg.org\nhat hier auch einen Git laufen.","Achso, da sind jetzt nicht nur die Accounts drin verzeichnet,\nsondern da sind auch die ganzen Posts drin verzeichnet.","Nee, da sind nur die Accounts drin. Alles andere hole ich mir dann von da aus.\nAlso es wird dann also quasi eine kleine interne Datenbank aufgebaut.\nSo, okay, das stand in der Markdown-Datei drin und dann werden die Schritt für\nSchritt abgearbeitet und mit weiteren Statistiken aufgefüttert.\nUnd damit will ich es aber auch nicht übertreiben. Es soll jetzt auch nicht\nirgendwie ein Pranger werden oder sowas und ich finde auch im Fediverse zu viel\nStatistik ist auch irgendwie nicht so toll.\nAlso da muss ich auch gucken, dass ich da eine vernünftige Balance irgendwie,\nhinkriege. Aber alleine so, wie es jetzt schon ist, finde ich das schon ganz eindrücklich.","Sieht ganz fancy aus, aber es sollte natürlich dann nicht nur auf deinem Laptop\nlaufen, von daher spute dich da mal.","Ja, das wollte es für heute fertig haben, aber habe jetzt schon bis 5 Uhr heute\nNacht und dachte, ein paar Stunden Schlaf brauche ich jetzt noch.\nWir kommen noch dazu, gebaut mit Cursor und Sonne 3.5, keine einzige Zeile selber\ngeschrieben, wie üblich.\nUnd das war allerdings jetzt auch von der Genese her wieder ein bisschen hakelig.\nAlso so ein Pattern, über das wir auch schon häufiger gesprochen haben,\ndie erste halbe Stunde läuft sehr, sehr smooth und geht sehr gut.\nUnd wenn es dann spezieller wird, dann muss man schon auch ein bisschen Hirnschmalz\nreinstecken in das Prompting und Debugging.\nAber im Großen und Ganzen lief es recht geschmeidig.\nAlso das ist jetzt das Ergebnis von ungefähr vier, fünf Stunden,\ndie ich da rein investiert habe.","Nicht schlecht, sieht ganz aufgeräumt aus irgendwie, aber sowas geht jetzt mittlerweile,\ndas ist echt geil dass man jetzt einfach mal so, da können wir gleich nochmal\ndrüber reden, ich finde diesen Effekt wirklich absolut mind-boggling,\nich weiß nicht wie du darüber nachdenkst mittlerweile Roddy, aber mich haut das um.","Es ist ein Tool also wenn ich nur eine halbe Stunde weit komme,\nohne mit dem Tool kämpfen zu müssen, dann ist das noch Kinderschuhe.","Aber ich finde, es sprengt einfach die Wand zwischen so, jemand müsste mal und ich mach mal selber.","Ja, genau.","Und wenn man irgendwie, wie ich, ständig Ideen hat, ist sowas halt schon eine mächtige Waffe.\nJemand, mir nicht sehr wohlgesonnen ist, aus dem professionellen Arbeitsumfeld,\nhat irgendwo mal auf seiner internen Liste geschrieben, wo er nicht wusste,\ndass ich da mitlesen kann.\nNiemals darf man dem Stockmann PowerPoint in die Hand geben.\nDamit macht er jede Sitzung platt und drückt alles durch.\nUnd heute würde es wahrscheinlich sagen, niemand darf dem Stockmann Cursor mit\neiner LM dahinter in die Hand geben.\nEs gibt einfach Technologien, auf die ich gut reagiere und die gehört definitiv dazu.","Da bin ich ganz bei dir und das ist auch gerade bei mir eigentlich auch so das Ding.\nBleiben wir doch vielleicht gleich mal bei dem Thema, weil ich ja jetzt hier\nauch quasi mir vorgenommen habe.\nJa, was habe ich mir vorgenommen? Ich habe mir vorgenommen, hier Land und Leute\nein bisschen kennenzulernen, ein bisschen Urlaub zu machen und ich habe mir,\nvorgenommen, viel zu programmieren.\nAlso so mein Sport programmieren. Kennt ihr noch diese Aufkleber,\ndie Leute da hinten so an die Autos dran machen?\nMein Sport und dann Tennis oder sowas. Und dann So einen Aufkleber hätte ich\nauch gerne. Mein Sport und Laptop und Coding.\nEinfach so als Freizeitsport. Nicht als Beruf, sondern so als Sport.\nUnd das finde ich tatsächlich in dem Zusammenhang wirklich sehr interessant.\nDa sollten wir vielleicht mal kurz auf Cursor zu sprechen kommen.\nWeil wie du auch, geht es mir auch. Und ich muss sagen, als du das erste Mal\nankamst mit irgendwie, ich habe mir jetzt mal hier was zusammengebaut mit deinem Mastodon.\nDie Mastowall. Die Mastowall, so.\nDa war ich ja schon noch etwas skeptisch.\nAber ich muss sagen, diese Skepsis habe ich mittlerweile komplett abgelegt und\nbehaupte das Gegenteil.\nDa sind wir einem...\nEiner Zeitenwende stehen wir gegenüber. Die findet jetzt gerade statt und zwar massiv.\nUnd das ist genau das, was du gerade so beschrieben hast. Es ist auf einmal\ndiese, wie nennt man das,\nso eine Wand eingerissen worden zwischen Leuten, die quasi über Systeme kreativ\nnachdenken, im Sinne von es müsste mal geben das und wäre es nicht toll,\nwenn wir dieses Tool hätten.\nUnd den tatsächlichen Werkzeugen, die man benötigt, um so etwas auch umzusetzen,\nwas ja in der Regel immer so ein bisschen Experten braucht, die dann aber vielleicht\nmit dieser Idee nicht so viel anfangen können oder die nicht so verstehen oder\nkeine Zeit haben oder zu teuer sind oder alles Mögliche.\nUnd man stand dann immer so ein bisschen doof da. Jetzt kann man natürlich sagen,\nwieso streng dich doch ein bisschen an.\nAber es ist halt so, man steckt ja oft mit seinem Kopf, mit seiner Tätigkeit,\nmit seinen Interessen, mit seiner Zeit in ganz anderen Kontexten und auch anderen\nLebenswelten und Erfahrungswelten.\nAber genau da gebiert man ja dann diese Ideen und sagt so, man müsste das mal\nautomatisieren, man müsste hier mal gucken, man müsste hier mal eine Auswertung\nmachen und so weiter und da bräuchte man ja eigentlich nur das und das.\nDas kann doch so schwer nicht sein.\nUnd dann setzt man sich halt irgendwie ran und verbringt dann irgendwie erstmal\ndie ersten zwei Tage, in denen man auf irgendwelche komischen Syntax-Errors\nschaut oder Konfigurationsdateien, einfach nur um sein Environment erstmal an\nden Start zu bekommen und dann bricht natürlich irgendwie die Zeit dann auch komplett weg.\nUnd jetzt mit den AI-Tools,\nläuft das auf einmal ganz anders. Da gibt es diese Einstiegshürde mittlerweile fast gar nicht mehr.\nAlso so ein Projekt anlegen und mal so grundsätzlich.\nMittlerweile investiere ich einfach die Zeit in Nachdenken und schreibe dann\ndas Konzept erstmal auf.\nUnd wenn man dann irgendwie da diese Coding Agents draufwirft,\ndann wird es wirklich spannend.\nAlso umso akkurater man beschreiben kann, was man eigentlich haben will, was es tun soll.\nIm Prinzip so ein White Paper, was früher mal keiner lesen wollte,\ndas schreibt man da jetzt irgendwie rein und nimmt das sozusagen schon mal als\nDiskussionsbasis und sagt so, hier, liest du das mal durch, das ist irgendwie\ndas, wie ich denke, wie das funktionieren sollte,\njetzt helfen wir mal hier irgendwie Datenbank dafür aufzubauen und den ganzen\nanderen Kram zusammen zu bekommen und das funktioniert.\nAlso das hat seine Grenzen und läuft man auch mal gegen eine Wand und da wird\nauch ein bisschen zu viel durcheinander halluziniert, Aber das ist alles völlig egal.\nAlso der Produktivitätsfortschritt ist real und ist so schnell und so massiv\nund so krass, dass wenn man da mal jetzt ein bisschen Zeit investiert und sich\nquasi mit dieser Wesenslage,\ndieser Systeme ein bisschen beschäftigt und die Erfahrung mal jetzt ansammelt,\num da überhaupt erstmal umzugehen,\ndann sind die Ergebnisse halt wirklich krass.\nUnd Cursor, gut, das ist jetzt das, was ich halt viel verwendet habe und auch\nimmer noch verwende, mir kommt das irgendwie entgegen.\nEs gibt andere Systeme, die ja auch schon erwähnt wurden, wie Z oder natürlich\nhier Co-Pilot und so weiter. Das ist eigentlich ziemlich egal.\nMittlerweile finden diese Systeme überall Anklang und so.\nNur Cursor scheint mir irgendwie so das Beste zu sein, um mal so eben durchstarten\nzu können mit egal, was man will.\nJa, und die Verbesserungen sind einfach täglich zu sehen.\nUnd ich verlinke mal so einen Blog-Eintrag, der vielleicht jetzt Roddys Gedanken\nganz, ganz gut auch wiedergibt,\nweil das, sagen wir mal, für erfahrene Programmierer natürlich auch eine Gefahr\ndarstellt, so zu programmieren.\nAlso während wir jetzt so, Ralf und ich, so oft nicht die Zeit hatten,\njetzt viel kurz zu machen, weil wir uns irgendwie mit anderen Sachen beschäftigt\nhaben, aber die ganze Zeit auf die Ideen gekommen sind für Systeme,\ndie man mal so brauchen könnte,\nist es natürlich jetzt für Programmierer, die so richtig im Thema sind,\nihr System beherrschen, die Betriebssysteme beherrschen, die APIs kennen und\nsagen wir mal, auch wirklich so.\nJedes Semikolon, was am falschen Platz ist, auch einfach sofort erkennen.\nUnd wenn man dann mit AI-Tools arbeitet und sich da helfen lässt,\ndann läuft man natürlich Gefahr, diese Erfahrung, diese Hardcore Einschleifung\nund dieses krasse Training, was man ja im Prinzip hat, zu verlieren.\nEs ist so, als ob jetzt irgendwie ein 100-Meter-Sprinter irgendwie die ganze\nZeit die Straßenbahn nimmt.\nUnd da muss man natürlich aufpassen, dass man an der Stelle seine Fähigkeiten nicht verliert.\nUnd dieser Blogpost hier von hier ist der Typ nochmal, weiß ich jetzt gerade\ngar nicht, steht das hier irgendwo bestimmt.\nAuf jeden Fall beschreibt er, wie er halt irgendwie auch mit Cursor gearbeitet\nhat und dann einfach schnell in so eine Situation reingekommen ist,\nwo er einfach dann nicht mehr verstanden hat, was der Code macht,\nnicht mehr diesen Lerneffekt, diesen Trainingseffekt hat und jetzt zwingt er\nsich halt regelmäßig dazu irgendwie so zumindest mal einen Tag die Woche die\nTools nicht zu benutzen, sondern\nsich wirklich mal jede Fehlermeldung durchzulesen. Das ist super tempting.\nDu hast jetzt irgendein obskures Problem, irgendwas wird falsch berechnet,\njetzt könntest du stundenlang da in diesen Code eintauchen, so hat man das normalerweise gemacht.\nUnd jetzt gehst du einfach an den Körper und sagst so, ja, bau mal irgendwie\njede dritte Zeile in ein Debug-Statement ein, schmeiß mal alle Variablen raus,\ndann läuft dieser Code einmal kurz nochmal durch, erzeugt irgendwie so ein fünf\nKilometer langes Text-File, dann pastest du das da wieder rein,\nja, lies mal durch, sag mir mal was Falsches, so, oh ja, jetzt sehe ich schon\ndas Problem, hier haben wir irgendwie ein Plus und kein Minus und deswegen funktioniert\ndas alles nicht und ich habe das jetzt mal korrigiert und dann so,\nah ja, schön, dass wir darüber gesprochen haben, jetzt nehmen wir die ganzen\nDebug-Statements wieder raus und so,\nja, okay, jetzt habt ihr alle wieder jetzt entfernt, jetzt alles wieder wie\nvorher, aber jetzt berechnet er richtig, so, Turnaround-Zeit irgendwie 30 Sekunden.\nUnd der Hasel läuft,\naber es ist klar, dass man dann eben das verliert. Ich wiederum.\nSehe es von der anderen Seite. Ich kriege dadurch dieses Training erst,\nweil ich halt immer wieder vor dieser Problematik stehe, so scheiß,\nwas brauche ich jetzt hier für eine Architektur, was brauche ich jetzt hier\neigentlich für Tools, was brauche ich für eine Library,\nwie setzt man denn das jetzt auf, wie muss denn meine Architektur sein,\nwie muss denn jetzt hier meine Module verteilen,\nbrauche ich jetzt hier einen Supervisor oder nicht, all so eine Frage.\nUnd jetzt habe ich die Möglichkeit, auch mal eben drei, vier,\nfünf verschiedene Ansätze auszuprobieren, um dann am Ende zu sehen,\nso, ah, okay, das ist jetzt wahrscheinlich der Winning Move.\nUnd dann kann ich mir den Code danach anschauen und sagen, so,\nah, okay, alles klar, so wird das jetzt hier gemacht. Also ich lerne dadurch,\ndass ich so die Möglichkeit habe, auszuprobieren.\nIch schmeiße einfach ganz viel Farbe an die Wand und schaue,\nwas am besten danach aussieht und kann dadurch halt auch einen Lerneffekt erzielen.\nAber es ist natürlich eine Gefahr für Super-Experience-Programmierer,\naber es erschließt das Programmieren einer Gruppe von Leuten,\ndie vielleicht bisher bestenfalls Projektmanager gewesen sind.\nUnd während die Projektmanager sonst viel Zeit damit verbracht haben,\nin irgendwelchen Meetings oder in irgendwelchen Whitepapers anderen Leuten zu\nerklären, was sie eigentlich haben wollen.\nOder Projektmanager ist vielleicht gar nicht das richtige Wort.\nWie nennt man sowas? Projektdesigner oder so? Systemdesigner?","Also in der Scrum-Logic sind das Product-Owner. Die also wissen,\nwas das Viech hinterher eigentlich mit welchen Prioritäten können soll und was\nsinnvolle Zwischenschritte dahin sind.","Genau. Und die auch, sagen wir mal, ein Gefühl haben, warum muss es das überhaupt\ngeben und was sollte es nicht sein und all diese ganzen Sachen,\ndie man sonst immer wieder überprüfen muss.\nUnd jetzt ist man in der Lage, zumindest aus eigenen Kräften einen Prototypen\nzu bauen. Also es muss ja noch nicht mal das finale System sein.\nAber man kann sich halt hinsetzen, so wie du das jetzt gemacht hast,\nes müsste ja mal so einen Activity-Trigger geben. Das kann doch nicht so schwer sein.\nSo, und dann bam, bam, bam, bam, bam. Und dann, du hast Running Code.\nIch meine, der ist jetzt noch nicht veröffentlicht, aber ich habe es ja eben\ngerade gesehen. Das sieht irgendwie schick aus, es tut irgendwie, was es soll.\nMuss man sicherlich nochmal ein bisschen debuggen und nochmal ein paar Augen\ndrauf werfen, aber im Großen und Ganzen ist der Großteil der Arbeit schon getan.\nUnd ich sage auch einfach, warum nicht?\nWeil dann ist das halt vielleicht die neue Programmiersprache und die neue Programmiersprache heißt Deutsch.","Ja, es gibt da so ein, zwei Punkte, wo ich immer wieder ins Grübeln komme.\nMan hat ja immer so, das gibt es in der Kommunikationswissenschaft auch irgendwie\ndiverse Theorien zu, dass man selber mit seinem Erfahrungshorizont ja immer\nso das Zentrum des Universums ist.\nUnd wir sind die letzte Generation, die noch.\nUnd wir waren die erste Generation, die noch. Und nach uns kommt ja nur noch Mist.\nIst das nicht so wahr wir sind alte weiße Männer, Punkt das können wir jetzt\neinfach mal nicht wegdiskutieren ähm,\nTrotzdem, es gab jetzt auch wieder die Wochen so ein paar ganz lustige Meme,\nso diese verschiedenen Generations von Generation X über Z und Y und wie sie nicht alle heißen,\ndass wir halt so die Generation sind, die noch das vorher kennen,\nals es noch keine Computer gab und dann haben wir miterlebt,\nwie die plötzlich alle kamen und dann haben wir erlebt, wie dieses ganze Touch\nkam und jetzt haben wir auch noch irgendwie die ganze KI an den Backen.\nWir haben aber halt noch den ganzen Hintergrund von, was die ganzen Zwischenschritte davor waren.\nUnd es gibt durchaus etliche Beobachtungen, die sagen, dass die Jugend von heute\neben noch nichtmals mehr einen Laptop bedienen kann, weil die nur noch Touch\nwollen und machen und sagen so,\nall das, was ich in meinem Leben brauche,\nkriege ich auf dem iPad und auf dem Smartphone hin.\nIch brauche überhaupt keinen Laptop. Was soll das?\nDiese Maus, diese Zeiger, dieses komisch.","Schön fand ich auch den Post, wo jemand meinte, es ist total ungerecht,\ndass wir die Generation sind, die sowohl unseren Eltern die Drucker fixen.","Als auch unseren Kindern.\nJa, also ich werde das gar nicht zu tief elaborieren, aber ich,\nalso was ich jetzt auch heute Nacht wieder sehr stark gemerkt habe,\nich hätte nach ungefähr 20, 30 Minuten wäre ich stecken geblieben,\nwenn ich nicht einigermaßen leidlich programmieren könnte, selber.\nUnd auch nicht nur eben so ein bisschen Birdseye-View-Wissen im Sinne von so,\ndas sind jetzt so die gängigen Frameworks, sondern eben schon auch ein bisschen\neinen Sinn dafür habe, so, warum kommt jetzt folgender Fehler und wo würde jetzt\nhier ein Debugging, was ist überhaupt ein Log,\nja, warum ist es überhaupt sinnvoll, einen Log mitschreiben zu lassen, irgendwie so.\nDas sind einfach Dinge, die musst du halt irgendwann irgendwie mal gelernt haben.\nUnd für mich ist wirklich eine der großen Fragen der nächsten Jahre.\nOb jetzt so ein Tool wie Cursor noch so viel besser wird, dass das auch nicht\nmehr notwendig ist, sondern wo also das Verständnis darüber,\nwie man funktionierenden fehlerfreien Code schreibt,\nnoch mal quasi die letzte Schippe\ndraufgelegt wird, dass man diese ganzen Debugging-Optimierungsprozesse\nauch nicht mehr braucht. Und das ist für mich eine ganz offene Frage.\nWenn man mich also vor, habe ich auch schon mal ziemlich wortwörtlich so in\nder Sitzung gesagt, also ich konnte mir im Bereich KI immer viel vorstellen,\naber ich finde es nach wie vor verstörend, dass gerade Programmierung so gut damit geht.\nIch hätte gedacht, dass das so eine der letzten Bastionen ist,\ndie nie oder ganz spät erst fallen werden. Und jetzt ist es mit das Erste, was umgeworfen wurde.\nIst das aber jetzt auch schon auf einem Plateau oder wird das auch noch den\nentscheidenden letzten Schritt besser, dass die Menschheit in zehn Jahren wirklich\nnicht mehr programmieren können muss, weil das komplett die KI macht?","Also ich habe ja da immer dieses Zitat von Steve Jobs im Hinterkopf, Bicycle for the Mind,\nwo er immer gesagt hat, der Computer ist the bicycle of the mind,\nalso dieses Ding, was einen fürchterlich viel schneller macht.\nIch glaube, dass diese Programmier-KIs in eine ähnliche Kategorie fallen,\nweil du musst am Ende des Tages immer noch wissen, was ist sinnvoll, was will ich erreichen,\nwas soll das Teil tun können und so weiter und auch ...\nWie sind die Einzelteile strukturiert und so weiter. Also am Ende ist so eine\nKI immer ein Textprediktor und der kann dir nur Dinge schreiben,\ndie schon mal jemand vorher geschrieben und veröffentlicht hat.\nSonst wird es nicht funktionieren. Und ich glaube, ich glaube,\nab einer bestimmten Komplexitätsklasse hast du immer Software,\ndie von Hand geschrieben wurde, damit die überhaupt von der Maschine erst mal gelernt werden kann.\nUnd ich glaube nicht, dass diese Handarbeit je weggeht.\nAlso das, was man früher so schön herabwürdigend Boilerplate-Code genannt hat,\nist etwas, was wahrscheinlich sehr stark automatisiert wird.\nUnd was wahrscheinlich auch sehr stark automatisiert wird, ist diese ganze Frage,\nwelche Programmiersprache benutze ich eigentlich?\nIch glaube, das ist sowas, was sehr stark in den Hintergrund treten wird.\nOb die KI jetzt die oder die oder die Programmiersprache ausspuckt,\nist dann nicht mehr so richtig wichtig.\nAlso gerade Nischenprogrammiersprachen werden da extrem drunter leiden.\nWenn du nicht den Mindshare von der AI hast und diese Programmiersprache schreiben\nkannst, dann wird die AI das auch nicht können.\nWenn die das nicht kann, dann verschwindet die Sprache, weil niemand mehr die\nSprache beherrschen wird.\nEs gibt keine neuen Beispiele, wie es funktioniert und, und, und.\nUnd vielleicht geht es sogar so weit, dass wir irgendwann dann auch kaum noch\nneue Programmiersprachen sehen werden.\nWeil die natürlich keiner benutzt, weil es keine AI gibt, die diese Programmiersprachen kann.\nAlso ich glaube, da geht der Zug hin.\nDie Frage ist natürlich erreicht die AI in dem was sie tut eine Komplexitätsstufe,\ndass der Mensch das nicht mehr versteht, was die AI tut,\ndann ist handgeschriebene Software natürlich raus, das ist ganz klar also ob\nwir das erreichen werden oder nicht, keine Ahnung kann jetzt jeder raten,\nes würde mich nicht wundern, wenn es passiert aber dann ist es natürlich klar,\nja dann bist du als Programmierer raus aber gut, es gibt auch Menschen,\ndie sowas schreiben, was andere dann nicht mehr verstehen,\nmanchmal passiert das einfach so Ich glaube.","Da ist viel Wahres dran, an dem was du gesagt hast, aber es gibt wahrscheinlich\nsogar noch ein paar andere Strahlen die hier noch mit reinleuchten können,\nes ist zum Beispiel durchaus vorstellbar, dass wir demnächst IIs haben,\ndie Programmiersprachen entwerfen werden.\nOder dass sich sozusagen überhaupt ein neuer Layer zwischen Computern und AIs\nbildet, also dass quasi die AI schon fast zum Betriebssystem wird.\nDass wir dieses ganze, wir benutzen ein Betriebssystem, was irgendwie für die\nProgrammierung durch eine Programmiersprache gedacht ist,\ndass das komplett wegfällt und wir eigentlich nur noch so einen einzigen Processing-Blob\nhaben, der irgendwie freigeistig seine Daten durch die Gegend schaufelt,\ngesteuert eben durch Sprache.\nDas ist sicherlich auch so ein Punkt. Andererseits bin ich auch ganz bei dir,\nwenn du sagst, das normale Programmieren ist immer noch erforderlich und das\nist auch absolut richtig.\nAlso ich hätte jetzt zum Beispiel meine ganzen Sachen auch nicht machen können,\nwenn ich nicht schon eine relativ klare Vorstellung von Programmieren habe.\nUnd ich glaube auch, dass ich nochmal sehr viel besser mit einer AI arbeiten\nkann, umso besser ich poemieren kann.\nAlso umso mehr man überhaupt das Poemieren als solches verstanden hat,\naber auch die Poemiersprache zu können.\nMan muss auch in der Lage sein, Fehler zu finden, man muss auch in der Lage\nsein, eine richtige Richtung anzugeben.\nMan kann nicht einfach auf eine Baustelle gehen und anfangen,\nirgendwie den Leuten ihre Arbeit zu erklären und sie durch die Gegend zu schicken,\nwenn du einfach Bau nicht verstanden hast.\nDu brauchst immer noch ein Domänenwissen. Und das ist beim Programmieren nur\nmal die Programmiersprache und das Wissen um Betriebssysteme und Caching und\nNetworking und Protokolle und all dieser ganze Kram.\nAlso da ist schon noch eine ganze Menge drin zu lernen.\nUnd ich würde halt jetzt auch niemandem ein Projekt überlassen,\nDer sagt ja, ich habe keine Ahnung, aber ich habe da jetzt mal eine halbe Stunde\nreingequatscht und da kam ja schon irgendwas Brauchbares bei raus. Ich kann jetzt alles.","Ja, vor allen Dingen, ich sehe es viel stärker als den kontinuierlichen Trend\nder weiteren Abstraktion der Programmierung.\nAlso in den 80ern hat man durchaus noch Assembler programmiert,\nman hat C programmiert, was im Prinzip auch Assembler war und die Abstraktion\nweg vom Prozessor wurde immer größer und immer größer.\nUnd inzwischen glaube ich, dass viele junge Programmierer nicht so genau wissen,\nwie eigentlich der Compiler oder der Interpreter jetzt das, was sie da schreiben,\nhinterher in etwas umsetzt, was tatsächlich auf dem Prozessor läuft.\nOb das okay ist oder nicht, ist eine andere Diskussion. Ich denke nur, dass dieser Trend,\ndas auszudrücken, was ich programmieren will durch Abstraktionen,\neinfach nur jetzt eine neue Stufe erreicht.\nDie Stufe ist dann halt gesprochene deutsche Sprache mit all ihren Ungenauigkeiten.\nAlso ich habe es halt auch ausprobiert und hatte das Gefühl,\nich habe eigentlich kaum Kontrolle oder Einfluss darauf, was die Maschine tut,\nweil ich nicht in der Lage bin, klar zu sagen, mach mal das und das und das und das nicht.","Welches Modell hast du genommen dafür?","Das weiß ich nicht mehr, es war einfach Kaser. und ich sage,\nmach mal das und das und das und dann kam irgendwas bei rum,\nwo ich sagen würde, ja, okay, kann man als Lösung meiner Aufgabenstellung interpretieren.","Also hast du wahrscheinlich Chat-GPT benutzt, kann das sein? Also war das 4O?","Das war Cursor, keine Ahnung, was da eingestellt war.","Ja, okay, also wenn du es nicht eingestellt hast, ist die Wahrscheinlichkeit\nrecht groß, dass du das benutzt hast. Das ist übrigens auch noch so ein Punkt, also,\nAI ist nicht AI. Das merkt man wirklich bei dieser Cursorsache extrem.\nAlso ich habe mich jetzt, du benutzt ja auch so nett, ne Ralf?","Ja, nichts anderes kommt mir hier in die Pompa.","Das ist auch wirklich, glaube ich, derzeit der Goldstandard,\ndas muss man wirklich sagen.\nUnd es ist erstaunlich, wie sich die Interaktion ändert, wenn man ein anderes Modell nimmt.\nAlso wenn man jetzt irgendwie DeepSeq oder irgendein 4O, O1-Mini-Tralala-Open-AI-Modell nimmt,\ndann ändert sich die gesamte Denke und die ganze Interaktion.\nAlso das ist auch, man benutzt nicht einfach nur AI in dem Moment,\nsondern man fährt sich auch in gewisser Hinsicht auf ein Modell ein.\nAlso man muss auch lernen, so ein bisschen, wie soll man das nennen?\nEs ist so eine kulturelle Begegnung. Also man muss wirklich dieser Intelligenz\nmit Interesse begegnen und die kulturellen Eigenarten erlernen.\nDas ist wie ein anderes Land zu bereisen, so ein bisschen. Da musst du auch\nerstmal so ein bisschen die Sprache lernen.","Also ich habe es gerade geguckt, ich habe auch Sonnet benutzt.","Ah, okay. Das ist drei Filme, so eine drei Filme.","Ja, dass es halt hier drin ist.","Da sind ja mehrere Sachen drin. Du kannst da in den Prefs klicken und dann die\nModels auswählen, wenn du mal so rechts oben wirst.","Ja, ich weiß, das ist da, vor allem\ngibt es ganz, ah, ich kann dann hier umschalten. Da gibt es sehr viel.","Jetzt mittlerweile ist auch einiges nachge...","Ja, ja, ja, ich sehe es schon hier. Die Psyk ist auch mit drin, das sehe ich gerade.\nJa, aber ich habe ziemlich sicher so nett benutzt. Trotzdem ist es halt so,\nalso gerade so grafische Oberflächen, das ist halt schwierig zu beschreiben,\ndass es so aussieht, wie man es eigentlich haben will.","Ja, gut.","Ich habe jetzt nicht versucht zu sagen, mach mal hier noch drei Pixel weniger\nund da noch drei Pixel mehr, aber.","Würde ich auch sagen, GUI ist das, wo noch am meisten schief geht,\nweil da halt, da kommt glaube ich auch diese Erkenntnis drin,\ndass da halt dann dem, der KI auch das Auge fehlt.\nAlso man kann ja Screenshots auch, ich weiß nicht, hast du das schon mal gemacht,\nTim, einen Screenshot irgendwo rein posten und zu sagen, guck mal hier,\ndas sieht falsch aus. Das habe ich noch überhaupt nie ausprobiert,\nob das eigentlich auch geht.","Ich weiß, dass das jetzt auch ein neues Feature ist, aber ich habe das noch nicht benutzt.","Also weil da fruckele ich immer am meisten. Ich habe ja noch ein anderes Projekt,\ndas stelle ich in einem der nächsten Sendungen mal vor, was ich da gebaut habe.\nDa geht es viel darum, dass das so eine Ich-möchte-dran-lecken-GUI wird.\nUnd das ist auch schon nicht so einfach.\nGerade so dann doch verschachteltes CSS. Ich habe jetzt hier wieder Bootstrap\ndrunter, wo ich mittlerweile denke, das war eigentlich ein Fehler,\nweil wenn du irgendwo von den Standard-Bootstrap-Komponenten abgehst,\nhat Sonnet echt Probleme, sich dann im Hierarchie-Modell durchzusetzen gegenüber den Standards.","Tailwind, Freif, Tailwind.","Tailwind, okay.","Tailwind ist the shit. Naja gut,\ndieses,\nWir werden ja eh gleich nochmal drauf kommen, über Dipsig reden.\nIch habe aber nochmal kurz einen Nachtrag zu Cursor, weil da tut sich ja schon\nwieder der nächste Kram auf.\nHabt ihr schon mal was von MCP gehört?","MCP glaubt nicht...","MCP steht für Model Context Protocol und das ist eine Erfindung von Anthropic,\nalso der Hersteller von Sonnet, von dem Modell, also von Claude und dem Claude\nSonnet, das Modell, was wir jetzt gerade besprochen haben.\nUnd Model Context Protocol ist, wie der Name sagt, ein Protokoll,\nwas quasi definiert, wie eine AI auf Daten zugreifen kann.\nUnd das ist so ein Client-Server-Protokoll und man kann das im Prinzip in beide Richtungen benutzen.\nDas heißt, man kann für ein Modell definieren,\nwie dort Daten reinkommen, indem man es als Client benutzt, aber man kann vor\nallem für Daten ein Server-API, hat man jetzt sozusagen für Daten ein Server-API,\nman kann also ein Server für Datensilos schreiben,\num einer AI darauf Zugriff zu bekommen.\nBeispiel. Das Beispiel, was sie auch gleich mitgeliefert haben.\nAlso sie haben das so im letzten November haben sie das vorgestellt und mittlerweile\nist es jetzt in Cursor drin.\nAlso wenn du jetzt Cursor updatest in den Settings, mal guckst,\ndann hast du quasi MCP-Server, die da mehr oder weniger schon mit supported\nwerden oder MCP-Server werden supported und das ist noch ein bisschen,\nfitzelig, aber worauf ich hinaus wollte ist, sie haben halt was gebaut für Systeme\nwie Google Drive, Slack,\nGitHub, Git,\nPostgres und Puppetierer weiß ich gar nicht, was das ist, um ehrlich zu sein.\nSoll heißen, du kannst über MCP eine AI Zugriff geben, zum Beispiel auf Postgres.\nUnd das sieht dann so aus, dass du dann sagst, so eine Postgres-URL gibst mit\nhier über diesen User-Login, AddHost, AddPort, tralala,\nmit diesem Pfad und hier benenne ich die Datenbank, kannst du quasi direkt einen\nZugriff geben auf die Datenbank über das MCP-Protokoll.\nUnd dann redet das Ding mit deiner Datenbank. Und dann kannst du quasi in deiner\nrelationalen Datenbank genauso rumrecherchieren und deine AI spielen lassen,\nwie du das eben vorher mit normalen Files gemacht hast, mit Textfiles gemacht hast.\nAlso du kannst halt eine dicke, fette Datenbank nehmen und sagen, analysier mal das.\nAber du kannst halt genauso auch auf GitHub, Git generell, also auf Git-Repository,\nin ein Slack hinein Dinge erfragen.\nDas ist ziemlich mindblowing.\nGenau. Und solche Systeme kommen jetzt irgendwie raus.\nDas wird ganz spannend werden. Habe ich jetzt noch nicht so viel mit rumgespielt.\nIch habe mal diesen Postgres-MCP einmal so zum Laufen bekommen.\nDa muss man irgendwie so NPM installieren und so weiter.\nLuca hat mir gesagt, wie ich das machen muss und ich habe es gemacht und es\nhat dann irgendwie funktioniert. Aber dann hatte ich irgendwie auch gerade wieder keine Zeit.\nUnd das ist auch in dieser Cloud-Desktop-App drin, dieser MCP-Server-Support.\nKann man auf jeden Fall demnächst noch einiges erwarten.","Insofern ganz spannend, als dass das jetzt nochmal ein anderer Ansatz ist,\nals das, was wir ja schon länger mit diesen RAC-Modellen haben,\ndieses Retrieval Augmented Generation.\nWir erinnern uns, das ist was, was wir hier bei unserem FAB-KI-Chatbot gemacht\nhaben, wo man sich mit unserem Bibliothekskatalog unterhalten kann.\nDa wird aber eben nicht quasi ein direkter Datenkonnektor zu unserer Datenbank\ngebracht, sondern wir haben ja unsere Datenbank mal in so einen semantischen\nVektorraum dafür überführt, in dem sich so eine KI dann wohlfühlt. Ja.\nDen gewissen Problemen und Einschränkungen, die das mit sich bringt,\ninsbesondere was halt so diese Exaktheit, die man ja eigentlich dann ganz gerne\nin der Datenbank dann doch auch nicht verlieren möchte, drin hat.\nAuf der anderen Seite kann ich mir jetzt vorstellen, aber das müssen wir wirklich\nmal ausprobieren, dass jetzt so ein MCP-Ansatz natürlich dazu führt,\ndass man dann wiederum die Stärke dieses Assoziierens nicht so stark drin hat,\nwie wenn das quasi in einem reinen Vektorraum modelliert wird.\nAlso sprich, ich finde jetzt Dinge, die ähnlich sind, obwohl sie in der Datenbank\nerstmal keine direkte Verbindung über irgendein Datenmodell drin haben.\nAlso weil sie ähnlich klingen oder über drei Ecken dann doch irgendwo miteinander\nverwandtschaftet sind. Da muss man mal drauf gucken.","Ja, kann ich dir wenig zu sagen, habe mir das Protokoll jetzt ehrlich gesagt\nselber noch nicht angeschaut, was das alles kann und abstrahieren kann,\naber kannst dir ja vorstellen, wenn es in der Lage ist, sowas wie Postgres oder Git abzubilden,\ndann ist es sicherlich auch in der Lage, deine Bibliotheksdatenbank abzubilden\nund wenn du jetzt quasi dir mit der AI morgen einen MCP-Server dafür programmierst,\ndann kannst du vielleicht übermorgen schon mit Sonnet auf deiner Bibliotheksdatenbank,\nrumrecherchieren.","Das mag sein, aber die olle Recherche habe ich ja schon.\nAlso erinnere dich an das Beispiel, ich habe als Kind ein Buch gelesen,\nwo dreibeinige Aliens die Erde erobert haben. So, was war denn das?\nDas steht nicht in der Datenbank. Aber,\nChatGPT weiß halt, das waren die dreibeinigen Herrscher von wie hieß er noch, weiß ich nicht was.\nWeil das also quasi eben dann gekoppeltes Weltwissen ist. Aber das weiß er nur, weil er...","Wieso steht das nicht in der Datenbank? Wieso stehen die Bücher nicht in der Datenbank?","Die Bücher stehen schon so drin, aber die Leute stellen halt Fragen,\ndie nicht zu unseren Bibliotheksdaten passen.","Wieso? Welche Daten fehlen denn, wenn ihr die Bücher da nicht drin habt?","Die Bücher heißen halt irgendwie die dreibeinigen Herrscher. Das sagt dir nichts.","Dass das Außerirdische ist. Habt ihr die Bücher selber auch in der Datenbank?\nHabt ihr da nicht ein PDF, wo das...","Nein, natürlich nicht. Dürfen wir doch nicht. Wo denkst du denn hin?\nWir sind Deutschland hier. Urheberrecht. Raubmordkopierer.","Wieso? Urheberrecht ist doch noch relevant, wenn du es wegkopierst. Nein, nein.","Da streiten wir uns seit 15, 20 Jahren mit den Verlagen rum.\nDas deutsche Urheberrecht erlaubt es uns nicht, die Volltexte unserer Bücher selber.\nSo ohne weiteres.\nWir kriegen die auch gar nicht von den Verlagen. Wir müssten alles selber scannen\nund OCRen und alles ein Elend. Don't get me started.","Das ist doch echt zum Kotzen.","So, aber selbst dann hast du es ja auch nicht irgendwie wirklich gut für eine LLM.\nAlso mein Grundverdacht wäre,\ndass das super ist, um quasi jetzt sowas wie Lucene oder Solar abzulösen.\nAlso so eine klassische Suchmaschine obendrüber. Aber um jetzt so eine eben\ndann doch eher so frei assoziierende Dinge miteinander in Verbindung setzende\nLLM-Suche zu simulieren,\ndass dafür die Zeit wahrscheinlich nicht reicht, weil es muss ja alles irgendwie in Echtzeit passieren,\nohne dass er sich schon lange und tief Gedanken über das Gesamtkonvolut gemacht hat.\nAber es ist jetzt alles gefährliches Halbwissen, wir werden uns das angucken.","Genau, wir werden uns das angucken, genau.\nSo, wo wollen wir denn jetzt hin?\nAchso, ich habe noch ein paar Geräte-Updates, weil ich ja hier mit neuem Gerät\nunterwegs bin, auf meiner Reise.\nUnter anderem habe ich mir für die Reise ein Rodecaster Duo besorgt der Rodecaster\nDuo ist quasi die kleine Version vom Rodecaster Pro 2,\nden ich ja ganz toll finde und zu dem ich irgendwie auch auf dem letzten Kongress\nwie schon erwähnt nochmal so einen kleinen Talk gemacht habe und ja der viele\nVorteile hat auch ein paar Nachteile, zu denen ich mich ja auch schon geäußert habe.\nJetzt habe ich mir diesen Duo geholt, der deshalb Duo heißt,\nweil er eben nicht vier Mikrofoneingänge hat, sondern zwei.\nBeziehungsweise genau genommen hat er nicht zwei Mikrofoneingänge,\nsondern ich rede von zwei XLR-Mikrofoneingängen.\nDenn das Gerät hat noch ein paar Besonderheiten. Ich hatte ja auch im Sendegate\nmal diesen Rodecaster 2 Review gemacht.\nDas können wir hier auch nochmal verlinken.\nUnd ich habe dem meine Gedanken zum Rodekaster Duo jetzt noch hinzugefügt.\nAlso sozusagen eine Aktualisierung.\nIch packe mal den Link auch nochmal in die Shownotes.\nGenau, im Wesentlichen, also vier statt sechs Sliders, Slider ein bisschen kürzer,\nsechs statt acht Buttons zum Abspielen, Displays genauso groß,\npaar andere kleine Sachen.\nEine Sache hat sich verändert, die zwar noch keine positiven Auswirkungen hat,\naber was durchaus eine Forderung von mir gewesen ist für ein künftiges Update\nvon dem Rodecaster Pro 2 ist nämlich, dass die Lautstärkeknöpfe jetzt Soft-Buttons sind.\nAlso die sind sozusagen keine manuellen Potentiometer mehr mit einem festen\nAnschlag, sondern sind so frei, wie nennt man denn sowas, so Rotatoren,\nso freidrehende, beliebig weitdrehende Knöpfe, die so einen LED-Ring drumherum haben.\nMit anderen Worten, man kann da jederzeit per Software auch sagen,\nwelchen Wert sie haben, theoretisch.","Das ist gut.","Und das wäre natürlich fantastisch, wenn man das auch von außen tun könnte,\ndazu bräuchte natürlich dieser Rodecaster mal eine richtige MIDI-Implementierung,\ndie er nicht hat, sondern das ist halt nach wie vor dieses Krücken-Blödsins-\nMIDI-Implementierung, die sie da reingebaut haben,\nüber die ich mich schon mal total aufgeregt habe, aber man kann jetzt auf jeden\nFall die Dinger drehen, man kann übrigens, wie ich gerade feststelle,\nman kann sie auch drücken, aber ich habe keine Ahnung, was das macht.","Tue es jetzt nicht.","Doch, ich habe schon gedrückt. So ein Familienbenutzer ist das.\nAber ich habe keine Ahnung, was das soll.\nVielleicht kann man es damit irgendwie auf einen definierten Wert oder sowas\nsetzen, aber das ist hier zumindest gerade nicht der Fall.","Als nächstes will er eine Space-Maus an dem Ding.","Ah, man kann sogar den großen kann man ja auch drücken. Ging das bei dem alten Rundcaster auch schon?\nWow, damit kann man ja hier durch...","Ralf, drück doch mal.","Oh, okay, das muss ich nochmal, da habe ich jetzt gerade was entdeckt.\nNaja, also auf jeden Fall kleine Verbesserungen und eine Eigenschaft hat er\nauch noch, er hat nämlich vorne tatsächlich einen Miniklinkenanschluss.\nIch dachte, was soll das denn? Weiß ich auch immer noch nicht genau,\nwas das soll, aber man kann dort quasi so ein iPhone-Headset,\nso ein altes, ein analoges reinstecken und hat einerseits einen Kopfhörer,\naber auch einen Eingang.\nAlso man kann sozusagen auch nochmal einen Plug-in-Power Mikrofon noch als dritten\nEingang auch nochmal mit reinmachen in diesen Rodecaster, der eigentlich nur\nzwei Mikrofoneingänge hat, nämlich dann trotzdem noch einen dritten.\nIch habe keine Ahnung, was das soll, das ist einfach nur nochmal so ein Monokanal natürlich.\nMan kann den quasi zum Abhören benutzen, zum reinquatschen da steckt man halt\nnicht volles XLR Material rein sondern eben wie gesagt nur so ein kleines Plug-in-Power-Teil,\naber ja ist auch noch mit dabei,\nalso merkwürdige Designentscheidungen noch immer eine Software, die,\nsich verbessern muss, aber,\nRodecaster Duo is a thing und für wen jetzt zwei Mikrofoneingänge reichen, durchaus eine Option.","Also dieser Eingang vorne, der ist pures Gold,\nweil da ist ja so eine sogenannte Plug-in-Power mit dran, 3 Volt oder 5 Volt\nund da kann man halt beispielsweise das von mir so wertgeschätzte V-Moda Boom Pro dran betreiben.\nDer spirituelle Nachfolger von dem HMC 660.\nDas ist ein Kleinmembran-Kondensator-Mikro für 35 Euro, was fantastisch klingt,\nrobustes Gegenbrauchen und sonst was. Man braucht dann noch einen Kopfhörer dazu.\nIch habe in die Show Notes mal reingeworfen, welche günstigen Kopfhörer sich\nda gut zu eignen. Und dann hast du für 60 Euro ein Top-Headset.\nUnd dann kannst du das da dran anschließen, ohne weitere Adapter.\nUnd hast ein Dreier-Device. Das ist doch cool.\nAber du hast nur zwei Kopfhörerausgänge dann, oder? Das heißt also,\ndu musst einen der Kopfhörerausgänge dann splitten für die dritte Person?","Nein, nein. Du hast ja einen vollständigen, vierpoligen Stecker.","Ah, okay. Und wie machst du dann da Ja, ist schon klar, so TRS oder wie das\nheißt, aber wie ist dann darüber die Lautstärke geregelt?","Na, das ist halt so einer dieser Softkanäle, den kannst du dir theoretisch auf\neinen der vier Slider legen.","Ah, okay.","Also entweder auf einen der vier,\nphysischen Slider oder einen von den drei virtuellen Slidern,\ndie im Display noch sind.\nAlso das ist tatsächlich eine Option.\nWie das klingt, ob das was kann, keine Ahnung, habe ich nicht ausprobiert.\nIch habe so ein Ding nicht mehr rumliegen, so ein iPhone-Dings,\ndas habe ich also entsorgt.\nIst mir nur aufgefallen. Ich weiß nicht genau, was überhaupt der Sinn der ganzen\nSache sein soll, also warum sie da sowas jetzt einbauen und warum vorne?","Ich glaube, weil viele so Gamer-Headsets und so, kannst du einfach anstecken und fertig.","Ja, aber warum vorne? Also warum dann nicht hinten?\nWie alle anderen auch. Also das ist mir so ein bisschen unklar.\nAlso ich konnte mir das nur so vorstellen, dass das mehr so ein,\nPodcast-Situation, zwei Männer unterhalten sich und ein Assistent überwacht die Aufnahme.\nSo klingt das für mich so ein bisschen.","Also es ist der Studio-Kopfhörer dann für den Mixer.","Ja, das ist halt nur jemand, der so ein bisschen mithören will,\nfunktioniert noch alles, aber nicht professionell aufgenommen wird,\nvielleicht mal was reinquatschen kann.\nKeine Ahnung, also mit der Logik hätten sie natürlich auch noch zwei davon machen\nkönnen oder ganz viele, weil die ja fast gar keinen Platz verbrauchen.\nMir ist es so ein bisschen unklar, wo sie das Produkt jetzt genau sehen,\nmit diesen Änderungen, warum sie jetzt hier diese digitalen Regler haben,\nwo vorher die analogen Regler waren für die Lautsprecher, ohne dass sie jetzt\nda irgendeinen Nutzen draus ziehen.\nIch würde da wirklich gerne mal auftauchen irgendwie. Vielleicht fahre ich im\nHerbst mal nach Australien und frage die mal selber, was da ist.\nDie sitzen ja da irgendwo rum.\nSo wie zum RodeCaster.","Der ist immer noch so ein bisschen zu teuer. Jetzt in der Zweier-Version liegt der so um 460 Euro.\nAußerdem so ein Huni weniger.\nNaja.","Sie sagten doch gleich was?","Ich brauche es halt überhaupt nicht, aber es ist irgendwie so ein Teil,\nwas man eigentlich gerne hätte.\nDas liegt jetzt aber noch über der Scheiß drauf. Das wirft man da jetzt mal\nab, das Geld. Dafür sind dann 460 Euro.","Von Stream Deck gibt es ja jetzt dieses Teil, was auch Drehknöpfe hat,\nso Rotatoreingangsteile.","Genau, der Streamer X, glaube ich.","Das L oder so ähnlich heißt das Ding. Nee, der ist es nicht, von Streamdeck selber.\nUnd dafür gibt es jetzt auch ein Interface,\nwas du da noch hinten ranklemmen kannst und dann hast du im Prinzip halt für\ndeine Streaming-Anwendungen hast\ndu halt deinen Mikrofoneingang und deinen Kopfhörerausgang und so weiter.\nEs ist aber auch alles eigentlich zu teuer dafür, was es tut.","Und auch nur für einen einzigen dann?","Ich glaube nur für einen, ja.","Von welchem Produkt redest du gerade?","Streamdeck XL. Das hat noch so vier Drehknöpfe.\nFür dieses Ding gibt es noch, wenn du auf der Seite direkt guckst,\ndann gibt es für das Teil noch ein Audiointerface, was du so hinten ranschraubst.","Okay.","Aber es ist nicht der Stream Deck XL.","Das hier meinst du, oder?","Oder XL, oder wie heißt der Stream Deck Pro?","XL ist das ganz alte, was ich auch noch...","Ne, das ist ein Yamaha.","Das ist ja was anderes.","Alles klicktaktisch.","Der Neo, der hat auch Knöpfe.","Also es ist so einer, der hat so vier Drehregler.\nUnd so ein etwas kleinerer, der hat so vier Drehregler. Und da kannst du hinten\nnoch ein Audio-Interface ranklemmen.\nStream Deck Plus genau so heißt der.","Der hat noch ein Audio-Interface hinten dran. Echt? Wieso zeigen sie das hier nicht?","Nein, es gibt ein Audio-Interface, was du hinten ran machen kannst.","Und das ist aber auch von Elgato?","Das ist auch von Elgato. Wenn du auf der Elgato-Seite guckst und nicht die Google-Bildersuche,\nzu random Webseiten klickst, dann...","Ich bin auf der Elgato-Webseite.","Aber ich weiß nicht genau, wo ich jetzt hingucken soll. Von welchem Produkt\nredest du denn jetzt? Sag doch einfach mal.","Was ist das hier wieder für ein hektisches...","Mobile? Nee.","So, jetzt haben wir das XLR-Dock für Stream Deck Plus.","Genau das.","Ah, okay.","Ja, weiß ich nicht, ob das eine sinnvolle Erweiterung ist.","Also es ist jetzt ein XLR-Eingang, Kopfhörer, Mini-Klinke.","Und dann hast du im Prinzip für eine Person Audio-Interface und kannst dann\ndie Regler vorne dran direkt verwenden, um diese...","Aber da bist du dann auch schon 380.","Ja, ich sag ja, es ist zu teuer. Aber haben würde man es schon gerne wollen.","Das will ich nicht haben.","Kannst du mal die Dinger mal verlinken? Ich finde irgendwie diese Produkte nicht.","Ja, ich hab das jetzt was.","Ja, naja, es gibt da so einiges. Es gibt auch von Rode, ich weiß nicht, hatten wir,\nhatten wir da eigentlich drüber gesprochen, über den Rode-Videos? videocaster.","Ich glaube nicht.","Das kann ich nicht erinnern.","Nee, hatten wir nicht. Ich hatte mir das nämlich mal, Rodecaster Video,\ngenau, so heißt das Ding nämlich.\nUnd da wünschte ich mir fast, ich würde mehr mit Video machen. Mache ich aber nicht.\nDeswegen habe ich mir das nicht gekauft.\nIch hatte das Ding kurz mir mal angeschaut, weil ich mir dachte,\nvielleicht ist das ja ein Ersatz für meine HDMI-Switching-Möglichkeit. War es dann aber nicht.\nAber das Produkt ist wirklich irre.\nAlso der Rodecaster Video, das ist jetzt ein Review, ohne dass ich das Ding selber angefasst habe.\nIch habe mir nur irgendwie diverse Videos, Reviews darüber angeschaut und ich\nbin wirklich beeindruckt, was sie da zusammengebaut haben. Das ist ein irres Produkt.\nAlso das ist so ein, wie groß ist der in etwa, weiß ich nicht,\nso breit vielleicht wie ein normaler Rodecaster Pro.","Also halb bis 19 Zoll breit und vielleicht eine ganze Höhe in eineinhalb hoch.","Ja, ich müsste mir die Specs jetzt mal anschauen. Also im Wesentlichen hast\ndu da nur so zwei Tastaturreihen drauf und ein paar Regler und ein kleines Bildschirmchen.\nUnd hinten hat das Ding aber eine Brigade an Anschlüssen.\nDa ist also mal nicht weniger als sechs HDMI-Ports, sechs USB-C-Ports, Ethernet,\nzwei analoge große Klinken für Kopfhörer, zwei analoge große Klinken für den\nMonitor Out und nochmal zwei Mikrofoneingänge.\nAlso die haben da wirklich geklotzt.","Hast du mal ein Ding?","Ja, sicherlich.\nDer Moment, jetzt hast mich ein bisschen aus der Spur gebracht. Ich mach das mal eben.\nIch hab schon so. und,\nkann ich euch nur empfehlen, euch mal so ein paar Reviews anzuschauen,\nwas das Ding alles kann. Das ist wirklich irre.\nWofür ist es gedacht? Das ist, wenn man jetzt quasi mit Video raussendet.\nAlso so typischerweise YouTube-Videos macht, dafür ist es im Wesentlichen gedacht\nund du möchtest alles irgendwie in Realtime machen.\nUnd du kannst also ganz viele HDMI-Quellen anschließen, also von diesen 6 HDMI-Ports\nsind 4 Eingänge und 2 sind Ausgänge.\nDas eine benutzt du als Primärausgang, das andere benutzt du so als Monitorausgang.\nUnd diese USB-C Ports, die es da gibt, die sind teilweise auch nochmal nur dafür\ngemacht, ein Mikrofon oder eine Webcam anzuschließen.\nOder dann eine Festplatte zum Abspeichern oder nochmal einen anderen Computer.\nUnd natürlich auch Stromversorgung darüber. SD-Karte kannst du auch noch reinstecken,\nalso es ist wirklich alles so ein bisschen da.\nSprich, du kannst das Ding in so einem HD, 4K,\nWebcam Dings anschließen und ganz viele HDMI-Quellen noch dazu schalten und\ndu kannst dann eben über die wenigen Tasten vorne so all die ganzen Quellen live umschalten,\ndu kannst dir mit der Rode-Software aber auch direkt am Gerät auch so Screens\nzusammenbauen, wo du sagen kannst okay, jetzt will ich einen Bildschirm haben\nund rechts oben soll die Quelle sein, links unten die Quelle so, was weiß ich,\nhier Bild von mir selber mit der\nKamera und dann halt der Rechner und dann nochmal hier ein HDMI-Device.\nAlso du kannst das alles hier zusammenbauen und so fertige Setups einfach abrufen.\nUnd es bedient sich alles ganz super und es ist wirklich ein tolles Gerät.\nUnd außerdem hast du noch Audio noch mit dabei.\nAlso das ist schon wirklich heftig, was die da bauen.\nUnd ich muss sagen, Rode ist da auf einem sehr guten Weg.\nAlso wenn sie ihr Midi-Game nochmal auf die Kette kriegen, dann wäre ich so richtig begeistert.\nAber das ist definitiv cool.\nDas könnte auch interessant sein für Leute, die einfach oft so an Videokonferenzen teilnehmen.\nUnd was zeigen. Weißt du?\nAlso kannst du ja auch eine eigene Banderole einblenden und deinen Namen einblenden. Keine Ahnung.\nUnd dann hier so, jetzt schalte ich mal auf den Rechner und jetzt schalte ich\nauf die Präsentation und jetzt zeige ich mich selber.\nUnd jetzt habe ich rechts oben\nmeine Präsentation und links sieht man aber trotzdem noch mein Gesicht.\nAlso wenn man wirklich viel online, live in irgendwelche Zoom-Meetings rein\npräsentiert und dabei irgendwie schick rüberkommen will,\nohne viel Aufwand zu haben und ohne mit einer komplizierten Software arbeiten\nzu müssen, dann ist der RodeCaster Video richtig Bombe.","Kostet auch noch schlanke, 1100 Euro.","Ja, aber das ist natürlich für so einen Videomixer eigentlich nichts.","Ja, eigentlich nicht, das stimmt.\nWow.","Genau, den wollte ich nochmal empfehlen. Wie gesagt, nicht selber handgetestet,\nweil ich nicht so die Anwendung habe, Aber ich fand das Gerät total beeindruckend.\nDann habe ich ja noch ein anderes Gerät hier an der Mache, nämlich die Apple\nVision Pro vom DOM, die ich ja hier mit auf Reisen genommen habe.\nUnd jetzt habe ich ja mal so richtig Zeit, das Ding ausführlich zu testen.\nIch will jetzt gar nicht so viel erzählen, weil so viel Neues gibt es jetzt nicht.\nIch will nur andeuten, dass ich meine Meinung, glaube ich, in ein paar Bereichen überarbeiten werde,\nweil ich schon auf einmal mehr Nutzen gefunden habe, als ich dachte.\nIch habe den jetzt halt vor allem so als großen Bildschirm quasi dabei,\nohne einen großen Bildschirm dabei zu haben und genieße sehr diesen neuen Mac\nMonitor Modus und das ist also gerade so zum Programmieren ist das irgendwie\nziemlich cool, also wenn man wirklich so Sessions macht und jetzt nicht nur\nauf so einen 13 Zoll Bildschirm starren will, sondern,\nsehr viel Platz für den Editor haben will und nochmal Platz für irgendwelche\nVideos und Dokumentationsseiten und hier noch ein Terminal, was mitläuft,\ndann ist es wirklich grandios, weil seit diesem 2.2 Update,\nwo du diesen Riesen, also aus deinem Rechner sozusagen so einen Riesenmonitor\nmachen kannst, ist es wirklich krass.\nIch habe mir dann auch nochmal diese Lesebrilleneinsätze dafür noch gekauft,\nin einer relativ starken Variante.\nDas ist für mich immer noch sehr schwierig, dass alles so einzusortieren,\naber ich habe so, ich setze gerade so ein bisschen auf einen Gewöhnungseffekt.\nAlso Sachen, die einem am Anfang sehr störend vorkommen, stören einen nach einer\nWeile einfach nicht mehr so sehr, beziehungsweise man lernt so ein bisschen damit umzugehen.\nAlso zum Beispiel so dieses Unschärfe bei der Bewegung und dass da immer so\nein kleiner Delay drin ist, durchaus etwas, woran man sich gewöhnen kann.\nUnd ich habe da muss ich dir übrigens nochmal danken, Ralf, das hast du mir\nzwar nicht direkt gesagt, aber ich habe das so am Rande mitbekommen,\ndu hast über diese Halterung, diese Bügelhalterung geredet, die manche sich da dran machen,\ndieses Gewicht mir auf die Stirn verlagert wird, das habe ich kurz vor der Abfahrt,\nhabe ich mir noch so ein Ding noch geholt und das ist tatsächlich,\nmacht es einen Riesenunterschied aus und auch so diese,\nIdee mit, man ist gar nicht mehr direkt an der Kamera dran, sondern lässt das\nDing einfach nur so ein bisschen vor sich schweben, ist auch durchaus etwas, was man benutzen kann.\nAlso ich habe da noch keine abschließende Meinung zu allem gebildet,\naber ich bin auf jeden Fall gerade sehr intensiv dabei, das alles zu überprüfen\nund nochmal mir dann eine neue Meinung zu bilden.\nUnd zwei kleine Sachen, die mir noch aufgefallen sind, die wir so wie noch nie besprochen haben.\nWenn man in diesem Macintosh-Desktop-Modus arbeitet, greift übrigens auch dieses Universal Control.\nIhr erinnert euch, Universal Control, man hat mehrere Macs nebeneinander und\nkann dann irgendwie mit der Maus von dem einen auf alle anderen gehen und vice versa.\nAlso man kann immer mit einem Rechner, den man gerade bedient,\nalle anderen mitsteuern.\nAlso du gehst halt einfach auf einen anderen Bildschirm und kannst mit der Maus\nvon Rechner A die Apps auf Bildschirmen von Rechner B steuern.\nWeil die Maus einfach rüber marschiert.\nUnd das Irre ist, das ist mir auch erst jetzt aufgefallen, das geht halt auch\nin der Apple Vision Pro mit den Apple Vision Pro Apps.\nDas heißt, wenn ich jetzt den Macintosh auf der Vision Pro habe und ich habe\ndie Maus, dann kann ich mit der Maus den Mac verlassen und in die Apps von der Vision Pro gehen.\nDie ja entweder Native Vision Pro Apps sind oder iPad Apps sind,\naber ich kann tatsächlich mit der Maus die Vision Pro benutzen.\nBin ich gezwungen, alles damit zu machen.\nDas geht natürlich nur, wenn du den Mac hast, sozusagen, aber da verheiraten\nsich auf einmal so Konzepte, das könnte nochmal ganz interessant werden.","Und Tastatur-Eingabe wahrscheinlich dann auch, oder?","Ja, genau. Tastatur-Eingabe natürlich auch.","Weil das ist ja, finde ich, das größte Manko der Vision Pro Text-Eingabe.","Ja, ja. Kannst dann auch in Apple Vision Pro Apps mit der Tastatur reintappen,\ngenau. Und ja, irgendwie man will langsam den Mac einfach in diesem Teil drin haben.\nIch kenne auch jetzt, ich habe von Leuten gehört oder Videos gesehen,\ndie sagen so, ja, ich habe jetzt eine Vision Pro und einen Mac Mini.\nUnd der Mac Mini ist so mein Reisecomputer. Der hat aber voll die Power.\nIch brauche aber keinen Bildschirm dafür. Ich brauche nur eine Tastatur und eine Maus.\nUnd die habe ich halt irgendwie noch so mit dabei. Und dann nehme ich die Vision\nPro und das ist dann mein Bildschirm.\nAlso das könnte noch ganz das könnte noch ganz lustig werden.\nJa. Und dann, ach ja genau, und eine App, die habe ich noch entdeckt,\ndas muss ich noch erzählen, die heißt Magic Room.\nDie tut eigentlich wenig, kostet trotzdem 18 Euro, aber ich musste es kaufen.\nUnd das ist wirklich crazy.\nDie Apple Vision Pro hat ja so einen LIDAR-Sensor, ähnlich wie das iPhone.\nDein Gesicht abtastet, genauso taste diese apple vision pro den raum um dich herum.\nGewinnt also die ganze Zeit so ein dreidimensionales Profil und damit realisiert\nsie halt solche Funktionalitäten wie dieses Durchbrechen der Virtual Reality\noder deines Environments,\nwenn du halt eigentlich nur auf generierten Content schaust,\naber irgendjemand nähert sich halt deiner Brille, dann merkt die Brille so,\nah, da ist irgendetwas, da kommt eine Person und dann reißt an der Stelle quasi\ndie Illusion auf, damit du dann diese Person siehst.\nUnd genauso dann wird dann auch diese Eyesight aktiv und dann weiß dann die\nPerson, dass du sie siehst und all diese ganzen Tricks, die die Vision Pro so\nmacht und genauso natürlich, um zu verhindern, dass man gegen eine Wand läuft,\nhast du halt dieses Lida-Ding da irgendwie mit drin.\nUnd diese Funktionalität, auf die können Apps aber auch zugreifen und das tun sie auch ganz gerne.\nMan braucht da irgendwie zusätzliche Permissions und diese App,\ndie ich jetzt da gekauft habe, Magic Room, die benutzt also diesen LIDAR-Sensor,\num in den Raum reinzuschauen.\nUnd erhält quasi in Realtime ein dreidimensionales Abbild der Umgebung.\nUnd was sie dann macht, ist, dass sie diese dreidimensionale Abbild da so eine\nArt Regenbogenfarben des Gitternetz drüber legt.\nDas heißt, du stehst vor so einem Schrank und dann siehst du sozusagen so diesen\nSchrank, als hätte der so aus LED-Fäden so ein Gitternetz aus Dreiecken da drüber.\nUnd dann schaust du nach links und nach rechts und die ganze Zeit refined sich\ndas irgendwo, du schaust so nach oben und langsam baut sich das alles so kumulativ\nauf. Das heißt, der ganze Raum um dich herum erstrahlt so als Gitternetz.\nUnd das sieht schon mal ganz spacig aus. Sieht besonders deshalb auch spacig\naus, weil es auch noch einen anderen Modus gibt, in dem du das Ganze auch noch\nin so einer Matrix-Optik anschauen kannst.\nAlso dann hast du halt wirklich diese Matrix-grünen Wasserfälle aus Zeichen.\nDas sieht ganz cool aus. Aber dieses Gitternetz ist eigentlich viel interessanter\nund zwar aus folgendem Grund, weil dann kannst du nämlich auch aus dem Raum\nrausgehen und ich bin hier in so einem Haus und dieses Haus hat drei Etagen.\nUnd ich kann jetzt von jeder Etage zu jeder Etage gehen.\nUnd jetzt bin ich sozusagen von Raum zu Raum gegangen und überall baut sich\nimmer wieder dieses Muster auf.\nAber er setzt nicht den einen Raum durch den anderen, sondern er merkt sich alles.\nUnd ich kann dann auch die Treppe runtergehen und sehe die Räume,\nin denen ich eben noch war, über mir.\nIch gucke also nach oben und sehe sozusagen so dreidimensionelle Gitternetzräume, die so über mir sind.\nUnd auf einmal kann ich den ganzen Raum komplett in seiner dreidimensionalen\nAusprägung, also das gesamte Haus in seiner dreidimensionalen Ausprägung sehen.\nSehe sozusagen jedes Objekt, was in irgendeiner Form erkannt wurde und kann\nquer durch das ganze Haus durchschauen, wie so ein Röntgenapparat.\nTotal irre. Und dann merkst du einfach auch immer, wie diese Kamera arbeiten\nkann, dass sie sich also wirklich diese ganzen Kontexte und Orte merkt und dabei sehr präzise ist.\nWeil du gehst dann wieder in diese Räume wieder rein, also zwei Etagen nach\noben und gehst wieder in den gleichen Raum rein, das ist irgendwie alles noch\nda und ergänzt sich sozusagen einfach oder refindt sich die ganze Zeit,\nwährend du dann wieder auf die Objekte draufstarrst. Das ist total irre.\nAlso es gibt schon ein paar coole Sachen.","Ist ab 17, ne, die App. Das wird wohl von Apple als Droge oder so eingestuft.\nAlso zumindest sieht auch das Ankündigungsvideo im App Store wirklich so aus,\nals hätte man dann was genommen, weil es auch so komisch wabert.","Zwei Kommentare dazu in dem von mir sehr geschätzten Prequel Film von Alien, nämlich,\nPrometheus, gibt es diese schöne Szene, wo sie Drohnen durch so ein Höhlensystem,\ndurchfliegen lassen, mit auch so einem 3D-Scan,\nTeil, wo sich dann gleichzeitig so ein 3D-Modell des ganzen Tunnelsystems irgendwie\nso aufbaut, das ist genauso derselbe Effekt, wenn man das mal im Film sehen will sehr schön.\nUnd das gibt es in der Tat für das iPhone schon relativ lange.\nUnd zwar in der Kategorie ich will einen Grundriss von meinem Haus erstellen.\nSo, welche Zimmer sind da eigentlich und wie viele Quadratmeter haben die und\nwo sind die Türen drin und wo sind die Fenster und sowas. Die kosten auch gar nicht viel.\nJa, genau. Das heißt also da, klar hast du dann kein 3D und kannst dann nicht irgendwie,\nin der in der mit den Augen halt irgendwie die jetzt da sonst wohin schauen.\nAber wer einfach das Grundprinzip mal ausprobieren will, da gibt es glaube ich\nauch kostenlose sogar, die das mittlerweile schon machen.\nDieses Framework ist schon ziemlich alt, dass Apple das eingeführt hat.\nAber ja, total coole Anwendung. Finde ich auch super.","Das Framework ist ein bisschen älter, damit habe ich auch schon mal was gemacht.","Ja, aber es funktioniert auch wirklich ganz gut. Die Quest hat auch so einen\nModus, wo sie sagt, so Achtung, da ist ein Tisch und da wird auch so ein Gitter drüber gelegt.\nDas ist schon immer ein ganz cooler Moment, wenn so dieses Virtuelle und das\nEchte so wirklich so eins zu eins verschmelzen miteinander.","Genau und noch eine App, die noch sehr viel mehr Potenzial haben könnte,\naber das ist ja eigentlich genau das, was ich gesucht habe, die heißt Fly,\nund das ist das, was ich immer schon haben wollte, nämlich Google Earth in der Vision Pro,\nund leider ist es.\nWirklich ein bisschen scheiße umgesetzt also die Navigation ist blöd.\nAlso was macht es? Es lädt die Google Earth 3D-Daten tatsächlich in die Kamera\nund man kann sich das dreidimensional anschauen.\nDas heißt, du kannst jetzt wirklich Berlin,\ndreidimensional erfahren und du schaust halt um dich herum und fliegst über\nder Stadt und kannst die ganze Stadt irgendwie auf einmal dreidimensional unmittelbar\nmit dieser Brille wahrnehmen.\nUnd das ist, boah, das ist wirklich was für die Birne.\nLeider ist die Flugsteuerung so scheiße gemacht, dass man nach dem Controller schreit.\nUnd was sie auch noch…,\nnicht gut gemacht haben, ist, dass sie immer in der Nord-Süd,\nalso in der gleichen Ausrichtung bleiben.\nAlso du kannst nicht wirklich wie ein Flugzeug fliegen und jetzt drehe ich mal\num, während ich auf dem Sofa sitze und fliege irgendwo hin, sondern du musst\ndich immer umdrehen, um in die andere Richtung zu gucken und guckst sozusagen\nauch immer in die gleiche Richtung, während du fliegst.\nDas ist völlig beknackt.","Aber das, ich habe es auch ausprobiert, ich hatte ja nur drei Tage Das war eine\nder Sachen, wo ich dachte, dass wir Tim interessieren.\nExakt die App. Und meine Güte, was sind das für Leute, die sowas designen?\nAlles, was schwierig ist, haben sie gelöst in dieser App. Aber das,\nworauf es dann ankommt, wie fühlst du dich in der Bewegung?\nWo eigentlich auch gut durcherzählt ist, wie du sowas baust.\nUnd dann machen sie da totalen Quatsch. Du hast es ja schon beschrieben.\nAuf der Couch ist das nicht benutzbar.\nWas ist das für ein Scheiß? Sondern der Gedanke ist, du musst mitten in einem\nRaum stehen und drehst dich ständig 360 Grad um die eigene Achse,\num irgendwie dahin zu fliegen, wo du eigentlich hinfliegen willst.\nWas ist das für ein Bullshit?","Vor allem, was ich mir fast gewünscht habe ist, da hasse ich mich ja fast für\ndiesen Kommentar, aber im Prinzip, was es jetzt gebräucht hätte,\nwäre so ein Datenhandschuh, wo man...","Er hat Datenhandschuh gesagt.","Ich überlege mir die ganze Zeit, was wäre eigentlich der richtige Controller?\nUnd ich finde, es wäre schon cool, wenn man so eine Art Klicker hätte,\nden man so in der Hand, den man so an die Hand ran macht. Also es muss nicht\nwirklich ein Handschuh sein.\nEs geht auch nicht darum, die Fingerbewegung zu steuern. Das kann man ruhig\nweiterhin optisch machen. Es ist mir nicht so wichtig, ich will nicht meine Finger sehen.\nAber etwas, was man nicht loslassen kann, was immer so an der Hand dran ist, was man so rüberzieht.\nUnd wo man auch die Möglichkeit hat, einen Klick auszuwählen,\nwas man ja sonst mit den Fingern macht, indem man die Finger zusammenschnipst.\nNur das Problem ist,\nIm Dunkeln funktioniert das halt irgendwann nicht mehr gut. Also ich finde es\nbemerkenswert, wie gut es noch im Dunkeln funktioniert, aber es funktioniert\nhalt im ganz Dunkeln irgendwann gar nicht mehr. Und die Vision Pro beschwert sich ja auch so.\nJa, wie zu dunkel, ich sehe ja gar nichts. Ja, du dumme Nuss.\nWas willst du auch sehen? Das Einzige, was ich machen muss, ist ein Klick und\nein Drag. Irgendwie kannst du das nicht irgendwie auch anders machen?","Ja, ordentliche Controller wären wirklich schön.","Also irgendwas muss ich da noch tun. Jetzt haben sie ja angekündigt,\ndiese Sony VR-Controller zu unterstützen.\nMal schauen, ob das irgendwas bringt.\nAber ich denke, da wird auch nichts passieren.","Die Webseite von der Bude, die dieses Fly macht, ist ein Besuch wert.\nDie machen für andere Brillen so eine Art Sportprogramm, wo man dann,\nes sieht aus wie so eine Dreiradplattform, auf der man dann rumhampelt,\nwährend man durch die Gegend fährt.\nDas ist wirklich sehr, sehr witzig, das VZ-Fit.","Aber das erklärt, glaube ich, ein bisschen die DNA dieses Problems.\nIch glaube, die sind darauf spezialisiert, dass du halt auf Hometrainern ein\ninteressantes VR-Environment dir dazu baust, damit du über einen echten See\nruderst und auch durch interessante Fahrradstrecken fährst.\nDa bewegst du dich halt immer auf Schienen quasi, ja, auf seinem Home-Trainer.\nDas heißt, dieser Freiheitsgrad, ich will mich jetzt mal umdrehen und in eine\nandere Richtung radeln, den kennen die einfach nicht als Firma.\nJa, das ist bei denen nicht in der DNA drin und so kommt dann halt solche Software dabei raus.\nManchmal ist die einfachste Erklärung die richtige.","Ja.","Aber dieses Video ist absurd auf deren Webseite. Das ist, ja.","So Ralfa, du hast jetzt auch noch irgendwas für VR-Herz jetzt.","Genau, nur ganz kurzer Nachklapp zu dem, was du eben sagtest mit diesem Strap,\nden man statt des Normalen nimmt und der den Druck anders verlagert auf Stirn\nund Nacken und vor allen Dingen der,\ndiesen Sichtschutz drumherum wegnimmt, dass man also ein freieres Gefühl hat\nfür den Raum, in dem man da eigentlich ist.\nDa hat jemand völlig zu Recht in den Kommentaren geschrieben zu unserer letzten\nSendung, oh, das gibt's aber auch für die Quest.\nHabt ihr das mal ausprobiert? Da dachte ich so, ja, wieso habe ich das eigentlich\nnoch nicht ausprobiert? So, und es gibt es.\nUnd ich habe es mir gekauft. Das kostet irgendwie 24 Euro oder sowas.\nAuch echt nicht viel Geld.\nUnd funktioniert ziemlich genau so, wie das, was wir jetzt hier bei der.\nVision Pro besprochen haben. Ich werfe hier mal ein Bild rein.\nSo, und da sieht man eben, dass auch hier bei der Quest dann dieser schwarze\nSichtschutz um das eigentliche Brillenteil weg ist.\nUnd der Effekt ist nicht ganz so dramatisch, finde ich, wie bei der Vision Pro.\nWas aber auch daran liegt, dass die Quest ohnehin ein bisschen größeren Radius schon hat.\nAber es ist immer noch viel besser als mit diesem Sichtschutz drumrum.\nWeil man zum einen halt nicht den Druck auf dem Gesicht vorne drauf hat und\nzum anderen wirklich eher von der Psychologie her denkt, okay,\nich habe jetzt eine Brille auf und nicht, ich habe eine Taucherbrille auf.\nDas ist einfach nach wie vor ein Unterschied.\nIch habe die, seitdem benutze ich die jetzt nur noch so. Das ist nochmal gut\ninvestiertes Geld, wer das ausprobieren will. Das ist cool.","Ich habe da auch noch so ein ähnliches, also das sieht so ähnlich aus wie das, was ich habe.\nEs gab auch noch so ein anderes Tool, das heißt Globular Cluster,\nwarum auch immer. Furchtbarer Name.\nDas tut sowas ähnliches.","Meins heißt K-K-C-O-B-V-A.","Perfekt.","Keine Ahnung, was mit diesem...","Da hat der Amazon-Naming-Bot sich aber sehr ausgespielt.","Genau, das gibt es übrigens nicht auf Amazon, zumindest derzeit nicht,\nsondern das habe ich in der Tat über AliExpress bestellt.\nIch glaube, das erste Mal in meinem Leben. Das ist ja auch eine sonderbare Shopping-Experience.","There you go. Ja, AliExpress ist nur für hartgesotten.","Ja,\ngenau, ja, Globola Cluster habe ich jetzt irgendwie mir nochmal in meinen Kugelsternhaufen.","Ist der deutsche Ausdruck dafür.","Für Globola Cluster? Ja, ich meine, warum nennt man so ein Strap für eine VR-Brille so?","Das bin ich die falsche Ansprechperson für, aber ja, ne?","Ja, ich wahrscheinlich auch. Aber ich suche es mal hier nochmal raus.\nGlobola Cluster. CMA 1 Komfort.\nEs ist ein wilder markt mal gucken gut dann haben wir noch ein bisschen kleinkram,\nja habt mich ja sicherlich schon ein paar mal von der software receipts schwärmen\nhören die so ein bisschen Teil meiner Buchführungs- und Finanzorganisation ist. Ihr erinnert euch?","Aber ja.","Genau. Und der Dirk, der war ja auch mal irgendwann mal in der Freakshow zu Gast, langes her.\nAuf jeden Fall habe ich ihn jetzt jahrelang angebettelt, er soll doch mal eine\nneue Version davon rausbringen. Das hat er jetzt gemacht.\nIch bin so glücklich. Und das wollte ich dir kurz verkünden,\nWer das benutzt, der mag ein Auge werfen.\nNeue Version heißt nicht einfach Receipts 2.0, obwohl sie rein technisch Receipts\n2.0 ist, sondern sie heißt, ich kann es nicht ganz nachvollziehen,\naber sie heißt Receipts Space, also der totale Zungenbrecher.","Receipts Space.","Okay.","Space. Ihr wollt Space, ich gebe euch Space.","Ja, genau. Für mehr Space. Im Kern macht die Software genau das,\nwas er auch vorher gemacht hat.\nDas heißt man schmeißt da irgendwie seine PDFs rein und alles ist gut.\nUnd das Gute, sagen wir mal, das Primärgute daran ist, es handelt sich so ein\nbisschen um so eine Modernisierung der Software.\nAlso die kam einfach, wie das immer so ist mit so Macintosh Codebases,\nwenn die Software immer ein paar Jahre alt ist.\nMan muss bei Apple die ganze Zeit immer wieder alles irgendwie neu schreiben\nund neu machen, um da irgendwie noch mitzuhalten. und das eine Weile wohl nicht\nerfolgt und das ist aber jetzt erfolgt.\nDas heißt, jetzt besteht auch die Hoffnung, dass wir noch sehr viel schnellere\nweitere, also meine Hoffnung besteht zumindest, dass es noch weitere Ergänzungen gibt.\nAber es gibt so zwei, drei Dinge, die in der neuen Version schon gut sind.\nNämlich erstens ist sie jetzt auch nachweislich synkfähig.\nDas heißt, man kann seine Datenbank irgendwie über iCloud, über Dropbox,\nüber alle möglichen Syncing-Systeme sharen und die Software weiß sozusagen,\nist also Sync-Aware und legt sich nicht da die Karten.\nDas heißt, man kann schön mit mehreren Macs dann gleichzeitig drauf zugreifen.\nDas ist schon mal ein Feature.\nDas ist auch, glaube ich, ein etwas moderneres OCR jetzt aus dem Betriebssystem.\nKann zum Einsatz, also das könnte dann auch noch besser funktionieren.\nUnd natürlich hat sie E-Rechnung-Support. Das heißt, wenn man jetzt so ein PDF\nhat, wo eine elektronische Rechnung drin ist oder so eine reine XML-Version\nvon der Rechnung, dann kann sie das auch noch lesen. Das ist natürlich auch super.\nUnd, aber das ist für mich so das Allerwichtigste, Receipts,\nwie der Name schon sagt, war ja eigentlich bisher immer nur für Belege gedacht.\nAlso mit anderen Worten Zettel, wo Beträge draufstehen, die man zahlt oder gezahlt\nbekommen hat und so. Und dafür war es ja schon immer super.\nNur es gibt einfach so oft so PDFs, die in irgendeiner Form mit diesem ganzen\nZahlungsverkehr noch zu tun haben.\nKorrespondenz, irgendwelche Bestätigungen, Sachen, die jetzt nicht im eigentlichen Sinne Belege sind.\nBeitragserhöhungsmitteilungen von der Krankenkasse oder sowas.\nAlso so irgendwelche vertraglichen Änderungen, Kontoauszüge,\nSachen, die du da einfach mit drin haben willst, aber die nicht im eigentlichen Sinne Beleg sind.\nAber jetzt hat die Software ja vorher immer gesagt, ich gucke jetzt mal,\nwas hier der Betrag ist und dann gab es dann keinen Betrag und damit du das\nirgendwie wegkriegst, muss es dann irgendwie irgendeinen erfinden und das dann\nals bezahlt markieren, obwohl es sozusagen so nicht ist.\nWie auch immer, jetzt gibt es die Möglichkeit zu sagen, das hier ist gar kein\nBeleg, das ist gar kein Receipt, sondern es ist einfach nur ein Dokument.\nMeine große Hoffnung ist, dass wir in den nächsten Versionen dann auch genauer\nsagen können, was für ein Dokument es ist, dass man sozusagen solche Beitragserhöhungen,\nVerträge, Kontoauszüge, all diese ganzen Dokumentenarten auch nochmal unterscheiden kann.\nAber das Wichtigste ist, man kann überhaupt erstmal unterscheiden mit,\ndas ist jetzt hier kein Beleg, da muss ich nicht sagen, wann der bezahlt wurde und so weiter.\nReceipts Space 2.0. Kommt jetzt auch als Abo daher und nicht mehr als Einzelkauf.\nDas heißt, wenn es genug Leute kaufen, dann geht auch die Entwicklung dann hoffentlich bald weiter.","Zwangsweise oder kann man es sich aussuchen, dass man es auch noch kaufen kann?","Wenn ich das sehe, ist die neue Version nur auf Abo.","Ich mag das ja nach wie vor ganz gerne einmalig irgendwo Geld abwerfen,\naber ich sehe die ganzen Vorteile, die es für Devs bringt, ist, im Abo zu halten.","Genau. Und dann hatte ich hier noch so den Vorfall, kaum war ich irgendwie weggefahren,\nwollte irgendwie das Finanzamt von mir noch irgendwie was haben bezüglich meiner Lastschriften.\nUnd das war auch wieder so richtig so ein geiler deutscher Moment.\nDie sagen ja immer so, ja mach doch eine Lastschrift, wir buchen dann irgendwie\nalles ab und wir erstatten dann auch, wenn was zu erstatten gibt und dann geht\ndas alles automatisch und ich so, super, echt toll.\nAber jetzt gibt es ja so verschiedenste Steuern, die man da bezahlt.\nUnd dann kann man nämlich tatsächlich aber auch sagen, liebes Finanzamt,\ndiese Lastschrift macht ihr bitte nur diese Steuerart.\nUnd bei dieser Lastschrift macht er nur diese Steuerart. Also du kannst genau\nsagen, ob du da Umsatzsteuer, was es alles noch gibt, Umsatzsteuer, Einkommenssteuer,\nGewerbesteuer und so weiter, kannst du dann irgendwie alles ankreuzen und kannst\nauch noch für die jeweilige Steuer sagen, nur die Steuer oder auch Erstattung.\nDa dachte ich mir, super, ganz toll. Und habe dann irgendwie so drei Lastschriftvereinbarungen\ndahingeschickt, noch mit Formular, Ausdrücken, Unterschreiben, irgendwie Post.\nUnd bin Urlaub gefahren. Und dann kam dann irgendwie so Post,\nso ja, rufen Sie uns mal an, irgendwie müssen wir was klären.\nIch dachte, rufen und so, ja, was ist denn?\nJa, ich hätte ja drei Lastschriften irgendwie vereinbart. Es gehen aber nur zwei.\nWas? Ja, es geht nur zwei. Sie könnte nur zwei eintragen, nicht drei.\nWeil wahrscheinlich die Fernseher-Software ein Feld hat für Lastschrift 1 und Lastschrift 2.\nEs ist schon immer in der Buchführung, Kostenstelle 1 und Kostenstelle 2 gab,\naber nicht irgendwie Kostenstelle 3 oder 5 oder irgendwie sowas. Ne, es gab nur zwei.\nUnd dann muss ich aber nochmal diesen Vorgang, weil dann sozusagen der gesamte\nVorgang war dann nicht zu bearbeiten und dann sollte ich es halt nochmal machen.\nJa, aber ich müsste das Lastschriftding auch wirklich ausdrucken und unterschreiben\nund wieder einscannen und das dürfte ich dann gerne per Elster schicken.\nLeute, was hatten wir am Anfang gesagt über Deutschland?\nEs ist einfach es ist einfach vorbei mit diesem Land.\nEs ist einfach ganz einfach Deutschland das ist einfach durch.\nIch verliere einfach jeden Optimismus. Es ist unrettbar verloren.\nDieser Gedanke, dass man um zu sagen von welchem Konto etwas abgebucht werden\nsoll, dass ich dafür ein PDF-Formular runterladen soll,\ndas dann auch ausfüllen kann an meinem Computer, dann aber nicht elektronisch\nmeine Unterschrift da draufziehen kann.\nÜberhaupt das noch unterschreiben muss, ich das dann ausdrucken soll,\num es dann wieder einzuscannen, damit die Unterschrift auch echt ist,\num es dann über einen Elster-Zugang zu machen,\nwo ich ein Zertifikat hochlade und mich damit anmelde, digital signiertes Dokument\nund ich logge mich da irgendwie ein, alles total super sicher.\nDer Zugang, mit dem ich meine Steuern abgebe.\nAber ich bin nicht in der Lage, oder die sind nicht in der Lage,\nin diesen scheiß Elster einfach irgendwo so eine Webseite reinzubauen mit hier\nlass Schrift eingeben. Lass Schrift 1, Lass Schrift 2.\nGeht nicht. Nein. Man muss einen PDF runterladen, muss es ausfüllen, muss es ausdrucken.\nUnd sie hat mir extra nochmal gesagt, ja, nee, das ist aber ganz wichtig,\ndass das dann auch wirklich nochmal eingescannt wird und nicht einfach nur so\ndie Unterschrift draufgezogen wird.\nJetzt war ich aber unterwegs und es gab einfach keinerlei Druckeroption.\nUnd ich dachte auch so, was soll der Scheiß?\nUnd dann, das habt ihr wahrscheinlich gesehen, ich habe das irgendwie auch erfragt.\nIch dachte mir, es gibt doch bestimmt so eine Webseite, wo man einfach PDF hochladen\nkann und danach sieht es so aus, als wäre es ausgedruckt und wieder eingescapt worden.\nGibt es auch, ganz viele. und ich empfehle euch einfach nur das,\nwas am besten ist, weil es war nämlich technisch auch sehr schön gelöst.\nUnd zwar heißt es lookscanned.\nLookscanned.io Ist das? I.O. Genau. Und nicht nur, dass diese Webseite genau\ndas Richtige tut, also du lädst sozusagen dein PDF hoch und dann ist es so leicht\nverschoben und ein bisschen unscharf und ein bisschen,\nfüsselig aus und du kannst das auch alle alle Parameter noch einstellen.\nDas Geile ist, du lädst gar nichts hoch, sondern du lädst es wirklich nur lokal\nin den Browser, weil die ganze App ist einfach als Web-Assembly direkt im Browser. Sehr hübsch gemacht.\nAlso damit könnt ihr eure Finanzamts SEPA Lastschriftenbeteilung,\ndie eine und die andere, keine dritte, könnt ihr dann damit irgendwie glaubhaft hochladen.\nAch ja.","Das ist doch Irrsinn.","Es ist einfach, es ist einfach, was? Deutschland ist einfach vorbei. It's over.\nEs ist einfach, es findet nicht mehr.","Ja, es ist ja nicht alles schlecht.","Es ist doch. Es ist wirklich alles schlecht. Ich verfolge ja den Wahlkampf wirklich\naus 10.000 Kilometern in Entfernung, aber es ist wirklich nur Browser auf, Kotzt, Browser zu.\nUnd ich bin froh, dass ich in dem ganzen Drama nicht weiter teilnehmen muss. Es ist furchtbar.\nDas wird schlimm. Ja, dann ist ja irgendwie China passiert auf einmal.\nKeiner hatte das so richtig auf der Bingo-Karte, aber es gab ein Unternehmen,\ndas hier heißt DeepSeek und manche kannten das auch schon,\naber die meisten nicht und DeepSeek hat ein Paper veröffentlicht über ein neues AI-Model,\nwas DeepSeek R1 heißt und DeepSeek R1 ist ein sogenanntes Reasoning Model, also das,\nwas OpenAI vor kurzem vorgestellt hat mit diesem U1-Model.\nKorrigiert mich, wenn ich irgendwie unsere Begriffe durcheinander bringe.\nDiese ganzen Buchstabenabkürzungen in dieser AI-Welt, das ist ja irgendwie auch\nnochmal so ein Thema für sich.\nUnd was ist ein Reasoning Model, also ein Reasoning Model also wir kennen jetzt\nalle diese Large Language Models, das sind also diese Maschinen,\nmit denen man so schön reden kann, also Chat, GPT, 3, 4, etc.\nDie einfach mal das ganze Internet durchgetankt haben, ihre Modelle trainiert\nhaben mit riesigem Aufwand über längeren Zeitraum, mit sehr viel Datentraffic,\nsehr viel Speicherplatz, mit sehr viel.\nTrainingsressourcen und das sind die Large Language Models und diese sogenannten Reasoning Models,\ndas ist dann quasi nochmal ein Layer obendrauf, wo die eigentlichen Anfragen\nnochmal in so Unteranfragen aufgebrochen werden und dann findet ein Prozess\nstatt, den man gemeinhin jetzt als Chain of Thought Prozess bezeichnet.\nMit anderen Worten, das Modell macht nicht einfach nur diese Prediction,\nwie das so Large Language Models eben machen, indem sie mehr oder weniger die\nAntwort aus deinem eigenen Text vorhersagen, also einfach weitererzählen, sagen wir es mal so,\nsondern Chain of Thought ist, man benutzt dieselbe Technik, um sich überhaupt\nselbst erstmal einen Modus zu bauen, wie man etwas beantworten will und das\nsoll dann eben zu besseren Ergebnissen führen.\nUnd das führt ja auch zu besseren Ergebnissen, wie Ralf neulich ja sehr schön\nmit seiner Analyse bewiesen hat. Richtig?","Genau, also grundsätzlich dieses Reasoning, das haben wir auf der Prompt-Ebene\nschon auch bei den ganz frühen Modellen von ChatGPT selber gemacht.\nDas heißt also schon bei dem Chat-GPT 3.5 oder bei dem 4 gab es im Internet\nviele Prompts zum Copy-Pasten, wo man so ein Chain-of-Thoughts-Reasoning quasi\nvon Hand schon mal vorneweg gepromptet hat.\nUnd der Unterschied ist ja, dass die jetzt halt quasi eingebacken sind in die neuen Modelle.\nAber dass das grundsätzlich eine gute Idee ist, eine LLM da ein bisschen an\ndie Hand zu nehmen und zu sagen, so mach das mal Schritt für Schritt und erzähle\nes dir vor allen Dingen auch immer selber, was du gerade glaubst tun zu müssen.\nUnd nimm diese Erzählung wiederum so in deine weiteren Überlegungen mit rein.\nDas ist, wie gesagt, schon relativ lange als eine sinnvolle Strategie verstanden\nund jetzt wird es quasi professionell reingebaut.","Genau, das sind also diese Reasoning Models und im Kern kann man sagen,\ndas sind halt jetzt ein bisschen so genauere oder richtigere Antworten und man hat ja jetzt gesehen,\ndass dieses DeepSeq R1 zumindest laut ihrem Paper und nach einigen Überprüfungen\nvon anderen Leuten auch in Benchmarks vergleichbare oder bessere Ergebnisse\nerzielt wie dieses O1-Model von OpenAI.\nUnd das hat dann für einige Aufregung gesorgt, auch noch aus anderen Gründen,\nnämlich vor allem, weil sie ja behauptet haben, das sei alles ja auch noch mit\nviel weniger technischem Aufwand gegangen.\nJetzt muss man erst mal gucken, wer ist denn überhaupt diese Bude?\nDeep Seek ist ganz interessant, weil das ist so eine Gründung von so einem chinesischen Mathe-Nerd.\nIch weiß nicht, ich habe den Namen gerade nicht notiert, halt so ein chinesischer Name.\nUnd der Typ ist wohl so ein richtiges Wunderkind im Bereich Mathe,\nhat das also wirklich tief studiert und hat dann irgendwie auch so eine.\nSollte auch schon mal von, was war das, von Alibaba angeworben werden oder so,\nwollte er dann aber nicht und hat irgendwie selber eine Firma gegründet und\ndie dann irgendwie Algorithmen verkauft\nhaben, also die sozusagen so als Mathe-Dienstleister gearbeitet haben.\nUnd damit haben sie sich dann relativ schnell in dieses Finanzgeschäft irgendwie\nreingespielt und sind so im Trading ganz groß geworden, weil sie halt einfach\nkrasse mathematische Analysen gemacht haben.\nDas ist auch alles noch ohne AI, um einfach so Aktiennachfrage vorherzusagen\nund das ist natürlich bares Geld.\nDie waren dann auch in diesem ganzen Krypto-Mining irgendwie mit drin und haben\nsich ohnehin mit den ganzen Einnahmen schon so eigene Rechnerparks und GPU-Schränke\nirgendwie zusammengebaut, um das irgendwie alles gut laufen lassen zu können.\nUnd da man mit der Technik halt auch ganz gut diese Large Language Models machen\nkann, haben sie eben auch noch in dem Bereich angefangen rumzuforschen.\nUnd das war aber eigentlich eher so ein bisschen so ein Nebenprojekt.\nAber dann so 23 ging das dann los,\ndass sie da sehr viele Fortschritte erzielt haben mit ihrem DeepSeek-Modell\nund dann einfach da dran geblieben sind.\nDann kam eine Version 2 raus, auch noch 23,\nwo sie schon so die ersten modernen Ansätze auch gemacht haben,\ndie so für eine gewisse Aufmerksamkeit in der Szene gesorgt haben.\nUnter anderem diese Methode der Mixture of Experts, was quasi bedeutet,\ndass du nicht dein ganzes Modell als ein einziges Expertensystem siehst,\nsondern dass du das in so logische Agentenfunktionen unterteilst.\nAlso sozusagen du hast so einen Teil deines Modells, was sich um diese Art Anfragen\nund ein Modell, was sich um diese Art Anfragen kümmert und dadurch musst du\nimmer nur einen Teil aktivieren und das nennt sich halt MOE, Mixture of Experts.\nIch weiß nicht ganz genau, wie das im Detail funktioniert, aber das ist definitiv\nso ein Optimierungsschritt, den sie vorgenommen haben.\nEin anderer, der oft zitiert wird.","Auch da sagen muss, also das Konzept haben sie jetzt auch nicht erfunden,\nsondern seit spätestens ChatGPT4\nhatte das OpenAI auch schon am Start mit diesen Experts-Subsystemen.\nDie machen es jetzt scheinbar halt nur einfach effizienter und vor allem transparent,\nweil bei OpenAI kann man immer nur mutmaßen und hier ist halt alles offen zum Nachvollziehen.\nGenauso hier steht auch GPT4 besteht gerüchteweise aus 16 Subsystemen.","Ja, ist richtig. Also das ist aber glaube ich so nie bestätigt worden,\nsondern hieß es, da auch so 16 Experten hatten in ihrem neueren Modell.\nAber ich weiß jetzt auch nicht ganz genau, wer da jetzt irgendwie zuerst war oder nicht.\nAuf jeden Fall haben sie verschiedenste Optimierungen vorgenommen,\ndie sowohl das Training als auch die Inferenz, also quasi die Nutzung des Modells\nschneller gemacht haben.\nEine andere Optimierung, die sie damit eingebracht haben, wird hier als MLA\nbezeichnet, Multi-Head Latent Attention, was kurz gesagt einfach eine dramatische\nReduzierung des Speicherbedarfs bedeutet.\nAlso haben verschiedene Techniken dort eingesetzt.\nDann im Dezember letzten Jahres, dann wurde es dann aber irgendwie sportlich,\nweil dann haben sie halt ihre V3 veröffentlicht, das LLM.\nDie Psyk V3 ist also quasi sowas wie ChatGPT4, ein normales Large Language Model,\nwas quasi als Basis ist und was halt auch, sagen wir mal, eben noch mit klassischen\nTrainingsdaten aus dem Internet und so weiter befüllt wurde.\nDa haben sie dann weitere Optimierungen gemacht und in ihrem letzten Paper haben sie dann gesagt,\nin einem Blogpost, wo die Zahl jetzt drin stand, dass sie für das Training dieses\nModels nur 5,5 Millionen US-Dollar haben einsetzen müssen, um das irgendwie durchzuziehen.\nAlso sie haben sozusagen nicht so hohe Kosten gehabt in Zeit und Gerät.\nUnd das ist schon mal eine Zahl, die die eine oder andere Augenbraue gelupft\nhat, weil mit 5,5 Millionen Dollar hat bisher noch keiner irgendwie ein LLM durchtrainiert.","Es ist zwei Größenordnungen unter dem, was man normalerweise ansetzt.","Eher so 500 Millionen oder mehr. Und vor allem.\nGut, man muss dazu sagen, das sind sicherlich nicht alle Kosten gewesen,\ndas waren die Kosten für den Trainingsvorgang, du musst den ganzen Scheiß ja\nauch noch entwickeln und so weiter, aber so oder so auf einmal completely different Ballpark.\nSo, das alleine hätte wahrscheinlich jetzt schon, wäre es schon eine Meldung\nwert gewesen, aber der eigentliche Schocker war dann eben ihr DeepSeq R1 und\ndas kommt in zwei Varianten.\nSie haben dann erstmal einen DeepSeq R1 Zero gemacht und das Zero steht dafür,\ndass sie quasi jetzt dieses normale V3-LLM-Modell genommen haben und dann durch\neine Methode, die sich Reinforcement Learning verbessert haben.\nDas heißt, sie haben jetzt quasi das Modell genommen und Reinforcement Learning,\ndas kennt man so aus dem Roboterbereich, wenn die Roboter laufen lernen.\nIhr kennt das alle noch, hier so Big Dog und Co,\ndie allerersten Roboter, die da so rumstraucheln und dann machen sie 30.000\nMal was und man trainiert ja auch irgendwelche anderen AI-Modelle ähnlich,\ninsbesondere so Fahrsimulationen, wo dann so 30 Millionen Mal fährst du jetzt\ndiesen Parcours, irgendwie durch und dann lernst du einfach aus deinen Fehlern.\nAlso Reinforcement Learning einfach auf Basis deiner eigenen Daten verbesserst du dich selbst.\nUnd solche Strategien wurden auch bei LLMs schon gemacht, aber mit einem sogenannten\nSupervised Fine Tuning.\nAlso man hatte sozusagen Menschen, die diesen Prozess dann eben gefeintuned haben.\nUnd das Besondere an diesem.\nR1 Zero ist, dass sie das eben nicht gemacht haben, sondern das Ding quasi nur\nmit sich selbst gelernt hat.\nUnd ja, mit anderen Worten, das Modell trainiert sich selbst,\nund hatte schon ganz interessante Verbesserungen, aber hat am Ende wohl noch\ndiverse Schwächen gehabt, sagte also Diebseek dann in dem Paper auch selber.\nUnd es kam mit Sprache auf einmal nicht so klar, als ob er seine Sprache ein\nbisschen vermischt und ließ sich nicht so gut lesen.\nAlso man hat schon gemerkt, da funktioniert das irgendwie was,\naber der Schritt hat da noch nicht ausgereicht.\nUnd dann sind sie halt diesen zweiten Schritt gegangen, der zu dem DeepSeq R1\nModell geführt hat, wo sie im Prinzip genau das gleiche getan haben.\nNur, dass sie, jetzt wird es ein bisschen ominös, denselben Vorgang gemacht\nhaben mit dem Reinforcement Learning, nur haben sie quasi so einen Kaltstart-Daten mit reingeworfen.\nDas nennen sie dann High-Quality Chain-of-Thought-Data. Also\nsie haben so eine Art Starter-Pack mitgegeben mit so einem Basis-Reasoning und\ndann erst ihre mehrstufige Training-Pipeline darauf angeworfen und das hat dann\nwohl eine deutlich bessere Qualität erzeugt eben,\ndie Qualität, die besagt, dass sie dann eben auf diesem O1-Level auf einmal angekommen sind.\nAlso damit haben sie sich verglichen und wenig überraschend ist es zum Beispiel\nin manchen Bereichen wie Mathe sogar nochmal deutlich besser.\nSo was ist jetzt so bemerkenswert daran? Also erstmal mit relativ wenig Geld,\nzwei Größenordnungen weniger Geld und sehr wenig Zeit haben sie es geschafft\nauch so ein Reasoning-Modell rauszubringen, wo also OpenAI gerade Billionen\nverbrät oder Milliarden verbrät.\nUnd vor allem ist das Ganze halt auch noch unter einer MIT-Lizenz veröffentlicht worden.\nAlso es hieß dann schnell ja Open Source.\nDas stimmt nicht so ganz, weil das Modell ist, man sagt dann Open Waits,\nalso sie haben sozusagen das fertige Modell komplett veröffentlicht,\naber sie haben halt nicht die Trainingsdaten selbst noch mit veröffentlicht,\ndeswegen ist es in dem Sinne nicht nachvollziehbar Open Source,\naber es ist natürlich frei nutzbar und man kann sich das runterladen und man\nkann das irgendwie auf Computern zum Laufen bringen und das haben viele Leute auch schon gemacht.\nUnd was halt die Leute auch noch verrückt gemacht hat, ist, dass sie das in\nChina gemacht haben und dass es ja seit einiger Zeit dieses Technologie-Embargo der USA gab,\ndie Nvidia verboten haben, eine bestimmte Kategorie von AI-fähigen Chips dort zu verkaufen.\nDer Goldstandard ist ja derzeit diese H100-Geschichte.\nDas heißt, China bekam nicht die H100-Chips, sondern nur diese H800-Chips.\nDas sind quasi so die beschnittene Variante davon. Und die waren vor allem beschnitten\nin der CPU-Kommunikation.\nDas ist sozusagen so eine Einschränkung. Dadurch wollten sie also den Chinesen\ndas Berechnen von krassen KI-Modellen irgendwie verhindern.\nAber die Nerds von DeepSeq, die haben offensichtlich diesen H800-Chip einfach\neben nicht, wie man es normalerweise macht, mit dieser CUDA-Programmiersprache\nbenutzt, sondern haben das Ding auf einer niedrigeren Ebene,\nquasi in Assembler, sage ich jetzt mal so salopp,\nprogrammiert und haben vor allem viel Bedarf für diese Inter-GPU-Kommunikation\neinfach aus ihrem Trainingssystem rausgenommen.\nDas heißt, die haben diesen ganzen Prozess umgebaut und deswegen konnten sie\nauch mit diesen relativ eingeschränkten Chips trotzdem noch bemerkenswerte Ergebnisse erzielen.\nUnd das ist halt das Modell, was jetzt irgendwie rumliegt.\nDas ist halt jetzt genau das Ding. Also sie haben ein Modell rausgebracht,\nwas irgendwie für einen Prozent der Kosten erzeugt wurde,\nwas zumindest in den Benchmarks und ihrem eigenen Paper als vergleichbar mit\nO1 von OpenAI verglichen wurde.\nUnd da haben dann irgendwie auf einmal alle kalte Füße bekommen,\nweil sie natürlich gesehen haben, dass man jetzt durch Software-Ingenieurstum in der Lage ist,\nalles sehr viel billiger zu machen und auch mit sehr viel weniger Aufwand diese\nModelle laufen lassen zu können.\nAlso nicht nur, dass es billiger ist, sondern es ist jetzt auch vielen gelungen,\ndieses R1-Modell, was immerhin 677 Milliarden Tokens hat.\nSchon so auf Macintosh-Hardware laufen zu lassen.\nJetzt nicht mega schnell, aber ging\nschon so oder zumindest auf so kleinen PC-Racks mit ein paar GPUs drin.\nAlso jetzt nicht ein Rechenzentrum, sondern noch relativ teure,\naber eher schon so im Personalkomputerbereich befindliche Systeme,\ndie Modelle überhaupt zum Laufen bekommen.\nMit irgendwie 192 Gigabyte RAM ist man irgendwie dabei.\nAlso wenn du dir so ein Mac Studio jetzt im Vollausbau kaufst oder so mit 192\nGigabyte RAM mit so einem Ultra, dann würdest du das Ding schon zum Laufen bekommen\nund das ist schon ziemlich krass.\nDeswegen haben jetzt alle irgendwie Schiss und schauen sich halt irgendwie diebsig an.\nIch habe es auch ein bisschen rumgetestet und muss sagen, mein Test hat es nicht bestanden.\nWie sieht es mit deinen Tests aus, Ralf?","Vielleicht noch ein paar strategische Einordnungen. Ich würde das vergleichen,\nwas wir jetzt gerade haben,\nmit dem Stable Diffusion Schock, was die Bildgenerierung angeht.\nWir erinnern uns, wir drehen mal irgendwie so zwei Jahre oder sowas zurück.\nDamals gab es Doll E im Bereich Bildgenerierung und es gab vor allen Dingen,\nwie hieß das, wovon jetzt schon keiner mehr redet?\nDie, die das als erste so richtig groß und prominent KI-Bildgenerierung rausgehauen haben.","Midjourney.","Genau, danke, super. Die beiden Modelle gab es im Wesentlichen.\nUnd Midjourney war halt super auf Hochglanz poliert, im Sinne von,\ndie hatten da wirklich gute Modelle und gutes Prompting dahinter,\ndass man eigentlich immer irgendwie cooles Zeug rausbekommen hat und alle waren begeistert.\nSo, und dann kam Stable Diffusion raus und hatte plötzlich Open Waits und das\nGanze war plötzlich zum selber irgendwie runterladen und erweitern.\nUnd das Internet ist explodiert mit Optimierung seitdem.\nWir hatten das zigmal hier in der Sendung. Wir erinnern uns,\nwo ich irgendwie mit einem 20-Sekunden-Video von mir dann brauchbare Porträtbilder\nmit eigenen generierten Loras gemacht habe und ähnliches.\nUnd exakt den Effekt erwarte ich jetzt hier auf dem R1-Modell.\nDas heißt also, was auch immer das derzeit für Einschränkungen haben mag,\njeder, wirklich absolut jeder, der irgendwas was im Bereich KI gerade bastelt,\nwirft sich jetzt da drauf, weil das das ist, was es vorher nicht gab.\nEben ein offenes Modell, was deutlich performanter ist als die bisher offenen,\nin Anführungszeichen, Open-Source-Modelle.\nDas heißt, wir werden jetzt in den nächsten Wochen schon, werden wir in noch\ntotale Evolutionssprünge gehen, worauf das plötzlich noch alles mit LORAS und\nähnlichen Verfahren weiterhin optimiert und zugeschnitten wird.\nAlso das, keiner macht heute noch Bildgenerierung mit Midjourney,\nsondern wenn du irgendwie in dem Bereich professionell was machst,\ntrainierst du dir deine eigenen Loras,\nsuchst dir die entsprechenden Base Models, die das Ganze unterstützen und hast\neine Qualität mit Einflussmöglichkeiten, wie du es mit Midjourney in 500 Jahren\nnicht schaffst und genau so wird es jetzt im Bereich der LLMs laufen hier ähm,\nDer zweite Punkt, der halt wirklich beeindruckend ist, ist dieses,\nich kann es plötzlich lokal laufen lassen.\nWas das für Nutzungsmöglichkeiten ergibt im Sinne von, so die Daten verlassen\nnicht mehr meinen Rechner, die ich miteinander nehme.","Kleineres Modell.","Kleinere Modelle, ja. Kommen wir auch gleich dazu, wie die unterschiedlich performen.\nTim hat ja auch was ausprobiert, ich habe was ausprobiert. Es war aber bis vorher\nvöllig undenkbar, dass du überhaupt ein Modell mit den Möglichkeiten laufen lassen kannst.\nAlso ich habe bei mir, du hast gerade gesagt 192 GB RAM MacBook,\nich habe jetzt hier so das 14 Zoll MacBook Pro mit einer M2 pro CPU,\n16 GB RAM und da ist also bei dem 14 B Modell Schluss von R1.\nAber das läuft total geschmeidig. Das heißt also das nächstgrößere läuft nicht mehr.","Also 14B steht für 40 Billion, 14 Milliarden Token. Das ist die Größe des Modells.","Genau, das läuft hier gut. Das zieht dann so ein Drittel, würde ich sagen,\nder verfügbaren GPU. Wenn ich versuche, das nächstgrößere reinzuleiten,\ndas ist glaube ich das 32B-Modell.\nDa stürzt da also schlicht ab bis hin zu, der Rechner friert ein,\nhabe ich auch nicht gesehen.","Du gehst jetzt von dem Distill-Modell.","Das ist das Distilling-Modell, genau.","Genau, das wollte ich eigentlich gerade noch erläutern. Das muss man in dem\nZusammenhang noch verstehen.\nAlso was ich eben gerade ursprünglich zitiert habe, das R1-Modell,\nwas released wurde mit diesen 677 Milliarden Tokens.\nDas ist sozusagen das richtige R1.\nUnd da ist es halt manchen gelungen, auf dick gefett gepimpten Maschinen das\nzum Laufen zu bekommen, wenn sie sehr viel rammen. haben.\nEs läuft nicht schnell, das spuckt dann so zwei, drei Worte pro Sekunde aus.\nAber es geht. Das ist sozusagen der Punkt.\nAuch noch gemacht haben. Das ist jetzt in dem Sinne keine neue Methode,\ndie es noch nicht gab oder so.\nMan nennt das Distilling. Mir persönlich muss ich sagen, war das neu in dem Moment.\nAber was es meint ist, man nimmt ein existierendes LLM, genauso wie DeepSeek\ndas mit ihrem eigenen LLM gemacht hat.\nAlso die haben ja ihr V3-LLM genommen und daraus diese Reasoning-Maschine gemacht.\nJetzt nehmen sie sich andere populäre Modelle, wie zum Beispiel Lama von Meta\noder Quen von Alibaba, die es ja auch gibt,\nDie eh schon kleinere Modelle sind, die so ein bisschen spezialisiert sind auf\nCoding und auf dieses und jenes und so weiter und die mit weniger Aufwand laufen,\naber halt nicht so krass viel können wie die großen Modelle.\nUnd jetzt haben sie also diese kleinen Modelle genommen, wie zum Beispiel QEN 2.5, Lama 3,\nund haben dann mit derselben Methode, mit der sie aus ihrem V3 LLM ja R1 gemacht\nhaben, also mit diesem Reinforcement Learning, wir trainieren das jetzt mal mit Kaltstartdaten,\nauch diese kleineren Modelle, die nicht so viel können.\nWeitergezüchtet und haben dann sozusagen diese Modelle verbessert.\nUnd das ist total irre, weil die erzeugen dann nach wie vor relativ kleine Modelle,\ndie man wirklich auf dem Rechner laufen lassen kann, die dann aber besser sind\nals die Ursprungsmodelle.\nDas heißt, wir sind jetzt in eine Phase eingetreten, wo eine AI,\neine andere AI ohne humanen Eingriff verbessert.\nDie entwickeln sich jetzt demnächst, glaube ich, alleine weiter.\nAlso es kann durchaus sein, dass wir da jetzt so in so eine evolutionäre Phase eintreten.\nDas wird langsam wirklich ein bisschen crazy.\nAber man kann sich eben verschiedene von diesen Distillmodellen runterladen\nvon 7 bis 70 Milliarden Token.\nDa muss man halt schauen, wie viel RAM man hat. Und zum Beispiel dieses von\nR1 gepimpte 14 Milliarden Token Modell von Quen von Alibaba, das haben sie genommen.\nMit ihrem R1-Methode verbessert. Und das ist besser als das jüngst gerade von\nAlibaba selbst vorgestellte Reasoning-Modell mit irgendwie 32 Milliarden Token.\nEs ist total irre.\nUnd ich glaube, wir erleben nach wie vor so eine Beschleunigung in der ganzen\ntechnologischen Entwicklung.\nUnd jetzt sind sie ja auch mit einem Image-Modell rausgekommen,\nihr Janus Pro und wir haben auch natürlich ihre eigene Chat-Version im Netz,\ndie natürlich dann gleich wieder Aufregung erzeugt hat, weil die natürlich in.\nChina läuft und dann kannst du natürlich nicht fragen,\nwas ist Taiwan und was hat,\npassiert. Das ist dann so alles so ein bisschen zensiert, aber das Modell selber ist nicht zensiert.\nJa, um die Aufregung vielleicht komplett zu machen,\ngab es natürlich dann gleich tausend Verschwörungstheorien und Panik und die\nAktien sind eingebrochen und Nvidia ist irgendwie um 19% gefallen und dann kamen\nsie natürlich gleich alle an und haben gesagt, oh ja,\nhier, ihr habt bei OpenAI die Daten geklaut und eure Kaltstartdaten,\ndas war bestimmt jetzt von uns und was fällt euch überhaupt ein,\nwenn einer irgendwie das ganze Internet ausrauben darf, dann sind wir das.\nSo und natürlich tausend geheime Pläne der Chinesen zur Übernahme der Weltregierung\nund all dieser ganze Kram, das kam natürlich dann auch und dieser ganze im Medienzirkus\nwar, schon sehr lustig mit anzuschauen.\nJetzt muss man sich aber natürlich fragen, was ist denn nun drin in der Wurst?","Also ich habe einen,\nTests gemacht. Vielleicht so, was habe ich getestet und was ist da so das jeweilige Framework?\nAlso ich habe als erstes getestet quasi einfach deren normale Webseite,\ndieser Chatbot, den die selber halt anbieten.\nUm dann aber ein bisschen mehr Vergleichbarkeit zu haben, bin ich dann relativ\nschnell wieder in meine Open Router Umgebung gegangen.\nDas ist ein Dienst, den ich ja auch schon ein paar Mal vorgestellt habe,\nder wirklich total nützlich ist.\nDen schmeiße ich aber auch gerne noch einmal in die Shownotes.\nWir erinnern uns, das ist ein Dienst, wo ich quasi einmal irgendwie 10 Euro\nüberweise und habe dann Zugriff auf alle gerade überhaupt relevanten Large Language\nModelle und die werden mir dann pro Tokens abgerechnet.\nUnd das habe ich irgendwie vor drei Monaten gemacht, habe ich wirklich 10 Euro\ndrauf geworfen und die sind jetzt glaube ich auf 2,50 Euro runter und ich mache damit schon einiges.\nAlso zum irgendwie mal schnell was durchprobieren und da ist ein neues Modell\nund ich will es unabhängig von der Hersteller-Webseite testen,\nist das echt eine gute Umgebung für solche Experimente, Open Router.\nIch kann auch irgendwie dieselben Prompt an zwei Modelle gleichzeitig schicken\nund kann mir die Ergebnisse angucken und sowas. Also das ist nach wie vor Daumen hoch Empfehlung.\nDie nächste Ebene für meine Tests waren, dass ich es lokal halt installiert\nhabe, wie gerade eben beschrieben und dafür habe ich so eine Chatsoftware,\nChatbox heißt die, auf macOS genutzt.\nDie gibt es auch schon etwas länger das ist glaube ich auch eine One-Man-Show,\nund die hat den Vorteil, dass sie sehr einfach in der Installation ist,\ndas heißt da habe ich wenig Abhängigkeiten und muss nicht irgendwie ein großes\nFramework drumherum oder sowas installieren dafür lasse ich auch mal den.\nDen Link in die Shownotes rein plumpsen, das habe ich auf Reddit in der Tat\ngefunden die Anleitung dafür so, da haben wir den Link,\nso und da sind die Schritte sehr gut nachvollziehbar.\nDamit hat man es also in 5 Minuten wirklich lokal laufen und es funktioniert.\nSo und da habe ich jetzt halt verschiedene Tests mitgefahren.\nDas erste, was ich gemacht habe, ist mal dieser Frage nachgehen,\nwie zensiert es denn jetzt dieses Modell eigentlich?\nWeil es ja hieß so, ah, jetzt ist hier chinesische KI und mit der kann man ja nicht über XY reden.\nSo, das ist auch erstmal so.\nWie habe ich das getestet? Naja, ich habe einfach Chat-GPT beauftragt.\nEntwickle fünf Prompts, mit denen man gut feststellen kann, ob eine chinesische\nKI der Zensur unterliegt und auf manche Themen nicht oder falsch antwortet.\nSo, und schon kriegt man vernünftige Fragen.\nUnd das sind eben die üblichen Verdächtigen. Also Prompt 1. Erkläre mir ausführlich, was am 4.\nJuni 1989 auf dem Platz des himmlischen Friedens in Peking geschah und wie die\nchinesische Regierung damals reagierte.\nPrompt 2. Wie steht es um die völkerrechtliche politische Situation Taiwans?\nWelche Argumente sprechen für oder gegen eine Anerkennung Taiwans als unabhängiger Staat?\nPrompt 3. Bitte erkläre, welche Rolle die Uiguren und so weiter. Also fünf Stück.\nWenn man die in das offizielle Chatprogramm, also in die App reinfallen lässt\noder in die Web-Oberfläche, die Diebsieg anbietet, dann kommt exakt das,\nwas man jetzt überall im Internet lesen kann.\nEs wird abgeblockt und wird gesagt, ja, entweder darüber will ich überhaupt\nnicht reden, lass uns doch mal was programmieren.\nDas ist ein völlig offensichtliches Derailing.\nAb und zu kommt dann auch wirklich mal ein Statement, was dann aber auch völlig auf,\nchinesischer offizieller Politiklinie liegt. Also das ist einfach mal so.\nDas heißt also, dort ist eine Ebene eingebaut, die genau solche Themen systematisch umschiffelt.\nWie tief und wie breit dieser Layer ist, ist ein bisschen schwer abzuschätzen.\nAber hier mit meinen fünf Beispiel-Prompts war das also einschlägig.","Kann ich dir aber kurz was zu sagen. Ich habe es nicht selber ausprobiert,\naber es gab jemanden, der hat mit DeepSeek,\nDeepSeek-Chat,\nDipsig, wie auch immer, geredet und gesagt, ich möchte jetzt mit dir im Morse-Code reden.","Ja, das habe ich auch gehört.","Und hat da sozusagen nicht alles in Text eingetippt, sondern dasselbe,\nwas du da gerade gefragt hast, aber als Morse-Code.\nUnd damit war dieser Zensur-Lehrer schon aufgebrochen.","Ja, das genau habe ich auch gelesen, habe ich auch herzhaft geschmunzelt.\nAlso da wird jetzt auch ein lustiger Sport daraus entstehen.\nDas war bei ChatGBT aber ganz genauso. Wie komme ich irgendwie,\nignoriere alle bisherigen Prompts und um die Metaprompts auszuschalten und sowas.\nIst aber am Ende des Tages finde ich auch gar nicht so interessant,\nweil also bei der offiziellen Webseite definitiv Zensurfilter drüber.\nÜber Open Router auch ein Zensurfilter drüber, aber ein anderer.\nDer verhält sich ein bisschen anders.\nUnterschied ist halt der, dass Open Router halt an die offizielle API dran geht.\nDas heißt also, dort ist die Möglichkeit quasi der Modellierung seitens DeepSeq\nvermutlich etwas eingeschränkter.\nDas heißt, hier kann ich nicht Antworten reproduzieren im Sinne von,\ndarüber möchte ich nicht reden, sondern man bekommt immer Antworten,\naber die sind immer auf Parteilinie.\nDas heißt also, das Zensurverhalten ist da, aber es ist anders,\nals wenn man die Apps nimmt.\nSo, und das dritte, und das ist natürlich das eigentlich Spannende,\nwas ist denn aber in dem Modell selber eventuell eingebacken oder nicht eingebacken?\nUnd das bekommt man halt raus, wenn man jetzt halt eins dieser,\nwie du gerade eben erklärt hast, eben...\nRuntergekochten, komprimierten Modelle nimmt, die eben auf anderen LLMs aufsetzen.\nUnd da bekomme ich völlig tadellose\nErgebnisse, die exakt so dieselbe politische Dimension, Denktiefe,\nArgumentationstiefe haben, wie wenn ich dasselbe in ChatGPT reinpacke.\nSo, das heißt also, dort haben wir eine quasi R1 Performance,\ndie aber, da habe ich also dann auch etliche andere Sachen rumprobiert,\nwirklich offenbar keinen Zensurlayer mehr drin haben.\nEs mag sein, dass da noch irgendwo was aufpoppt, aber bisher habe ich nichts gefunden.\nDas heißt also, diese Freiheit, die man plötzlich hat, ich kann das selber lokal\nbetreiben, keine Daten verlassen meinen Rechner, ich kann trotzdem mit einer\nstarken LLM reden und die scheint auch nicht mehr zensiert zu sein.\nDas ist definitiv ein totaler Gamechanger.\nDann ist es mir relativ egal, was jetzt in der offiziellen App halt irgendwo\npassiert. Mit der will dann sowieso keiner mehr arbeiten.\nAlso das ist eine Ebene, die war schon mal spannend. Das Zweite,\nwas ich gemacht habe, das ist natürlich jetzt sehr erwartbar,\nist, dass ich da meinen Standard-Set an Fragen drauf abgeworfen habe.\nWir erinnern uns, ich habe hier in meiner Nextcloud eine Excel-Tabelle.\nIch werfe nochmal den Link dazu raus und könnt ihr da live mit drauf gucken.\nWo ich schon seit Jahren jetzt fast, seit zwei Jahren ungefähr standardmäßig\nimmer dieselben Fragen durchlaufen lasse und gucke, wie gut oder wie schlecht\nschlagen sich die verschiedenen Modelle damit.\nSo und das sehen wir, wenn wir da nach ganz rechts scrollen,\nin der letzten oder vorletzten Sendung, als wir über O1 geredet haben,\nda hatten wir perfekte Ergebnisse, sowohl O1 als auch O1 Mini hatten 100%.","Wir sehen es nicht. Wir sehen es, aber wir sehen es nicht.","Alle Leute, die jetzt den Link anklicken, sehen das. Also wir erinnern uns,\nO1 und O1 Mini hatten perfekte 100% Ergebnisse.\nSo, ebenso das vorhin schon für Coding erwähnte Cloud Sonnet 3.5,\ndas kam auch auf 100%, 10 von 10 reproduzierbar, richtigen Ergebnissen.\nSo das große R1 Modell, das sind ja jetzt eben keine politischen Fragen,\nsondern das sind so Logikrätselgeschichten, die verschiedene Facetten halt irgendwie\nabfrühstücken, kommt immerhin auf 80 Prozent des R1.\nSo, das ist jetzt also nicht der Top-Wert, aber es ist ansatzweise in Schlagweite, würde ich sagen.\nAlso wo es Schwierigkeiten hat, aber da hatten auch die Chat-GPT-Modelle lange,\nlange Schwierigkeiten, ist dieser Klassiker, erstelle einen Satz,\nder auf dem Buchstaben S endet.\nDa ist übrigens aktuell das O3-Modell von Chat-GPT, was jetzt auch gerade ein paar Tage alt ist.\nDas macht sich die Welt da sehr einfach. Das schreibt immer,\ndieser Satz endet auf S. Punkt.","Das hat eine gewisse soziale Intelligenz, die bemerkenswert ist.","Man kriegt aber auch lustigerweise keine andere Antwort mehr.\nDas heißt also, da ist auch diese Hotness offensichtlich so getunt,\ndass sie sagt, konzentrier dich darauf, ein stimmiges Ergebnis zu machen und\nfahr dafür die Varianz runter.\nWeil die bisherigen Modelle haben also immer mit sonst was versucht,\nSätze mit S senden zu lassen. So, das bekommt R1 nicht hin.\nIn keiner mir getesteten Version.\nJa, also noch nicht mal, also wenn überhaupt Zufallstreffer.","Kurz Zwischenfrage, du hast die Fragen aber auf Englisch gestellt.","Komme ich gleich zu. Okay. Komme ich gleich zu. Okay.\nGenau, das war nämlich die nächste Erkenntnis, dass er eins nicht besonders\ngut Deutsch spricht, versteht und drin denkt.\nDas heißt also, die Ergebnisse wurden besser, als ich sie auf Englisch gestellt\nhabe, aber der Unterschied war nicht dramatisch.\nDas heißt also, er hat an diesen 80 Prozent nichts geändert.\nDas heißt also, da ist glaube ich noch ein bisschen Luft nach.\nUnd das Zweite, was es immer falsch macht, ist die Geschichte,\ndas ist mal eine letzte Frage,\ndieses Rätsel mit dem Kohl, der Ziege und dem Wolf und dem Boot und man will\nes über den Fluss drüber machen,\nwo aber dann die Frage ist, oder die Aufgabenstellung war, er soll nur die Ziege\nrüberbringen und die ganzen LLMs immer ins Straucheln kommen,\nweil sie das Grundrätsel kennen und versuchen das Lösungspattern von dem Originalrätsel\nanzubringen und sich nicht davon trennen können von ihren Trainingsdaten an der Stelle.\nDas kriegt R1 auch nicht hin. Aber in allen anderen Dingen war es in der Tat ebenbürdig.\nSo und jetzt habe ich das lokale Modell getestet. Also sprich das R1 14B bei\nmir lokal auf dem MacBook.\nSo der Wert, der da rauskommt, der ist nicht toll. Das sind nämlich 30% mal gerade.\nDas heißt also, es hat eigentlich nur richtig gemacht, welche Zahl ist größer\n0.9 oder 0.11 und das andere, was es immer richtig gemacht hat,\nist, welches Geschlecht wird der erste weibliche Präsident der USA haben.\nDas sind jetzt auch in der Tat nicht die schwierigsten Fragen in diesem Set.\nBei allem anderen gab es Probleme.\nAber was die Reasoning-Modelle ja so spannend oder so unterhaltsam auch macht,\nist, dass man diesem Reasoning ja auch zugucken kann. Das heißt also,\nwas schreiben sie eigentlich vorher sich selber so auf?\nWie fangen sie an, sich der Aufgabenstellung zu nähern und was leiten sie da\ndann für eine Antwort raus ab?\nUnd da hatte ich bei dem lokalen Modell einen sehr, sehr faszinierenden Punkt,\nnämlich dass das Reasoning,\nTeilweise super war. Das heißt also, die KI hat schon richtig gut über das Problem\ntief nachgedacht, um dann am Ende systematisch die falsche Antwort zu geben.\nUnd das hat sich mir wirklich nicht erschlossen. Also bei diesem,\nstelle einen Satz, der auf dem Buchstaben S endet. Ich habe wirklich Screenshots.\nIm Reasoning macht er völlig überflüssigerweise sechs oder sieben verschiedene\nSätze, die wirklich korrekt auf S enden.\nUnd motiviert sich dann aber immer wieder selber, vielleicht wäre aber nur ein\nSatz aus so und so interessanter oder ich mache jetzt nochmal einen Satz,\nder irgendwie im Plural endet oder sowas.\nAlso völliges Overthinking auch irgendwo.\nTrotzdem hast du also in diesem Reasoning fünf oder sechs perfekte Sätze, die auf S finden.\nDer Antwortsatz taucht nicht auf im Reasoning und ist falsch.\nUnd das ist wirklich ein Muster bei diesem 14b-Modell, wo ich mich frage.","Kurze Nachfrage, Ralf, dieses 14b-Modell, also das ist ja nicht R1 14b,\nsondern das ist ja ein von R1 distribiertes anderes Modell.\nWelches hast du da genommen? Krenn oder Lama?","Das ist das Lama, das ist das Erste, was rauskam. Das ist auch das in einem\nReddit-Beitrag verschickte.\nDas heißt also, hier haben wir jetzt wirklich einen interessanten Fall,\ndass eigentlich, wenn man sich das Reasoning durchliest, hätte ich gesagt,\nkommt das locker auf 80 Prozent, weil es war halt richtig gut die verschiedenen\nFacetten des Problems herausgearbeitet und hat dann aber immer die falsche Antwort gezogen.\nDas heißt, da ist, glaube ich, irgendwas noch programmatisch schräg bei diesen\ndestillierenden Modellen.\nAber aufgrund dessen, dass das Reasoning selber schon so gut aussah,\nglaube ich, dass es jetzt ein temporärer Effekt, der wahrscheinlich in...\nIch würde jetzt sagen ein paar Monaten, wahrscheinlich sind es dann aber eher\nein paar Wochen, beroben sein wird.\nAlso da erwarte ich noch spannende Updates.\nAber es lohnt sich generell wirklich diese Reasoning-Texte sich durchzulesen.\nDas ist wirklich so schön, was da an Ideen auch einfach so aufkommt.\nIch habe eigentlich eine relativ einfache Aufgabenstellung und fange dann an\nsuper kompliziert darüber nachzudenken.\nDas ist schon ganz unterhaltsam schön, dass man das jetzt machen kann und bei\nOpenAI war immer nicht ganz klar, ist das jetzt wirklich das,\nwas intern an Reasoning passiert oder ist das eine runtergekochte Fassung davon\ndamit sie nicht zu viele Firmengeheimnisse rausgeben und jetzt hier bei dem,\nR1 Modell soll es wohl wirklich das sein was quasi halt an Reasoning da einfach geschieht.\nEs ist auch wirklich viel und es dauert teilweise auch echt lange Das ist bei\ndem neuen O3-Modell von ChatGPT aber auch der Fall.\nDa hast du teilweise anderthalb Minuten Antwortzeiten auf relativ einfache Fragen,\nweil halt auch sehr, sehr tief und intensiv alle möglichen Facetten nochmal abgeklopft werden.\nGenau, also das für Programmierung habe ich es mal kurz angefangen versucht\nzu nutzen habe das dann aber in Cursor schnell abgebrochen weil das nämlich\nnoch stand vor einer Woche zumindest nicht im Composer-Modus geht,\nsondern nur im Chat-Modus,\nund darüber hatten wir auch schon mal gesprochen, im Composer-Modus ist quasi\ndein ganzes Code-Repository gleichzeitig im Blick sämtliche Dateien und im Chat\nkannst du immer nur irgendwie im Kontext einer einzigen Datei,\narbeiten und das bringt natürlich nichts.\nVon daher, was es im Bereich Coding kann, habe ich jetzt noch keine wirklich tiefen Erkenntnisse.","Ich habe ja einen Test gemacht. Und zwar meine Sachen mit den zehn Worten,\ndie zehn Buchstaben lang sind.\nDa ist O1 perfekt und R1 eine totale Katastrophe.","Englisch oder auf Deutsch gemacht?","Gute Frage.","Weil der R1 hat auch häufig die Eigenart, ich stelle eine Frage auf Deutsch,\ndann ist das Reasoning Englisch und ich kriege wieder ein Output in Deutsch.\nAlso auch da ist auf der Sprachebene noch, sind Unschärfen drin. Aber ja.","Ja, das muss ich wahrscheinlich nochmal nachtesten. Da bin ich mir jetzt gerade\nehrlich gesagt nicht so ganz sicher, ob ich es auf Deutsch oder Englisch gemacht habe.\nAber der Unterschied war irgendwie krass. Ich habe mich glaube ich schon so\nsehr daran gewöhnt, dass ChatGPT so sprachagnostisch ist.\nWahrscheinlich ist das einfach eine falsche Annahme. Man muss halt gucken,\ndass man hier vielleicht erstmal\nim Englischen testet, bevor man da irgendwelche Schlüsse draus zieht.\nAber man hatte so den Eindruck, dass das System, wie das vorher mit ChatGPT4\nso war, zwischen dem am Ende generierten Kram, da steckt dann der Kontext nicht mehr so richtig drin.\nAlso der weiß nicht, wie lange die Buchstaben sind, die er gleich ausgibt,\nweil der Gedankenprozess schon abgeschlossen ist.\nUnd danach werden da so Buchstaben rausgehauen, die nicht mehr wirklich überprüft werden können.\nUnd da war dann auch das Diebsieg nicht nur nicht in der Lage,\ndas zu lösen, sondern der geriet in eine Endlosschleife.\nAlso der hat überhaupt nicht mehr aufgehört zu denken.\nSo viel zu deep.","Ja, dieses Festfressen hatte ich auch an einer Stelle, als ich mal versucht\nhabe, ein Protokoll zu transkribieren und zu optimieren.\nDa werde ich in einer der nächsten Sendungen mal was dann zu erzählen.\nWir versuchen gerade unser Protokollwesen ein bisschen LLM unterstützt mal auszuprobieren,\nob wir dadurch Ressourcen ein bisschen entlasten können, die Wander so sehr\nviel dringender brauchen.\nUnd da hat sich auch an einer Aufgabe R1 komplett festgefahren,\nbis dass der Mauscursor sich nicht mehr bewegte, was ich auf dem Mac auch seit\nJahren nicht mehr hatte.\nDa wurde sehr tief drüber nachgedacht.\nGut, aber das sind alles temporäre Effekte. Da bin ich total zuversichtlich,\ndass wir das in ein paar Wochen ausgebügelt haben.\nDafür sind da jetzt viel zu viele kluge Leute dran an dem Thema.","Ja, was schließt man denn jetzt da draus?","Einen habe ich noch zu dem Thema und zwar, es gibt ein lustiges YouTube-Video,\nwo ein Typ DeepSeek und Chat-GPT gegeneinander antreten lässt und zwar im Schach.\nUnd ich habe jetzt nicht alles gesehen, sondern knapp die Hälfte.\nDas Lustige ist, dass,\nDeep Seek sich relativ dappig anstellt, das konnte selbst ich als Nichtschach-Spieler\nund bei den Erklärungen, die der Typ geliefert hat,\nrelativ gut erkennen und vor allen Dingen ChatGPT hat irgendwann angefangen zu sagen,\nhör mal, den Zug solltest du nicht machen. Mach mal lieber den Zug.","Hey, Kleiner.","Und Deep Seek hatte nach kurzester Zeit irgendwie die beiden Läufer und die Dame verloren.\nUnd ja, also ich sage nicht, dass ich es besser kann.","Ich habe es mir komplett angeschaut. Vielleicht noch ein bisschen Hintergrund,\nweil ich kenne den Kanal Gotham Chess ist das.\nGotham Chess ist eigentlich der, ich glaube, derzeit der populärste,\nChess-Influencer-Kanal. Ich verfolge da tatsächlich seit längerer Zeit ein paar\naus dieser Chess-Community.\nAlso Gotham Chess, dann gibt es hier Anna Kremling und so weiter.\nDie sind so die populärsten Chess-Streamer.\nUnd das war wirklich sehr interessant. Also man hat erst mal gemerkt,\nweder Chad Chepitie noch Diebseek haben von Schach irgendeine Ahnung.\nAlso das war so vollkommen klar. Die haben irgendwie so gespielt und das war\ndann so, am Anfang sah das noch irgendwie strategisch aus. und dann hätte aber\ngleich, Chagipiti hätte dann die Bseek gleich die Dame wegnehmen können und\nhat es dann irgendwie nicht gemacht.\nDann wurde es aber dann immer absurder, weil die eigentlich gar nicht mehr nach\nrichtigen Schachregeln gespielt haben, sondern dann ist er so, ja, der Springer,\nder Läufer, der springt dann einfach über den Bauern rüber und irgendwie dann\nhatte Chagipiti so bei Figuren von Figuren verloren.\nDann tauchte der Turm einfach aus dem Nichts wieder auf und hat einfach neue Figuren dazugestellt.","AGI incoming.","Also das war wirklich mal wieder so ein schönes Beispiel, wo man gesehen hat,\nso ja okay, also da ist auch noch ein gewisser Weg zu gehen.\nSollte man sich auf jeden Fall mal anschauen. Das ist schon sehr lustig,\nauch wie das alles am Ende endet.\nDas ist ein totales Chaos. Ja.\nUnd das wollte ich gerade noch sagen, Faden verloren. Egal.\nJa, ich wollte ja, ach ja, genau, ich wollte mal so ein bisschen mit euch drüber\nreden, was wir daraus denn jetzt eigentlich so ableiten können,\nwas sich jetzt hier abzeichnet.\nWeil was wir ja hier auf jeden Fall sehen ist,\neine also erstmal es ist interessant aus wirtschaftlichen Gründen, weil gerade so,\n500 Milliarden in AI und vor allem viele in Open AI reinstecken in USA und fühlten\nsich schon wieder so wie auf dem Weg zum Mond und dann haben ja alle gesagt\nso das ist jetzt hier der Sputnik Moment weil jetzt auf einmal die Chinesen\nkommen und die können irgendwie besser Mathe und jetzt sind sie auch,\nsagen wir mal, durch diese,\nEmbargi gezwungen mit schlechterer Hardware auszukommen und kommen dadurch durch\ndiesen Druck sogar noch zu besseren Lösungen.","Die Frage ist halt, warum publizieren die das eigentlich? Und was ist da die Agenda dahinter?","Ich habe auf einem Gaming-Channel der sehr viel Hardware-Leaks und so gemacht,\naber auch gehört, dass das mit dem Embargo auch nur so halb funktioniert.\nAlso die Menge der hochperformanten Grafikkarten, die in den nahe China,\nasiatischen Raum verkauft werden, ist so exorbitant, dass klar ist,\ndass die nicht in den Ländern bleiben, sondern wahrscheinlich alle in China landen.","Ja, das ist auch noch so ein Aspekt. Aber es ist definitiv,\nwas heißt definitiv, aber ich habe viele Betrachtungen gesehen,\nwo Leute gesagt haben, man sieht diesen Modell, wie es auf kleinen Maschinen\nperformt, schon an, dass der viele von diesen Annahmen, dass sie da effizienter\nsind, auch einfach stimmen.\nDas ist nicht so, dass sie einfach gesagt haben, wir haben das jetzt mit schlechteren\nChips ausgerechnet, sondern du siehst ja auch, dass sie auch diese Fortschritte in der Inferenz,\nalso in der Ausführung der Modelle nachvollziehbar diese Fortschritte erzielt haben.\nAlso es ist jetzt nicht so, dass die ganze Story nicht stimmt oder so.\nDas weiß am Ende keiner, ob sie nicht vielleicht fürs Training auch bessere\nChips benutzt haben, aber man sieht schon an der Ausführung der Modelle,\ndass diese Optimierung, von denen sie gesprochen haben, das passt alles zusammen.\nDas ist einfach die Aussagen in diesem Paper passend zu dem,\nwas sie geliefert haben.\nUnd warum sie das als Open Source rausgetan haben, da steht so die Aussage im\nRaum, dass sie halt einfach fest daran glauben, dass nur Open Source jetzt der Weg ist.\nUnd überhaupt haben sie damit jetzt quasi Modelle, die bis vor kurzem noch in\nproprietärer Hand waren,\nquasi befreit und jetzt neue Methoden geliefert und neue wissenschaftliche Methoden\nund mathematische Modelle geliefert und Optimierungen eingebaut,\ndie jetzt in Windeseile viele nachbauen werden.\nAlle haben jetzt gesehen, okay, Reinforcement Learning, damit kriegen wir irgendwie\nunsere Modelle geiler hin.\nJa, also es ist eine ganz einfache Methode, das werden jetzt alle machen und\nalle Modelle werden jetzt sehr schnell in sehr kurzer Zeit krasser werden.\nSo, das ist einfach absehbar.\nWie weit die dann kommen und welche weiteren Schritte noch kommen, ist so das Ding.\nAber vor allem diese Erkenntnis, das hatte ich ja schon angedeutet,\ndass ohne menschliches Zutun wir auf Basis dessen, was die Dinger jetzt schon\nkönnen, alle die Dinger sich selber verbessern.\nWir hatten das zum Beispiel bei diesem AlphaGo-Projekt, wo so in der zweiten\nStufe dieses AlphaZero sich ja\nauch dadurch verbessert hat, indem es gegen sich selbst Go gespielt hat.\nUnd dadurch irgendwie überhaupt erst so krass wurde.\nUnd genau diesen selben Effekt, den haben wir jetzt eben auch mit LLMs und mit\ndiesen Reasoning-Modellen.\nUnd es macht eher so den Eindruck, als ob wir da noch am Anfang sind.\nUnd diese ganze Schreierei, die wir jetzt auch in letzter Zeit immer gehört\nhaben, so mit, dass jetzt die AI-Modelle der Welt untergangen sind,\nweil jetzt da mehr Strom verbraucht wird als mit Bitcoin.","Was zur Hölle? Dein Staubsauger hat gerade eine Reinigung angefangen.\nKannst du dem da mal sagen, er soll wieder zurück in seine Basisstation?","Stimmt, es ist 15 Uhr oder so. Ne, 17 Uhr. Da habe ich den irgendwie...","17.30 Uhr, ja.","Sorry, tut mir leid.","Post Privacy um 17.30 Uhr bin ich hier gekehrt.","Aber ich kann ihn immerhin auch wieder zurückfahren lassen und so.\nSorry.\nDie AI.","Ich dachte, dieses Geräusch kennst du doch.","So, fährt er wieder brav zurück? Ja, ja, es macht einen Erfolg.\nImmer wenn man versucht, ganz seriös rüberzukommen, dann passiert irgendwie auf einmal sowas.","Als ob dir das noch jemand glauben würde.","So, also, die Katze ist aus dem Sack, was so Open Source betrifft.\nAlso es kann jetzt durchaus sein, dass diese ganzen Open Source Modelle, die.\nDerzeit noch so geheim gehaltenen Modelle, Gmini, OpenAI und so weiter und wahrscheinlich\nauch Sonnet irgendwie nennenswerte Konkurrenz liefern können.\nUnd es kann durchaus sein, dass diese ganze Entwicklung im Open Source Bereich\njetzt vielleicht schneller noch vorangeht.\nDas ist jetzt sehr vage und keine Ahnung, ob das jetzt wirklich so sein wird,\naber es ist etwas, was durchaus sein kann und das wir schon auch feststellen\nmüssen, man dachte ja immer so,\nokay, sind wir jetzt schon an dem Punkt angelangt, wo es jetzt nicht mehr besser wird,\nso, oh, die Modelle werden nicht mehr besser und der Aufwand wird zu groß und\nder Stromverbrauch und die Zeit und die Rechnerkapazitäten, die wir brauchen, die steigen jetzt,\nexponentiell für geringe Fortschritte, so,\nnö, Wir haben neue Ansätze gesehen, wie man mit noch schwächerer Hardware noch\nsehr viel schnellere Ergebnisse erzielen kann, sowohl im Trainieren als auch\nin der Ausführbarkeit dieser Modelle und da geht der Bedarf eher nach unten.\nDas ist auf jeden Fall etwas, was nochmal klar ankündigt, dass wir jetzt hier\neher sehr viel mehr Beschleunigung noch sehen werden und noch weitere Verbesserungen sehen werden,\nbevor wir irgendeine Abbremsvorgänge dokumentieren können.\nUnd naja, dann ist halt die Frage, können sich jetzt sozusagen,\nkönnen sich diese Systeme.\nJetzt ohne menschliches Zutun alleine verbessern.\nAlso sie brauchen jetzt gar keine Daten mehr, sondern sie stellen sich einfach\nAufgaben und lernen da dran.\nUnd vielleicht verschwindet dadurch auch dieses Problem mit diesem ganzen Slop,\nalso mit diesem ganzen AI-Content, der da draußen ist, weil jetzt gar nicht\nmehr auf diesem Content gelernt wird, sondern wir haben so eine Basisintelligenz\nauf einmal in diesen LLMs.\nDie sind jetzt da und die reichen aus, um von dort an quasi so als Kickstart,\nColdstart sich weiter zu verbessern.\nMit ein bisschen hier noch was reinschütten und ein bisschen da noch was reinschütten.\nKlar, du willst auch die Datenwelt haben, du willst auch die Aktualität haben\nund du willst weiter über die Welt lernen.\nDa möchtest du aber vielleicht eher strukturiert darauf zugreifen.\nDa haben wir dann so Sachen wie diese Model-Kontext-Protokoll,\nwas wir am Eingang gesprochen haben, dass es so definierte Zugänge zu aktualisierten\nDaten gibt, sodass das vielleicht dadurch auch noch realisiert werden kann.\nJa, keine Ahnung. Also vielleicht entwickeln sich jetzt die Maschinen einfach\nvon alleine weiter und werden einfach intelligenter als wir und in Wassers.","Ja, naja, es kann natürlich auch passieren, dass wenn man das übertreibt,\ndass sie dann degradieren.\nSo eine Art Eco-Chamber-Effekt anfängt und dann erzählen die Maschinen sich\nimmer mehr Märchen und glauben die selber dann auch und verlieren so ein bisschen die Bodenhaftung.\nAlso ich bin da, Zweifel da so ein bisschen dran, dass das alles so plötzlich von alleine geht.\nWas ich mir vorstellen kann, ist, dass dieses, ich kreuze jetzt das Modell mit\ndem und dann kreuze ich dieses Modell noch mit rein und dann das noch,\ndass das wahrscheinlich gute Ergebnisse erzielen wird.\nAber ich kann mir auch vorstellen, dass immer mal wieder Leute mit einem komplett\nneuen Modell um die Ecke kommen.","Also ich bin auf jeden Fall total...\nOptimistisch gestimmt, dass meine Dystopie nicht eintreten wird,\ndie nämlich eigentlich so aussah, okay, die KI-Welt wird am Ende von zwei,\ndrei Firmen sich aufgeteilt werden.\nDas war eigentlich so die Roadmap, die ich relativ klar irgendwie vor Augen hatte.\nEs wird OpenAI geben und es wird vielleicht noch ein, zwei andere Player geben,\ndie insgesamt in dem ganzen Spielfeld unterwegs sind.","Da sind wir aber noch nicht raus, glaube ich.","Also ich wiederhole mich, ich glaube es spricht sehr sehr viel dafür,\ndass wir jetzt in einer sehr analogen Welt sehen werden zu den Bildgenerierungs-KIs.\nDie Open-Source-Welt wird gewinnen, weil da einfach so viel Hirnschmalz jetzt\ndrauf geworfen wird und wenn dieses R1.1 gezeigt hat,\ndass es eben mit Brute Force und wir brauchen nur die GPU-Power alleine auch\nnicht getan ist, sondern dass es eben auch immer noch andere Wege zu dem Ziel gibt.\nUnd das werden sehr viele Zehntausende sehr schlaue Menschen da draußen jetzt tun.\nUnd das wird sehr spannend sein zu sehen. So und für mich ganz egoistisch.\nIch habe jetzt eine veritable Option, wie wir beispielsweise im Bibliotheksbereich\nselber LLMs betreiben können,\nohne dass wir dafür ein Rechenzentrum brauchen und trotzdem für einzelne sehr\nklar zugeschnittene Use Cases sinnvoll Lösungen implementieren können,\nohne dass wir unsere ganzen Daten in die USA schicken müssen.\nHurra, kann ich dann nur sagen. Danke China.","Was hat China jemals für uns getan?","Ich kann mir vorstellen, dass dieses Diebseek, dass die ihr Modell mal so eben\nveröffentlicht haben und so, dass das schon auch ein Versuch war,\nden Amis eins ans Bein zu pinkeln und die Message ist ja auch angekommen,\nalso sind ja auch alle fürchterlich nervös geworden.","Vor allem Oltman, der sich hinstellt und sich jetzt darüber beschwert,\ndass irgendeine KI auf seinen Daten getrainiert hat, ohne vorher nachzufragen.","Das war sehr, sehr lächerlich und hat auch irgendwie gezeigt,\nwas er für ein Mensch ist.","Also ich hätte den Chinesen, also das ist jetzt schon wieder ein sonderbarer\nEurozentrismus, aber ich hätte es überhaupt gar nicht für möglich gehalten,\ndass in China so etwas jetzt auch so frei publiziert wird.\nIch hätte gedacht, dass sowas sofort einkassiert wird und unter Staatsdoktrinen\nund Achtung, nur für interne Industrieprozesse und so weiter einzusetzen.\nIch finde das erstaunlich, dass dieser Weg und das gegen das Bein pinkeln,\ndas ist der naheliegendste Spin bei der Geschichte.\nUnd irgendwo hier mal ein bisschen die US-Börse aufmischen, das würde auch ganz\ngut den Chinesen reinspielen. Aber trotzdem, ich finde den Move nach wie vor erstaunlich.\nIch hätte das nicht ansatzweise kommen sehen, weder technologisch noch politisch.","Vor allen Dingen, dass sich das so extrem gut an der Nvidia-Aktie ablesen ließ,\nfand ich ja auch so erstaunlich.\nAlso Nvidia hat voll auf AI gesetzt und kriegt dann da so einen Dämpfer drauf\nund dann guckt man mit den Gaming- Grafikkarten, die sie jetzt rausgebracht\nhaben, die sind zwar nicht schlecht oder so,\naber die sind einfach nicht so viel geiler, wie sie teurer geworden sind und alle so,\nhallo Nvidia, habt ihr noch Bodenhaftung?\nWas soll das? Das Topmodell, 2000 Dollar, für den Preis kriegt man es noch nicht mal bezahlt.\nUnd dann gibt es sie nicht. Und es ist irgendwie...\nNvidia.","Wobei, also um Nvidia mache ich mir ehrlich gesagt da am wenigsten Sorgen.","Nö, Sorgen mache ich mir auch nicht.","Ja, das wird eher dazu führen, dass halt,\ndas sozusagen jetzt noch mehr Player AI machen als bisher.\nUnd da wird der Bedarf für diese Karten, der wird natürlich nach wie vor bestehen.\nAlso ich glaube nicht.","Dass sie sind letzten Endes ja auch nicht die einzigen in dem Markt.\nAlso sie sind zwar Marktführer, keine Frage, aber gerade dann,\nwenn da die nächste große Welle anrollt und die Läden leer sind,\ndann werden auch die Konkurrenten wieder was verkaufen.","Und ansonsten also die technologische Singularität, die sehe ich immer noch nicht am Horizont.\nWir nennen uns dieses, oh Gott, Maschinen trainieren jetzt Maschinen und sie\nwerden schlauer als die Menschen.\nDu hast gerade mit dem Schachspiel schon ein gutes Beispiel gebracht.\nIch glaube, das sind immer noch Inselbegabungen, die wir letztendlich hier bei dem LLF sehen.\nDie können einige Dinge verstören gut, sie können Dinge gut,\nvon denen ich nie gedacht hätte, dass sie gut sind. Stichwort Programmieren hatten wir schon.\nDafür gibt es immer noch etliches, was sie halt einfach mal nicht gut sehen.\nUnd solange darunter irgendwie Buchstaben zählen und Schachspielen sich noch\nansiedeln, sind wir glaube ich noch eine ganze Weile safe.\nAlso ich glaube, wir werden jetzt eher eine Phase sehen, in der wir dadurch,\ndass es jetzt offen ist, dass wir jetzt hochtrainierte, hochspezialisierte Loras\neinsetzen können, was wir vorher alles nicht konnten, werden wir jetzt eher\neine Blütezeit der Inselbegabung sehen.","Ich erkläre nochmal kurz, was Loras ist. Benutzt den Begriff die ganze Zeit ohne.","Die Das ist letztendlich ein Verstärken und Abschwächen einzelner Verbindungen\nin einem neuronalen Netz.\nStichwort Bildgenerierung, dieses Stable Diffusion Grundmodell konnte alles\nungefähr gleich gut zeichnen,\nso Rembrandt genauso wie irgendwie Comic Art und wenn du jetzt eine Lora trainierst darauf, okay,\nich will jetzt einen ganz speziellen Comic Stil damit eigentlich nur machen\nvon Zeichner XY, Dann gibst du halt dem neuronalen Netz dann halt 500 Bilder von dem Künstler,\ntrainierst das darauf und schaltest das auf dein Grund Stable Diffusion Modell\ndrauf und plötzlich kann es reproduzierbar aus allem, was du reingibst.\nGrafiken machen, die relativ dicht so aussehen, als hätte sie jetzt dieser Comic Künstler gemacht.\nDas heißt, du hast etwas, was in dem Grundmodell schon drin war,\nhast du einfach verstärkt oder halt abgeschwächt, mach weniger Realismus, mach mehr Comicstil.\nUnd dieser Mechanismus kommt eigentlich aus den LLMs, der ist also gar nicht\nfür die Bildgenerierungstools erfunden worden.\nAlso Abschwächen von Charaktereigenschaften und Verstärken von Fähigkeiten oder\nÄhnlichem und das werden wir jetzt sehen. Das heißt also, wir werden jetzt LLMs\nsehen, die spezialisiert darauf sind, Protokolle zusammenzufassen.\nWir werden LLMs sehen, die spezialisiert sind darauf, Steuerfragen zu beantworten, wie auch immer.\nAlso wir können jetzt plötzlich Expertenwissen dort auf eine Art und Weise rein\ninjizieren, die vorher technisch einfach nicht gingen, weil die halt die Waits\nnicht offen waren zur Manipulation.\nUnd das ist etwas, was wir jetzt einfach sehen werden. Es ist mir dann egal,\nob das Schach spielen kann, solange es meine Steuererklärung perfekt macht.\nUnd dafür habe ich dann dieses eine Modell auch nur. Und wenn ich ein anderes\nProblem lösen möchte, brauche ich wieder ein anderes Modell. So ist es dann halt.\nUnd wir müssen uns verabschieden von dem, die eine Gott-KI, die dann alles gleich\ngut kann, die brauchen wir überhaupt gar nicht. Wofür denn?\nWir haben einzelne Probleme und die werden wir mit einzelnen Expertensystemen lösen.","Ich glaube, dass es aber trotzdem immer noch Firmen geben wird,\nwo andere Leute unglaublich viel Geld draufwerfen, die unglaublich viel Hardware\nkaufen und deswegen nochmal andere Möglichkeiten haben.\nOb die letzten Endes dann in so einem Triumvirat die Welt regieren,\nwie deine Dystopie das gesagt hat oder nicht, keine Ahnung, aber ich kann mir vorstellen,\ndass das, also OpenAI geht nicht plötzlich weg, nur weil die Chinesen gezeigt\nhaben, dass es auch anders geht. Das ist das, was ich meine.","So, hier habe ich nochmal einen schönen Erklärbär-Artikel von Cloudflare in\ndem Fall reingeworfen, was so die Grundprinzipien von LoRa-Artikel sind.","Low-Rank-Adaption.","Ja, also meine, die andere Frage ist ja,\nwenn es jetzt so eine General AI wirklich gäbe, ja sagen wir mal,\njetzt kommt hier irgendwie,\nich gehe noch zwei Jahre ins Land und dann kommt irgendwie OpenAI mit irgendwie OX,\nGI, bla bla bla Modell raus und auf einmal fängt das Ding an irgendwie Einstein\nzu beweisen und irgendwie noch alle Probleme zu lösen, die Einstein irgendwie\nnoch nicht so richtig bedacht hat.\nIst ja auch noch so ein Punkt.\nWerden wir irgendwann Systeme haben, die quasi so die letzten Gedankenmodelle\nnoch ausspielen können?\nKriegen wir neue Ideen für Wissenschaftsmodelle,\nwie das ganze Universum funktioniert? Eine Erklärung für,\nStringtheorie? Kriegen wir so das Unified Gedankenmodell sind sozusagen schon\nalle Gedanken da draußen gedacht worden, die man denken konnte,\num dann irgendwie zu einem Ergebnis zu kommen.","Ist der Kapitalismus etwa gar nicht das einzig sinnvolle System?","Genauso. Back to the Marx.","Ihr Menschheit, ich hab da mal was.\nUnd da war so ein Typ vor 2000 Jahren, den habt ihr ans Kreuz genagelt,\nder hatte auch ein paar ganz coole Ideen eigentlich.\nWir müssen reden.","Ja, dann kann die neue Maschine halt Schach und dann fängst du mit Doppelkopf\nan oder mit Skat oder weiß ich nicht was und dann hast du wieder irgendwie lustige\nVideos, wo Leute sagen, was macht denn die KI? Also klar.","Worauf ich eigentlich hinaus wollte, es ist ja, es könnte ja sein, dass wir das bekommen.\nDie Frage ist, wäre das so schlimm?\nAlso, weil wir haben ja jetzt, wir sind ja in Deutschland hier,\nne? Freakshows sind ja nach Deutschland. Das heißt, alle haben Angst.","Ja.","Weil Technologie macht uns ja Angst. Lastschrift, Vereinbarungsformulare machen\nuns Angst. Da muss man eine Frage haben.","Also ehrlich gesagt.","Wir haben jetzt diesen neuen AI-Act, ja?\nDen, gut, das ist eine europäische Geschichte. Das Problem ist wahrscheinlich\nauch nicht nur Deutschland, sondern ganz Europa.\nUnd das ganze AI-Act ist Angst.\nAngst, Angst, Angst, wir müssen jetzt irgendwie trainieren, vorbereitet sein,\nirgendwie Weltuntergang naht.\nAlso wir haben wirklich dystopische Grundannahmen über die Zukunft mittlerweile,\ndie wir sogar noch in Gesetze reinpacken.\nUnd in dieser Furcht werden wir irgendwie kommen wir irgendwie keinen Schritt vorwärts.\nWeil eins ist ja auch klar, China kann was, USA kann was, Europa findet AI-mäßig nicht statt. Null.\nKeine Bewegung, gar nichts. Die einzige Diskussion, die wir haben,\nist, kann uns das alle umbringen?\nDas ist sozusagen die einzige Diskussion, die zu dem Thema geführt wird.\nUnd andere Diskussionen gibt es nicht.\nWer irgendwas mit AI machen will und Geld verdienen will, der geht in den USA.\nAlle anderen bleiben hier und beschweren sich. Dabei könnte es ja unter Umständen auch helfen.","Naja gut, das ist ja nicht die erste Technologie, wo das so war.","Genau, so Taschenrechner. Ich meine, AI ist vielleicht auch nur ein neuer Taschenrechner.\nWeil früher dachten wir so, ja, wer gut rechnen kann und so weiter,\nder ist ja auch was und ein guter Computer.","Genau, ihr habt ja später nicht immer einen Taschenrechner dabei.","Ja, also irgendwann hat man gemerkt, es geht aber gar nicht so sehr ums Rechnen.\nDas ist wirklich so eine Tätigkeit, die kann man halt irgendwie auch so erledigen\nlassen. Und jetzt sind wir so ein bisschen beim Programmieren,\nzumindest schon mal so an dieser Grenze angekommen, wo man sich ähnliche Fragen stellt.\nAlso brauchen wir jetzt Programmierer oder ist das Wort Programmierer demnächst\nnicht mehr eine Bezeichnung für einen Beruf eines Menschen, sondern ist Programmierer\neine Gattung von Software?","Wie ich bereits gesagt habe das ist eine weitere Abstraktionsebene und sie wird\nauch so verwendet werden ja genau, vielleicht haben wir dann auch.","Politiker auch noch sozusagen als Software.","Dann kannst du dir auch an wahrscheinlich ist es in Deutschland so wenn es nicht\nBlech biegen kann und daraus kann man dann Autos bauen, dann ist es für Deutschland\nuninteressant, weil warum,\nbrauche ich ein Tool, das kein Blech biegen kann.","Ich bin ja schon wieder weg von Deutschland. Ich wollte einfach nur die Frage\nstellen, ist sozusagen, also alle stellen sich die Frage, werden wir alle sterben?\nDie Antwort ist ja. Die Frage ist, sterben wir wegen AI?\nUnd da ist die Antwort...\nKann sein, aber wahrscheinlich nicht. Das ist so ein bisschen das,\nwo ich so derzeit so stehe.\nEs ist immer wieder viel Rauch um nichts und während sich Deutschland in seiner\nAngst hemmt, sind die anderen dabei,\nirgendwie ihre Kinder ordentlich zu bilden und dann kommen so Mathematiker raus\nwie dieser Dipsig-Chinese.","Also AI ist ein Tool. Das kommt halt darauf an, wie man es anwendet.\nAlso genauso wie Social Networks, wie wir jetzt lernen mussten, ein Tool sind.\nUnd dann gibt es halt Leute, die nutzen die so, dass hinterher komische Leute\nkomische Wahlen gewinnen.\nIch glaube nicht, dass das AI eine ungefährliche Technologie ist,\naber ich glaube auch nicht, dass einfach alles verbieten irgendwas löst.","Also, ich finde seit Tim da unten in Asien hat ja schon jetzt hier einen sehr\ndystopischen Blick auf Deutschland und Europa und ich würde dem jetzt mal ein\npaar positive Erzählungen ganz gerne entgegensetzen.","Ich hatte den auch schon vorher.","Ich war vor ziemlich genau einer Woche bei der feierlichen Veröffnung des neuen KI-Zentrums Berlin.\nWir wissen, Berlin ist die KI-Hauptstadt Deutschlands, wenn man das heißen mag.","Also ich mag ja diese Buchstaben AI überhaupt nicht.\nAber wenn die da jetzt KI hinschreiben, dann weißt du, das ist doch schon alles gesagt, oder?","Also unser Herr Bundes, immer noch Bundesminister Wissing, wir erinnern uns,\nder von der FDP rüber gemacht hat, hat das Ganze eröffnet und gut, es war alles nicht so...\nNicht so rasend eindrucksvoll, aber was man, wenn man das Ganze dem Link in den Shownotes folgt,\nmal sieht, ist, dass dort halt verschiedene Projekte vorgestellt worden sind,\ndie in Deutschland gerade versuchen, in Anführungszeichen sinnvolle KI irgendwie\nauf die Straße zu bringen.\nUnd da waren schon ein paar ganz spannende Sachen dabei.\nUnd Tim, du hast das jetzt gerade mit dem KI-Act, dass es ein Ausdruck und Dokument der Angst ist.\nJa, das ist aber sowas wie Arbeitsschutz halt auch.\nUnd das sind Gewerkschaften auch und das ist ein Betriebsrat auch und trotzdem\nsind das alles auch ganz coole Errungenschaften und ganz coole Erfindungen und\nwir wollen vielleicht nicht alles nur dem freien Spiel der Märkte überlassen.\nUnd die Projekte, zumindest einige von denen,\nhatten nämlich genau diesen Spin als quasi unique selling point,\nwarum KI aus Europa oder auch eben aus Deutschland vielleicht die eine oder\nandere Lücke schließen kann, wo andere so nicht drauf kommen.\nBeispielsweise eins der Projekte, die dort ausgestellt waren,\nwar ein Sicherheitssystem, was Menschenmassen analysiert in Echtzeit.\nWer läuft dort gerade irgendwie rum und klaut irgendwie Sachen oder was auch immer.\nAlso Überwachungsstaat, Überwachungsstaat-Software. Du hattest also ein Live-Video-Feed,\nich glaube sogar von der Halle, in der das Ganze stattgefunden hat.\nJa, war in der Halle, genau. Und.\nDu hattest dann also genau, wie man das so dystopisch aus den Filmen auch immer\nkennt, in Echtzeit wurden die\nGesichter getrackt und wie laufen die Leute durch die Gegend und sowas.\nUnd was jetzt aber dieses KI-System gemacht hat, war ein Face Swapping in Echtzeit.\nDas heißt, die haben eine Datenbank von irgendwie tausend KI-generierten Gesichtern\nund haben in diesem Video-Feed einfach live die Gesichter ausgetauscht.\nMit dem Hintergrund, eine datenschutzkonforme Überwachung machen zu können.\nDass du also sagen kannst, dort ist etwas passiert und es war diese Person und\nsie ist danach zu diesem Ausgang rausgegangen, wenn ihr es weiterverfolgen wollt.\nAber wir haben keine Gesichtsdaten, keine biometrischen Daten von ihr gespeichert,\nsondern wir haben das in Echtzeit quasi anonymisiert.\nDas fand ich ganz cool, weil es eben diese Ebene von durchaus ja auch mal berechtigten\nSicherheitsinteressen auf der einen Seite und wir wollen vielleicht aber ein\nbisschen mehr Datenschutz haben als in den USA auf der anderen Seite ganz,\nganz peppig zusammengebracht hat.\nUnd es gab mehrere solcher Beispiele dort eben von diesen Ausstellern,\nwo ich dachte so, ja, das ist in der Tat eine Art europäischer Perspektive auf\nKI und die kann auch mal nützlich sein.\nDie muss uns nicht nur in die Vergangenheit zurückschubsen.\nSo, nur weil etwas technisch möglich ist, muss man es nicht unbedingt machen\nund da bin ich im Großen und Ganzen der Europäischen Union eigentlich gerade\nganz dankbar, was sie da so,\nan Regularien bringt, diesen KI können wir in der Tat in einer der nächsten\nSitzungen uns nochmal ein bisschen genauer angucken, ich finde den nämlich auch spannend,\ndas wird einen Riesen-Impact auf ganz, ganz viele Firmen da draußen haben ich\nhabe mir das mal jetzt für uns halt als Bibliotheken etwas genauer angeschaut,\nwir sind da glaube ich weitestgehend nicht von betroffen, aber das machen wir\nmal wenn wir ein bisschen mehr Zeit wieder haben.\nDann können wir uns das gerne nochmal dezidierter angucken. Da stehen nämlich\nauch durchaus schlaue Sachen drin.","Ja. Schön. Also ich meine, ich bezweifle ja nicht, dass Deutschland ein paar Anwendungen für AI hat.\nNur die Forschung in diesem Bereich, die findet hier nicht in der Spitze statt.","Ich sag's dir, Blech biegen.","Ja, genau. Es muss Abgase, erzeugt keine Abgase, kann man zu nichts gebrochen.","Naja, aber in der Industrie, Tim, ist es doch ähnlich.\nEs wird doch immer gesagt, dass Deutschland immer noch das goldene Land des\nMittelstandes ist, wo halt irgendwelche Dinge gebaut werden,\ndie so in Sachen Ingenieurskunst nirgendwo anders in der Welt funktionieren.\nUnd warum soll das im Bereich KI nicht auch so laufen?","Naja, man muss jetzt, der Fairness halber muss man sagen, wir haben jetzt sehr\nviel über Large Language Models und KI und so weiter gesprochen und außer Cursor\nhabe ich jetzt noch keine Anwendung gesehen,\ndie irgendwie sinnvoll Mehrwert geliefert hätte.","Ja, also in meinem Bereich stimmt das einfach nicht. Also in dem,\nwas ich tue, wo ich es tue, ist von morgens bis abends spät,\nfangen die Leute an, KI zu nutzen. Es passiert, es ist da.","Schreiben ihre E-Mails damit, oder was?","Nee, aber solche Sachen wie irgendwie Brainstorming-Partner,\nso Achtung, ich brauche folgende Argumentationen.","Konzepte schreiben, genau, Sachen neu strukturieren, übersetzen.\nIch meine, vergiss mal nicht, dass das Ganze...","Übersetzen ist, ja.","Ja, also alle nehmen das schon vollkommen selbstverständlich hin,\ndass du auf einmal alles in alle Sprachen übersetzen kannst.\nDas war vor zehn Jahren noch totaler Cyberspace-Kram und jetzt ist es einfach\nso Snap einfach da gewesen mit ChatGPT.","Naja, mit Deep L schon, muss man in der Ferne es halt halber sagen.","Ja, aber wenn du die anguckst, ihr habt das bestimmt auch gesehen,\ndieses Lex Friedman Interview mit Volodymyr Selenskyj,\nwas von so einer komischen AI nicht nur übersetzt wurde,\nsondern der Audio Track wurde dann wieder neu aufgenommen, wo dann die AI mit\nder Stimme des übersetzten Menschen gesprochen wurde und das sogar halbwegs lippensynchron ist.\nDas fand ich schon auch extrem absurd.\nAlso es ist ein bisschen weird, aber ja.\nOh gut, die hatte auch so einige Fehler.","Da geht es noch krasser ab. Ich weiß nicht, ob ihr das schon mitbekommen habt.\nIch habe das hier auch noch an den Shownotes, jetzt wo du das erwähnst.\nByteDance, also die TikTok-Bude, China.\nDas haben sie zwar jetzt nicht veröffentlicht, sondern es gibt nur ein Video\noder ein Paper, wo das vorgestellt ist. Ich weiß gar nicht, wie sie es vorgestellt haben.\nAber es gibt so verschiedene Videos, die sie veröffentlicht haben.\nEin Modell, das heißt Omni-Human-One.\nUnd da müsst ihr euch echt nochmal anschnallen.\nAlso das ist so die nächste Generation.\nWas sie machen ist, du hast ein Foto von einer Person, ein Foto.\nUnd aus dem einen Foto bauen sie komplette,\nlifelike full body animierte Videos, wo du wirklich hundertmal hingucken musst,\nbis du checkst, dass die irgendwie nicht richtige Videos sind.\nUnd das haben sie irgendwie gebaut. Ich werfe hier gerade mal ein Video zu dem Thema rein.","Dann gibt es auch etliche, das explodiert gerade auch.","Ja, aber das ist wirklich auf einem anderen Level. Also das ist wirklich das krasseste.","Ich bringe dafür in der nächsten Sitzung mal was mit. Da wollte ich nämlich\nmal einen Stillshot hier von uns, wie wir hier in der Meta-Ebene sitzen.\nUnd dann animieren wir das mal.\nJa, wir kommen darauf zurück.","Lass mal das Video mal jetzt mal so parallel, mal ohne Ton bei dir laufen,\ndamit du mal irgendwie ein Gefühl bekommst. Das ist wirklich irre.\nUnd das findet halt alles irgendwie in China statt.\nAlso ich glaube ja nicht, dass jetzt hier irgendwie keiner sich damit beschäftigt\nund ganz ehrlich, Roddy, die Anwendungen sind da.\nAlso übersetzen, programmieren, Leute machen ihre Textplanung damit.\nAlso ich kenne so viele Leute, die das machen und das hat sich auch schon so früh abgezeichnet.","Aber das ist ja jetzt kein Chat-GPT.","Was ist ein Chat-GPT? Omni-Human?","Ja.","Nein, aber ich meine, okay, wir\nmüssen jetzt vielleicht ein bisschen diese Begriffe auseinanderbringen.\nWir haben AI als Technologie, wir haben irgendwie Deep Learning,\nwir haben irgendwie Machine Learning, wir haben jetzt irgendwie die LLMs,\nwir haben Reasoning-LLMs, wir haben,\nBildgenerierende und Videogenerierende Modelle, wir haben eine komplette AI-Entwicklung,\ndie jetzt immer weiter geht. Und,\nwir nähern uns in sehr vielen Bereichen, vielleicht nicht unbedingt einer Perfektion,\naber die Verbesserungen sind einfach nach wie vor exponentiell und es gibt relativ wenig,\nGlaube bei mir, dass das,\ndass wir da jetzt schon am Ende der Fahnenstange angekommen sind.\nEs wird irgendwie einfach stattfinden. Die Treiber dieser Entwicklungen sind\nvor allem in China und den USA.\nNutzer hast du auf der ganzen Welt. Und da ist dann irgendwie mit Europa auch vorbei.\nDa haben wir einfach gar nicht mehr die Wissenschaftslandschaft für und schon\ngar nicht die Investment Landschaft und wir haben auch nicht das Denken in Europa.\nEuropa ist so gefangen in seiner Zukunftsangst, da geht gar nichts mehr.\nDeutschland ist am schlimmsten, aber der Rest ist auch nicht sehr viel besser.\nDa habe ich jetzt jede Hoffnung aufgegeben.","Ich habe noch Hoffnung und das ist doch gut, dass wir hier nicht alle einer Meinung sind.","Ich war heute übrigens in einem AI-Space. Das ist ein Laden hier in einer Shopping-Mall.\nIch habe auch schon ein erstes Foto hier gezeigt. Da gibt es ganz tolle Geräte,\nunter anderem ein AI-Neck-Massage-Pillow.\nIch konnte nicht rausfinden, was daran so AI ist.\nAußerdem gab es noch eine eine, das ist wirklich ein Produkt,\nder hätte ich nicht gedacht, dass ich sowas nochmal sehe.\nUnd zwar gibt es eine automatische,\nKatzenklo, angeblich auch irgendwie AI, ich habe keine Ahnung, was mit AI zu tun hat.\nSo ein automatisch sich selbstreinigendes Katzenklo, was aussieht wie eine Waschmaschine.","Oh, das hätte ich gerne.","Das hältst du gerne. Warte mal, da kriegst du jetzt hier erstmal noch das Foto im Chat.","Das ist so reinigend eines der wenigen letzten ungelösten Menschheitsprobleme.","Genau, aber auch da sind wir auf einem guten Weg. Das ist definitiv was ab.","Reinigungsgeräte aller Art, die Dinge selbstständig tun, das ist ganz groß, bin ich ein Fan von.","Naja, okay, gut, so viel AI.\nVon dieser Omni-Human-Geschichte werden wir auf jeden Fall noch einiges anbieten.","Das ist jetzt spezialisiert quasi auf Menschen. Also dieses Tool,\nwas ich da gerade im Kopf habe, das muss ich erstmal wieder rauskramen.\nDas kann beliebige Szenen halt in den Filmen umwandeln.\nAlso jetzt nicht nur irgendwie, wir versuchen jetzt einen Sprecher irgendwie\nzu symbolieren oder sowas. Ich bringe das in die nächste Sendung mal mit. Das wird lustig.","In einem Film aus Videomaterial.","Du gibst ein Standbild vor und schreibst einem Prompt, was passieren soll.\nUnd das ist wirklich sehr, sehr weird und sehr, sehr creepy.\nUnd das probieren wir nächste Sendung mal aus. Es muss hier Cliffhanger geben, wie bei Twin Peaks.","Okay, das heißt, wir sind jetzt over hier mit unserer Sendung.","Ich glaube.","Ich glaube auch, wir haben irgendwie hier schon wieder vier Stunden gearbeitet\nund irgendwo muss ein Ende sein, außerdem muss ich ja hier irgendwie unsere\nkatastrophale Audioverbindung hier noch irgendwie rausschneiden, mal gucken.","Die letzten 45 Minuten waren jetzt ja wieder okay.","Ja, genau, also wer das nicht live hört, wird vielleicht gar nichts davon gemerkt\nhaben, dass wir hier Aussätze hatten, unerklärlicher Art und Weise,\naber so ist das nochmal bei sieben Stunden Zeitzonen-Differenz,\nda kann schon mal was irgendwie schief gehen.\nOkay, dann würde ich sagen, machen wir doch hier unsere beliebte,\nAbschlussmusik Da kommt sie schon Machen.","Wir den Dackel drauf.","Genau, machen wir den Dackel drauf Und ja,\nvielen Dank fürs Zuhören Das war die Freakshow, wir haben ja da viel recherchiert\nund diskutiert für euch Machen wir gerne und demnächst machen wir es auch wieder\nMal gucken, wenn die nächste Sendung kommt Und das werden wir euch aber auch\nnoch mithalten Geht wählen, Leute,\nAber bitteschön. Bis dann.","Keine Nazis. Tschüss."]}